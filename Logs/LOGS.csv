date,data-type,net-architecture,loss-func,optim,early-stopping-patience,parameters-amount,n-epochs,batch-size,best-train-loss,best-valid-loss,best-kappa,cfg,time_estimated,lb-kappa-score,dataset,trainloop,time-estimated,scheduler
2019.08.05 23:20:37,,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models

import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024, bias=True),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            self.model = model

",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.00015
    weight_decay: 0
)",8.0,44599361.0,10.0,32.0,0.34075300000000003,0.329401,0.6176,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

class Config:
    def __init__(self):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.seed = 13
        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            self.df_logger = Logger(self.logsFileName + '.csv', 'df')
            self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'



        ## MODEL PARAMETERS
        self.model_type = 'ResNet'

        self.model = MainModel(model_type=self.model_type).model
        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = 0.00015
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = None
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 8
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 12
        self.n_epochs = 15
        self.batch_size = 32
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = 0.4

        ## PRINT FREQUENCY
        self.print_frequency = 25
",327.82,,,,,
2019.08.05 23:29:59,,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models

import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024, bias=True),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            self.model = model

",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.00015
    weight_decay: 0
)",8.0,44599361.0,15.0,32.0,0.12393399999999999,0.298256,0.6095,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

class Config:
    def __init__(self):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.seed = 13
        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            self.df_logger = Logger(self.logsFileName + '.csv', 'df')
            self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'



        ## MODEL PARAMETERS
        self.model_type = 'ResNet'

        self.model = MainModel(model_type=self.model_type).model
        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = 0.00015
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = None
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 8
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'kappa' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 12
        self.n_epochs = 15
        self.batch_size = 32
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = 0.4

        ## PRINT FREQUENCY
        self.print_frequency = 25
",,,,,,
2019.08.05 23:43:43,new_comp,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models

import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024, bias=True),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            self.model = model

",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.00015
    weight_decay: 0
)",10.0,44599361.0,20.0,32.0,0.073034,0.316579,0.637,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

class Config:
    def __init__(self):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.seed = 13
        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            self.df_logger = Logger(self.logsFileName + '.csv', 'df')
            self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.data_type = 'new_comp'


        ## MODEL PARAMETERS
        self.model_type = 'ResNet'

        self.model = MainModel(model_type=self.model_type).model
        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = 0.00015
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = None
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 10
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #pytorch

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 12
        self.n_epochs = 50
        self.batch_size = 32
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = 0.4

        ## PRINT FREQUENCY
        self.print_frequency = 25
",690.71,,,,,
2019.08.05 23:55:42,new_comp,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models

import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024, bias=True),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            self.model = model

",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.00015
    weight_decay: 0
)",10.0,44599361.0,21.0,32.0,0.136981,0.303939,0.6071,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

class Config:
    def __init__(self):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.seed = 13
        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            self.df_logger = Logger(self.logsFileName + '.csv', 'df')
            self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.data_type = 'new_comp'


        ## MODEL PARAMETERS
        self.model_type = 'ResNet'

        self.model = MainModel(model_type=self.model_type).model
        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = 0.00015
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = None
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 10
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'kappa' #pytorch

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 12
        self.n_epochs = 50
        self.batch_size = 32
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = 0.4

        ## PRINT FREQUENCY
        self.print_frequency = 25
",717.39,,,,,
2019.08.06 08:11:27,new_comp,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models

import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024, bias=True),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            self.model = model

",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.00015
    weight_decay: 0
)",15.0,44599361.0,26.0,1.0,0.337197,1.329923,,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            self.df_logger = Logger(self.logsFileName + '.csv', 'df')
            self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.data_type = 'new_comp'


        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.model_type = 'ResNet'

        self.model = MainModel(model_type=self.model_type).model
        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = 0.00015
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = None
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 15
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 12
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = 0.4

        ## PRINT FREQUENCY
        self.print_frequency = 25
",5222.55,,,,,
2019.08.06 09:38:31,new_comp,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models

import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024, bias=True),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            self.model = model

",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.00015
    weight_decay: 0
)",15.0,44599361.0,24.0,2.0,0.30700900000000003,0.430778,,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            self.df_logger = Logger(self.logsFileName + '.csv', 'df')
            self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.data_type = 'new_comp'


        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.model_type = 'ResNet'

        self.model = MainModel(model_type=self.model_type).model
        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = 0.00015
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = None
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 15
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 12
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = 0.4

        ## PRINT FREQUENCY
        self.print_frequency = 25
",2654.21,,,,,
2019.08.06 10:22:46,new_comp,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models

import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024, bias=True),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            self.model = model

",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.00015
    weight_decay: 0
)",15.0,44599361.0,28.0,4.0,0.164537,0.325475,,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            self.df_logger = Logger(self.logsFileName + '.csv', 'df')
            self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.data_type = 'new_comp'


        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.model_type = 'ResNet'

        self.model = MainModel(model_type=self.model_type).model
        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = 0.00015
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = None
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 15
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 12
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = 0.4

        ## PRINT FREQUENCY
        self.print_frequency = 25
",1850.02,,,,,
2019.08.06 10:53:37,new_comp,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models

import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024, bias=True),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            self.model = model

",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.00015
    weight_decay: 0
)",15.0,44599361.0,69.0,8.0,0.032534,0.287978,,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            self.df_logger = Logger(self.logsFileName + '.csv', 'df')
            self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.data_type = 'new_comp'


        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.model_type = 'ResNet'

        self.model = MainModel(model_type=self.model_type).model
        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = 0.00015
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = None
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 15
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 12
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = 0.4

        ## PRINT FREQUENCY
        self.print_frequency = 25
",3120.4,,,,,
2019.08.06 11:45:38,new_comp,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models

import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024, bias=True),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            self.model = model

",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.00015
    weight_decay: 0
)",15.0,44599361.0,38.0,16.0,0.046238,0.29125700000000004,0.6244,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            self.df_logger = Logger(self.logsFileName + '.csv', 'df')
            self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.data_type = 'new_comp'


        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.model_type = 'ResNet'

        self.model = MainModel(model_type=self.model_type).model
        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = 0.00015
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = None
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 15
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 12
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = 0.4

        ## PRINT FREQUENCY
        self.print_frequency = 25
",1351.18,,,,,
2019.08.06 12:08:10,new_comp,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models

import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024, bias=True),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            self.model = model

",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.00015
    weight_decay: 0
)",15.0,44599361.0,37.0,32.0,0.031294,0.27269099999999996,0.6325,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            self.df_logger = Logger(self.logsFileName + '.csv', 'df')
            self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.data_type = 'new_comp'


        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.model_type = 'ResNet'

        self.model = MainModel(model_type=self.model_type).model
        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = 0.00015
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = None
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 15
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 12
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = 0.4

        ## PRINT FREQUENCY
        self.print_frequency = 25
",1212.47,,,,,
2019.08.06 12:28:23,new_comp,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models

import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024, bias=True),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            self.model = model

",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.00015
    weight_decay: 0
)",15.0,44599361.0,36.0,48.0,0.124594,0.304088,0.5833,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            self.df_logger = Logger(self.logsFileName + '.csv', 'df')
            self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.data_type = 'new_comp'


        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.model_type = 'ResNet'

        self.model = MainModel(model_type=self.model_type).model
        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = 0.00015
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = None
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 15
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 12
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = 0.4

        ## PRINT FREQUENCY
        self.print_frequency = 25
",1226.16,,,,,
2019.08.06 12:48:50,new_comp,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models

import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024, bias=True),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            self.model = model

",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.00015
    weight_decay: 0
)",15.0,44599361.0,39.0,64.0,0.021981,0.263108,0.6509999999999999,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            self.df_logger = Logger(self.logsFileName + '.csv', 'df')
            self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.data_type = 'new_comp'


        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.model_type = 'ResNet'

        self.model = MainModel(model_type=self.model_type).model
        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = 0.00015
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = None
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 15
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 12
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = 0.4

        ## PRINT FREQUENCY
        self.print_frequency = 25
",1294.01,,,,,
2019.08.06 17:06:42,new_comp,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models

import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024, bias=True),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            self.model = model

",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.1
    weight_decay: 0
)",15.0,44599361.0,20.0,64.0,271.764497,89.030222,0.0029,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            self.df_logger = Logger(self.logsFileName + '.csv', 'df')
            self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.data_type = 'new_comp'


        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.model_type = 'ResNet'

        self.model = MainModel(model_type=self.model_type).model
        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = None
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 15
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 12
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = 0.4

        ## PRINT FREQUENCY
        self.print_frequency = 25
",662.72,,,,,
2019.08.06 17:17:45,new_comp,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models

import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024, bias=True),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            self.model = model

",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.01
    weight_decay: 0
)",15.0,44599361.0,63.0,64.0,0.466971,0.53052,0.2794,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            self.df_logger = Logger(self.logsFileName + '.csv', 'df')
            self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.data_type = 'new_comp'


        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.model_type = 'ResNet'

        self.model = MainModel(model_type=self.model_type).model
        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = None
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 15
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 12
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = 0.4

        ## PRINT FREQUENCY
        self.print_frequency = 25
",2089.43,,,,,
2019.08.06 17:52:35,new_comp,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models

import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024, bias=True),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            self.model = model

",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0
)",15.0,44599361.0,61.0,64.0,0.05952,0.324779,0.5353,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            self.df_logger = Logger(self.logsFileName + '.csv', 'df')
            self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.data_type = 'new_comp'


        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.model_type = 'ResNet'

        self.model = MainModel(model_type=self.model_type).model
        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = None
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 15
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 12
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = 0.4

        ## PRINT FREQUENCY
        self.print_frequency = 25
",2023.55,,,,,
2019.08.06 18:26:20,new_comp,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models

import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024, bias=True),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            self.model = model

",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)",15.0,44599361.0,54.0,64.0,0.043727999999999996,0.305871,0.588,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            self.df_logger = Logger(self.logsFileName + '.csv', 'df')
            self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.data_type = 'new_comp'


        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.model_type = 'ResNet'

        self.model = MainModel(model_type=self.model_type).model
        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = None
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 15
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 12
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = 0.4

        ## PRINT FREQUENCY
        self.print_frequency = 25
",1793.97,,,,,
2019.08.06 18:56:14,new_comp,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models

import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024, bias=True),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            self.model = model

",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",15.0,44599361.0,61.0,64.0,0.019651,0.259806,0.6612,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            self.df_logger = Logger(self.logsFileName + '.csv', 'df')
            self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.data_type = 'new_comp'


        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.model_type = 'ResNet'

        self.model = MainModel(model_type=self.model_type).model
        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = None
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 15
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 12
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = 0.4

        ## PRINT FREQUENCY
        self.print_frequency = 25
",2032.91,,,,,
2019.08.06 19:30:08,new_comp,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models

import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024, bias=True),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            self.model = model

",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    weight_decay: 0
)",15.0,44599361.0,61.0,64.0,0.023965,0.279988,0.6435,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            self.df_logger = Logger(self.logsFileName + '.csv', 'df')
            self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.data_type = 'new_comp'


        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.model_type = 'ResNet'

        self.model = MainModel(model_type=self.model_type).model
        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = None
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 15
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 12
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = 0.4

        ## PRINT FREQUENCY
        self.print_frequency = 25
",2028.09,,,,,
2019.08.06 20:03:57,new_comp,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models

import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024, bias=True),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            self.model = model

",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 1e-05
    weight_decay: 0
)",15.0,44599361.0,23.0,64.0,0.079316,0.331172,0.6062,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            self.df_logger = Logger(self.logsFileName + '.csv', 'df')
            self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.data_type = 'new_comp'


        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.model_type = 'ResNet'

        self.model = MainModel(model_type=self.model_type).model
        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = None
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 15
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 12
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = 0.4

        ## PRINT FREQUENCY
        self.print_frequency = 25
",766.31,,,,,
2019.08.06 21:08:37,new_comp,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models

import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024, bias=True),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            self.model = model

",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.00015
    weight_decay: 0
)",15.0,44599361.0,39.0,64.0,0.021981,0.263108,0.6509999999999999,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            self.df_logger = Logger(self.logsFileName + '.csv', 'df')
            self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.data_type = 'new_comp'


        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.model_type = 'ResNet'

        self.model = MainModel(model_type=self.model_type).model
        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = None
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 15
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 12
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip

        ## PRINT FREQUENCY
        self.print_frequency = 25
",1318.98,,,,,
2019.08.06 21:30:37,new_comp,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models

import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024, bias=True),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            self.model = model

",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.00015
    weight_decay: 0
)",15.0,44599361.0,39.0,64.0,0.021981,0.263108,0.6509999999999999,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            self.df_logger = Logger(self.logsFileName + '.csv', 'df')
            self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.data_type = 'new_comp'


        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.model_type = 'ResNet'

        self.model = MainModel(model_type=self.model_type).model
        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = None
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 15
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 12
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip

        ## PRINT FREQUENCY
        self.print_frequency = 25
",1320.6,,,,,
2019.08.06 22:00:46,new_comp_quadratic_kappa,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models

import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024, bias=True),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            self.model = model

",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.00015
    weight_decay: 0
)",15.0,44599361.0,39.0,64.0,0.021981,0.263108,0.8797,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            self.df_logger = Logger(self.logsFileName + '.csv', 'df')
            self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.data_type = 'new_comp_quadratic_kappa'


        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.model_type = 'ResNet'

        self.model = MainModel(model_type=self.model_type).model
        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = None
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 15
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 12
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip

        ## PRINT FREQUENCY
        self.print_frequency = 25
",1318.7,,,,,
2019.08.06 22:28:37,new_comp_quadratic_kappa,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models

import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024, bias=True),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            self.model = model

",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.00015
    weight_decay: 0
)",15.0,44599361.0,100.0,64.0,0.803977,1.546088,0.3728,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            self.df_logger = Logger(self.logsFileName + '.csv', 'df')
            self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.data_type = 'new_comp_quadratic_kappa'

        self.img_size = 224

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.model_type = 'ResNet'

        self.model = MainModel(model_type=self.model_type).model
        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = None
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 15
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 12
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip

        ## PRINT FREQUENCY
        self.print_frequency = 25
",,,,,,
2019.08.06 22:35:19,new_comp_quadratic_kappa,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models

import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

def _resnext(path, block, layers, pretrained, progress, **kwargs):
    model = ResNet(block, layers, **kwargs)
    model.load_state_dict(torch.load(path))
    return model

def resnext101_32x8d_wsl(path, progress=True, **kwargs):
    """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
    and finetuned on ImageNet from Figure 5 in
    `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
    Args:
        progress (bool): If True, displays a progress bar of the download to stderr.
    """"""
    kwargs['groups'] = 32
    kwargs['width_per_group'] = 8
    return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            for param in model.parameters():
                param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024, bias=True),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            self.model = model
        elif model_type == 'ResNext':
            model = resnext101_32x8d_wsl(path='./input/pretrained_models/ig_resnext101_32x8-c38310e5.pth')

            for param in model.parameters():
                param.requires_grad = False

            model.fc = torch.nn.Linear(2048, 1)


",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.00015
    weight_decay: 0
)",15.0,2099201.0,59.0,64.0,0.390875,0.448205,0.7998,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            self.df_logger = Logger(self.logsFileName + '.csv', 'df')
            self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.data_type = 'new_comp_quadratic_kappa'

        self.img_size = 224

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.model_type = 'ResNet'

        self.model = MainModel(model_type=self.model_type).model
        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = None
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 15
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 12
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip

        ## PRINT FREQUENCY
        self.print_frequency = 25
",1802.03,,,,,
2019.08.06 23:37:58,new_comp_quadratic_kappa,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models

import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            for param in model.parameters():
                param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024, bias=True),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            self.model = model
        elif model_type == 'ResNext101_32x16d':
            self.model = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x16d_wsl')

    def get_model(self):
        return self.model

",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.00015
    weight_decay: 0
)",15.0,2049.0,100.0,64.0,0.509555,0.551465,0.7007,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            self.df_logger = Logger(self.logsFileName + '.csv', 'df')
            self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.data_type = 'new_comp_quadratic_kappa'

        self.img_size = 256

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.model_type = 'ResNext101_32x16d'

        self.model = MainModel(model_type=self.model_type).get_model()
        if self.model_type == 'ResNext101_32x16d':
            for param in self.model.parameters():
                param.requires_grad = False
            self.model.fc = torch.nn.Linear(2048, 1)

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = None
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 15
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = ""kappa"" #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 12
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip

        ## PRINT FREQUENCY
        self.print_frequency = 25
",,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from config import Config


cfg = Config()

transforms_train = transforms.Compose([
    transforms.ToPILImage(),
    transforms.Resize((cfg.img_size, cfg.img_size)),
    transforms.RandomHorizontalFlip(p=cfg.p_horizontalflip),
    #transforms.ColorJitter(brightness=2, contrast=2),
    transforms.ToTensor(),
    transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))
])

transforms_valid = transforms.Compose([
    transforms.ToPILImage(),
    transforms.Resize((cfg.img_size, cfg.img_size)),
    transforms.ToTensor(),
    transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))
])


class CreateDataset(Dataset):
    def __init__(self, df_data, data_dir='./input/', transform=None):
        super().__init__()
        self.df = df_data.values
        self.data_dir = data_dir
        self.transform = transform

    def __len__(self):
        return len(self.df)

    def __getitem__(self, index):
        img_name, label = self.df[index]
        img_path = os.path.join(self.data_dir, img_name + '.png')
        image = cv2.imread(img_path)
        if self.transform is not None:
            image = self.transform(image)
        return image, label","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from config import Config
from train_dataset import transforms_train, transforms_valid, CreateDataset
from model import MainModel
from logger import Logger

# Open source libs


def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_csv = pd.read_csv('./input/train.csv')
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))



    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))

    # Data Processing

    train_path = ""./input/train_images/""

    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed)
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.mean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## LOGGINS LOSSES
        if valid_loss_best > valid_loss_epoch:
            valid_loss_best  = valid_loss_epoch
            train_loss_best = train_loss_epoch
            kappa_best = valid_kappa
            add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
            add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
            add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name + '.pt', epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))

if __name__ == '__main__':
    batch_size_list = [64]
    lr_list = [0.00015]
    p_horizontalflip_list = [0.4]
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}\n\n\n"".format(batch_size, lr, p_horizontalflip)
                print(info)
                main(batch_size, lr, p_horizontalflip, info)
",,
2019.08.06 23:52:23,new_comp_quadratic_kappa,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models

import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            for param in model.parameters():
                param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024, bias=True),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            self.model = model
        elif model_type == 'ResNext101_32x16d':
            self.model = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x16d_wsl')

    def get_model(self):
        return self.model

",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.00015
    weight_decay: 0
)",15.0,2049.0,78.0,64.0,0.35261,0.49415,0.7726,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            self.df_logger = Logger(self.logsFileName + '.csv', 'df')
            self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.data_type = 'new_comp_quadratic_kappa'

        self.img_size = 256

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.model_type = 'ResNext101_32x16d'

        self.model = MainModel(model_type=self.model_type).get_model()
        if self.model_type == 'ResNext101_32x16d':
            for param in self.model.parameters():
                param.requires_grad = False
            self.model.fc = torch.nn.Linear(2048, 1)

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = None
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 15
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = ""pytorch"" #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 12
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip

        ## PRINT FREQUENCY
        self.print_frequency = 25
",3647.06,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from config import Config


cfg = Config()

transforms_train = transforms.Compose([
    transforms.ToPILImage(),
    transforms.Resize((cfg.img_size, cfg.img_size)),
    # transforms.RandomHorizontalFlip(p=cfg.p_horizontalflip),
    #transforms.ColorJitter(brightness=2, contrast=2),
    transforms.ToTensor(),
    transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))
])

transforms_valid = transforms.Compose([
    transforms.ToPILImage(),
    transforms.Resize((cfg.img_size, cfg.img_size)),
    transforms.ToTensor(),
    transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))
])


class CreateDataset(Dataset):
    def __init__(self, df_data, data_dir='./input/', transform=None):
        super().__init__()
        self.df = df_data.values
        self.data_dir = data_dir
        self.transform = transform

    def __len__(self):
        return len(self.df)

    def __getitem__(self, index):
        img_name, label = self.df[index]
        img_path = os.path.join(self.data_dir, img_name + '.png')
        image = cv2.imread(img_path)
        if self.transform is not None:
            image = self.transform(image)
        return image, label","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from config import Config
from train_dataset import transforms_train, transforms_valid, CreateDataset
from model import MainModel
from logger import Logger

# Open source libs


def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_csv = pd.read_csv('./input/train.csv')
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))



    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))

    # Data Processing

    train_path = ""./input/train_images/""

    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed)
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.mean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## LOGGINS LOSSES
        if valid_loss_best > valid_loss_epoch:
            valid_loss_best  = valid_loss_epoch
            train_loss_best = train_loss_epoch
            kappa_best = valid_kappa
            add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
            add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
            add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name + '.pt', epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))

if __name__ == '__main__':
    batch_size_list = [64]
    lr_list = [0.00015]
    p_horizontalflip_list = [0.4]
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}\n\n\n"".format(batch_size, lr, p_horizontalflip)
                print(info)
                main(batch_size, lr, p_horizontalflip, info)
",,
2019.08.07 01:40:35,new_comp_quadratic_kappa,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models

import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            for param in model.parameters():
                param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024, bias=True),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            self.model = model
        elif model_type == 'ResNext101_32x16d':
            self.model = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x16d_wsl')

    def get_model(self):
        return self.model

",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.00015
    weight_decay: 0
)",15.0,2099201.0,100.0,64.0,0.56591,0.529179,0.7258,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            self.df_logger = Logger(self.logsFileName + '.csv', 'df')
            self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.data_type = 'new_comp_quadratic_kappa'

        self.img_size = 256

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.model_type = 'ResNet101'

        self.model = MainModel(model_type=self.model_type).get_model()
        if self.model_type == 'ResNext101_32x16d':
            for param in self.model.parameters():
                param.requires_grad = False
            self.model.fc = torch.nn.Linear(2048, 1)

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = None
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 15
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = ""pytorch"" #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 12
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip

        ## PRINT FREQUENCY
        self.print_frequency = 25
",,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_2015 + p + '.png'):
    #     return train_2015 + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image = crop_image_from_gray(image)
        image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])
","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from config import Config
from train_dataset import transforms_train, transforms_valid, CreateDataset
from model import MainModel
from logger import Logger

# Open source libs


def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_csv = pd.read_csv('./input/train.csv')
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))



    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))

    # Data Processing

    train_path = ""./input/train_images/""

    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed)
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.mean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## LOGGINS LOSSES
        if valid_loss_best > valid_loss_epoch:
            valid_loss_best  = valid_loss_epoch
            train_loss_best = train_loss_epoch
            kappa_best = valid_kappa
            add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
            add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
            add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name + '.pt', epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))

if __name__ == '__main__':
    batch_size_list = [64]
    lr_list = [0.00015]
    p_horizontalflip_list = [0.4]
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}\n\n\n"".format(batch_size, lr, p_horizontalflip)
                print(info)
                main(batch_size, lr, p_horizontalflip, info)
",,
2019.08.07 02:19:27,new_comp_quadratic_kappa,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models

import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            for param in model.parameters():
                param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024, bias=True),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            self.model = model
        elif model_type == 'ResNext101_32x16d':
            self.model = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x16d_wsl')

    def get_model(self):
        return self.model

",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.00015
    weight_decay: 0
)",15.0,2099201.0,100.0,128.0,,0.838298,0.5452,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            self.df_logger = Logger(self.logsFileName + '.csv', 'df')
            self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.data_type = 'new_comp_quadratic_kappa'

        self.img_size = 256

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.model_type = 'ResNet101'

        self.model = MainModel(model_type=self.model_type).get_model()
        if self.model_type == 'ResNext101_32x16d':
            for param in self.model.parameters():
                param.requires_grad = False
            self.model.fc = torch.nn.Linear(2048, 1)

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = None
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 15
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = ""pytorch"" #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip

        ## PRINT FREQUENCY
        self.print_frequency = 25
",,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_2015 + p + '.png'):
    #     return train_2015 + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image = crop_image_from_gray(image)
        image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])
","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from config import Config
from train_dataset import transforms_train, transforms_valid, CreateDataset
from model import MainModel
from logger import Logger

# Open source libs


def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_csv = pd.read_csv('./input/train.csv')
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))



    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))

    # Data Processing

    train_path = ""./input/train_images/""

    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed)
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.mean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## LOGGINS LOSSES
        if valid_loss_best > valid_loss_epoch:
            valid_loss_best  = valid_loss_epoch
            train_loss_best = train_loss_epoch
            kappa_best = valid_kappa
            add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
            add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
            add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name + '.pt', epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))

if __name__ == '__main__':
    batch_size_list = [128]
    lr_list = [0.00015]
    p_horizontalflip_list = [0.4]
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}\n\n\n"".format(batch_size, lr, p_horizontalflip)
                print(info)
                main(batch_size, lr, p_horizontalflip, info)
",,
2019.08.07 02:23:27,new_comp_quadratic_kappa,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models

import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            for param in model.parameters():
                param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024, bias=True),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            self.model = model
        elif model_type == 'ResNext101_32x16d':
            self.model = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x16d_wsl')

    def get_model(self):
        return self.model

",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.00015
    weight_decay: 0
)",15.0,2099201.0,72.0,32.0,0.485065,0.485729,0.7666,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            self.df_logger = Logger(self.logsFileName + '.csv', 'df')
            self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.data_type = 'new_comp_quadratic_kappa'

        self.img_size = 256

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.model_type = 'ResNet101'

        self.model = MainModel(model_type=self.model_type).get_model()
        if self.model_type == 'ResNext101_32x16d':
            for param in self.model.parameters():
                param.requires_grad = False
            self.model.fc = torch.nn.Linear(2048, 1)

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = None
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 15
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = ""pytorch"" #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip

        ## PRINT FREQUENCY
        self.print_frequency = 25
",11631.53,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_2015 + p + '.png'):
    #     return train_2015 + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image = crop_image_from_gray(image)
        image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])
","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from config import Config
from train_dataset import transforms_train, transforms_valid, CreateDataset
from model import MainModel
from logger import Logger

# Open source libs


def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_csv = pd.read_csv('./input/train.csv')
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))



    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))

    # Data Processing

    train_path = ""./input/train_images/""

    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed)
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.mean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## LOGGINS LOSSES
        if valid_loss_best > valid_loss_epoch:
            valid_loss_best  = valid_loss_epoch
            train_loss_best = train_loss_epoch
            kappa_best = valid_kappa
            add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
            add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
            add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name + '.pt', epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))

if __name__ == '__main__':
    batch_size_list = [32]
    lr_list = [0.00015]
    p_horizontalflip_list = [0.4]
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}\n\n\n"".format(batch_size, lr, p_horizontalflip)
                print(info)
                main(batch_size, lr, p_horizontalflip, info)
",,
2019.08.07 08:44:40,new_comp_quadratic_kappa,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models

import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            for param in model.parameters():
                param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024, bias=True),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            self.model = model
        elif model_type == 'ResNext101_32x16d':
            self.model = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x16d_wsl')

    def get_model(self):
        return self.model

",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.00015
    weight_decay: 0
)",8.0,2099201.0,50.0,512.0,2.464634,2.461174,0.0038,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            self.df_logger = Logger(self.logsFileName + '.csv', 'df')
            self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.data_type = 'new_comp_quadratic_kappa'

        self.img_size = 256

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.model_type = 'ResNet101'

        self.model = MainModel(model_type=self.model_type).get_model()
        if self.model_type == 'ResNext101_32x16d':
            for param in self.model.parameters():
                param.requires_grad = False
            self.model.fc = torch.nn.Linear(2048, 1)

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = None
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 8
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = ""pytorch"" #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 50
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip

        ## PRINT FREQUENCY
        self.print_frequency = 25
",,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_2015 + p + '.png'):
    #     return train_2015 + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image = crop_image_from_gray(image)
        image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])
","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from config import Config
from train_dataset import transforms_train, transforms_valid, CreateDataset
from model import MainModel
from logger import Logger

# Open source libs


def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_csv = pd.read_csv('./input/train.csv')
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))



    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))

    # Data Processing

    train_path = ""./input/train_images/""

    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed)
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.mean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## LOGGINS LOSSES
        if valid_loss_best > valid_loss_epoch:
            valid_loss_best  = valid_loss_epoch
            train_loss_best = train_loss_epoch
            kappa_best = valid_kappa
            add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
            add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
            add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name + '.pt', epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))

if __name__ == '__main__':
    batch_size_list = [512]
    lr_list = [0.00015]
    p_horizontalflip_list = [0.4]
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}\n\n\n"".format(batch_size, lr, p_horizontalflip)
                print(info)
                main(batch_size, lr, p_horizontalflip, info)
",,
2019.08.07 08:49:26,new_comp_quadratic_kappa,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models

import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            for param in model.parameters():
                param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024, bias=True),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            self.model = model
        elif model_type == 'ResNext101_32x16d':
            self.model = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x16d_wsl')

    def get_model(self):
        return self.model

",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.00015
    weight_decay: 0
)",8.0,2099201.0,50.0,64.0,0.658462,0.602823,0.6959,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            self.df_logger = Logger(self.logsFileName + '.csv', 'df')
            self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.data_type = 'new_comp_quadratic_kappa'

        self.img_size = 256

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.model_type = 'ResNet101'

        self.model = MainModel(model_type=self.model_type).get_model()
        if self.model_type == 'ResNext101_32x16d':
            for param in self.model.parameters():
                param.requires_grad = False
            self.model.fc = torch.nn.Linear(2048, 1)

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = None
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 8
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = ""pytorch"" #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 50
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip

        ## PRINT FREQUENCY
        self.print_frequency = 25
",,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_2015 + p + '.png'):
    #     return train_2015 + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image = crop_image_from_gray(image)
        image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])
","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from config import Config
from train_dataset import transforms_train, transforms_valid, CreateDataset
from model import MainModel
from logger import Logger

# Open source libs


def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_csv = pd.read_csv('./input/train.csv')
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))



    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))

    # Data Processing

    train_path = ""./input/train_images/""

    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed)
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.mean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## LOGGINS LOSSES
        if valid_loss_best > valid_loss_epoch:
            valid_loss_best  = valid_loss_epoch
            train_loss_best = train_loss_epoch
            kappa_best = valid_kappa
            add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
            add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
            add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name + '.pt', epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))

if __name__ == '__main__':
    batch_size_list = [64]
    lr_list = [0.00015]
    p_horizontalflip_list = [0.4]
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}\n\n\n"".format(batch_size, lr, p_horizontalflip)
                print(info)
                main(batch_size, lr, p_horizontalflip, info)
",,
2019.08.07 09:02:11,new_comp_quadratic_kappa,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models

import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024, bias=True),
                # nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            self.model = model
        elif model_type == 'ResNext101_32x16d':
            self.model = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x16d_wsl')

    def get_model(self):
        return self.model

",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.00015
    weight_decay: 0
)",8.0,44599361.0,27.0,32.0,0.195692,0.25443899999999997,0.8735,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            self.df_logger = Logger(self.logsFileName + '.csv', 'df')
            self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.data_type = 'new_comp_quadratic_kappa'

        self.img_size = 256

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.model_type = 'ResNet101'

        self.model = MainModel(model_type=self.model_type).get_model()
        if self.model_type == 'ResNext101_32x16d':
            for param in self.model.parameters():
                param.requires_grad = False
            self.model.fc = torch.nn.Linear(2048, 1)

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = None
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 8
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = ""pytorch"" #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 50
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip

        ## PRINT FREQUENCY
        self.print_frequency = 25
",4407.88,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_2015 + p + '.png'):
    #     return train_2015 + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image = crop_image_from_gray(image)
        image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])
","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from config import Config
from train_dataset import transforms_train, transforms_valid, CreateDataset
from model import MainModel
from logger import Logger

# Open source libs


def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_csv = pd.read_csv('./input/train.csv')
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))



    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))

    # Data Processing

    train_path = ""./input/train_images/""

    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed)
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.mean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## LOGGINS LOSSES
        if valid_loss_best > valid_loss_epoch:
            valid_loss_best  = valid_loss_epoch
            train_loss_best = train_loss_epoch
            kappa_best = valid_kappa
            add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
            add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
            add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name + '.pt', epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))

if __name__ == '__main__':
    batch_size_list = [32]
    lr_list = [0.00015]
    p_horizontalflip_list = [0.4]
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}\n\n\n"".format(batch_size, lr, p_horizontalflip)
                print(info)
                main(batch_size, lr, p_horizontalflip, info)
",,
2019.08.07 15:19:31,new_comp_quadratic_kappa,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models

import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            self.model = model
        elif model_type == 'ResNext101_32x16d':
            self.model = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x16d_wsl')

    def get_model(self):
        return self.model

",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.00015
    weight_decay: 0
)",8.0,44599361.0,27.0,32.0,0.186261,0.255677,0.8759999999999999,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            self.df_logger = Logger(self.logsFileName + '.csv', 'df')
            self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.data_type = 'new_comp_quadratic_kappa'

        self.img_size = 256

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.model_type = 'ResNet101'

        self.model = MainModel(model_type=self.model_type).get_model()
        if self.model_type == 'ResNext101_32x16d':
            for param in self.model.parameters():
                param.requires_grad = False
            self.model.fc = torch.nn.Linear(2048, 1)

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = None
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 8
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = ""pytorch"" #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 50
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip

        ## PRINT FREQUENCY
        self.print_frequency = 25
",4404.76,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_2015 + p + '.png'):
    #     return train_2015 + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image = crop_image_from_gray(image)
        image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])
","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from config import Config
from train_dataset import transforms_train, transforms_valid, CreateDataset
from model import MainModel
from logger import Logger

# Open source libs


def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_csv = pd.read_csv('./input/train.csv')
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))



    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))

    # Data Processing

    train_path = ""./input/train_images/""

    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed)
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.mean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## LOGGINS LOSSES
        if valid_loss_best > valid_loss_epoch:
            valid_loss_best  = valid_loss_epoch
            train_loss_best = train_loss_epoch
            kappa_best = valid_kappa
            add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
            add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
            add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name + '.pt', epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))

if __name__ == '__main__':
    batch_size_list = [32]
    lr_list = [0.00015]
    p_horizontalflip_list = [0.4]
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}\n\n\n"".format(batch_size, lr, p_horizontalflip)
                print(info)
                main(batch_size, lr, p_horizontalflip, info)
",,
2019.08.07 16:36:39,new_comp_quadratic_kappa,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models

import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            self.model = model
        elif model_type == 'ResNext101_32x16d':
            self.model = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x16d_wsl')

    def get_model(self):
        return self.model

",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.00015
    weight_decay: 0
)",8.0,44599361.0,14.0,32.0,0.310639,0.284727,0.8585,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            self.df_logger = Logger(self.logsFileName + '.csv', 'df')
            self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.data_type = 'new_comp_quadratic_kappa'

        self.img_size = 224

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.model_type = 'ResNet101'

        self.model = MainModel(model_type=self.model_type).get_model()
        if self.model_type == 'ResNext101_32x16d':
            for param in self.model.parameters():
                param.requires_grad = False
            self.model.fc = torch.nn.Linear(2048, 1)

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = None
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 8
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = ""pytorch"" #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 50
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip

        ## PRINT FREQUENCY
        self.print_frequency = 25
",2217.21,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_2015 + p + '.png'):
    #     return train_2015 + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image = crop_image_from_gray(image)
        image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])
","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from config import Config
from train_dataset import transforms_train, transforms_valid, CreateDataset
from model import MainModel
from logger import Logger

# Open source libs


def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_csv = pd.read_csv('./input/train.csv')
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))



    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))

    # Data Processing

    train_path = ""./input/train_images/""

    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed)
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.mean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## LOGGINS LOSSES
        if valid_loss_best > valid_loss_epoch:
            valid_loss_best  = valid_loss_epoch
            train_loss_best = train_loss_epoch
            kappa_best = valid_kappa
            add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
            add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
            add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name + '.pt', epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))

if __name__ == '__main__':
    batch_size_list = [32]
    lr_list = [0.00015]
    p_horizontalflip_list = [0.4]
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}\n\n\n"".format(batch_size, lr, p_horizontalflip)
                print(info)
                main(batch_size, lr, p_horizontalflip, info)
",,
2019.08.07 19:09:56,new_comp_quadratic_kappa,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models

import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            self.model = model
        elif model_type == 'ResNext101_32x16d':
            self.model = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x16d_wsl')

    def get_model(self):
        return self.model

",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.00015
    weight_decay: 0
)",8.0,44599361.0,73.0,32.0,0.186261,0.255677,0.8759999999999999,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            self.df_logger = Logger(self.logsFileName + '.csv', 'df')
            self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.data_type = 'new_comp_quadratic_kappa'

        self.img_size = 256

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.model_type = 'ResNet101'

        self.model = MainModel(model_type=self.model_type).get_model()
        if self.model_type == 'ResNext101_32x16d':
            for param in self.model.parameters():
                param.requires_grad = False
            self.model.fc = torch.nn.Linear(2048, 1)

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = None
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 8
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = ""kappa"" #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip

        ## PRINT FREQUENCY
        self.print_frequency = 25
",11959.07,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_2015 + p + '.png'):
    #     return train_2015 + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image = crop_image_from_gray(image)
        image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])
","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from config import Config
from train_dataset import transforms_train, transforms_valid, CreateDataset
from model import MainModel
from logger import Logger

# Open source libs


def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_csv = pd.read_csv('./input/train.csv')
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))



    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))

    # Data Processing

    train_path = ""./input/train_images/""

    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed)
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.mean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## LOGGINS LOSSES
        if valid_loss_best > valid_loss_epoch:
            valid_loss_best  = valid_loss_epoch
            train_loss_best = train_loss_epoch
            kappa_best = valid_kappa
            add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
            add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
            add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name + '.pt', epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))

if __name__ == '__main__':
    batch_size_list = [32]
    lr_list = [0.00015]
    p_horizontalflip_list = [0.4]
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}\n\n\n"".format(batch_size, lr, p_horizontalflip)
                print(info)
                main(batch_size, lr, p_horizontalflip, info)
",,
2019.08.08 01:02:16,new_comp_quadratic_kappa,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model



",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.00015
    weight_decay: 0
)",8.0,5320317.0,29.0,32.0,0.147883,0.285261,0.8643,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            self.df_logger = Logger(self.logsFileName + '.csv', 'df')
            self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.data_type = 'new_comp_quadratic_kappa'

        self.img_size = 224

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = None
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 8
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = ""pytorch"" #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip

        ## PRINT FREQUENCY
        self.print_frequency = 25",4415.18,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_2015 + p + '.png'):
    #     return train_2015 + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image = crop_image_from_gray(image)
        image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_csv = pd.read_csv('./input/train.csv')
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))



    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))

    # Data Processing

    train_path = ""./input/train_images/""

    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed)
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.mean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name + '.pt', epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))

if __name__ == '__main__':
    batch_size_list = [32]
    lr_list = [0.00015]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b0', 'efficientnet-b1', 'efficientnet-b2', 'efficientnet-b3', 'efficientnet-b4', 'efficientnet-b5']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,
2019.08.08 02:15:52,new_comp_quadratic_kappa,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model



",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.00015
    weight_decay: 0
)",8.0,7825953.0,24.0,32.0,0.188381,0.31049499999999997,0.8032,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            self.df_logger = Logger(self.logsFileName + '.csv', 'df')
            self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.data_type = 'new_comp_quadratic_kappa'

        self.img_size = 224

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = None
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 8
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = ""pytorch"" #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip

        ## PRINT FREQUENCY
        self.print_frequency = 25",3660.66,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_2015 + p + '.png'):
    #     return train_2015 + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image = crop_image_from_gray(image)
        image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_csv = pd.read_csv('./input/train.csv')
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))



    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))

    # Data Processing

    train_path = ""./input/train_images/""

    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed)
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.mean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name + '.pt', epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))

if __name__ == '__main__':
    batch_size_list = [32]
    lr_list = [0.00015]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b0', 'efficientnet-b1', 'efficientnet-b2', 'efficientnet-b3', 'efficientnet-b4', 'efficientnet-b5']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,
2019.08.08 03:16:52,new_comp_quadratic_kappa,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model



",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.00015
    weight_decay: 0
)",8.0,9144835.0,30.0,32.0,0.129978,0.284721,0.8528,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            self.df_logger = Logger(self.logsFileName + '.csv', 'df')
            self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.data_type = 'new_comp_quadratic_kappa'

        self.img_size = 224

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = None
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 8
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = ""pytorch"" #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip

        ## PRINT FREQUENCY
        self.print_frequency = 25",4578.67,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_2015 + p + '.png'):
    #     return train_2015 + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image = crop_image_from_gray(image)
        image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_csv = pd.read_csv('./input/train.csv')
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))



    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))

    # Data Processing

    train_path = ""./input/train_images/""

    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed)
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.mean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name + '.pt', epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))

if __name__ == '__main__':
    batch_size_list = [32]
    lr_list = [0.00015]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b0', 'efficientnet-b1', 'efficientnet-b2', 'efficientnet-b3', 'efficientnet-b4', 'efficientnet-b5']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,
2019.08.08 08:03:14,new_comp_quadratic_kappa,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model



",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.00015
    weight_decay: 0
)",8.0,12271145.0,44.0,32.0,0.081768,0.288204,0.8522,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            self.df_logger = Logger(self.logsFileName + '.csv', 'df')
            self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.data_type = 'new_comp_quadratic_kappa'

        self.img_size = 224

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = None
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 8
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = ""pytorch"" #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip

        ## PRINT FREQUENCY
        self.print_frequency = 25",6829.12,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_2015 + p + '.png'):
    #     return train_2015 + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image = crop_image_from_gray(image)
        image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_csv = pd.read_csv('./input/train.csv')
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))



    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))

    # Data Processing

    train_path = ""./input/train_images/""

    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed)
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.mean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name + '.pt', epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))

if __name__ == '__main__':
    batch_size_list = [32]
    lr_list = [0.00015]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b3', 'efficientnet-b4', 'efficientnet-b5']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,
2019.08.08 14:12:38,new_comp_quadratic_kappa,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model



",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.00015
    weight_decay: 0
)",8.0,19385673.0,22.0,16.0,0.20179,0.269436,0.8448,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            self.df_logger = Logger(self.logsFileName + '.csv', 'df')
            self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.data_type = 'new_comp_quadratic_kappa'

        self.img_size = 224

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = None
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 8
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = ""pytorch"" #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip

        ## PRINT FREQUENCY
        self.print_frequency = 25",3445.98,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_2015 + p + '.png'):
    #     return train_2015 + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image = crop_image_from_gray(image)
        image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_csv = pd.read_csv('./input/train.csv')
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))



    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))

    # Data Processing

    train_path = ""./input/train_images/""

    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed)
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.mean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name + '.pt', epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))

if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [0.00015]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b4', 'efficientnet-b5']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,
2019.08.08 15:10:05,new_comp_quadratic_kappa,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model



",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.00015
    weight_decay: 0
)",8.0,30439985.0,32.0,16.0,0.095233,0.28225,0.8564,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            self.df_logger = Logger(self.logsFileName + '.csv', 'df')
            self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.data_type = 'new_comp_quadratic_kappa'

        self.img_size = 224

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = None
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 8
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = ""pytorch"" #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip

        ## PRINT FREQUENCY
        self.print_frequency = 25",5032.83,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_2015 + p + '.png'):
    #     return train_2015 + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image = crop_image_from_gray(image)
        image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_csv = pd.read_csv('./input/train.csv')
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))



    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))

    # Data Processing

    train_path = ""./input/train_images/""

    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed)
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.mean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name + '.pt', epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))

if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [0.00015]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b4', 'efficientnet-b5']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,
2019.08.08 19:23:20,new_comp_quadratic_kappa,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model



",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.00015
    weight_decay: 0
)",8.0,5320317.0,100.0,32.0,0.47047,0.452932,0.7693,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            self.df_logger = Logger(self.logsFileName + '.csv', 'df')
            self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.data_type = 'new_comp_quadratic_kappa'

        self.img_size = 224

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = None
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 8
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = ""pytorch"" #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip

        ## PRINT FREQUENCY
        self.print_frequency = 25",,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_2015 + p + '.png'):
    #     return train_2015 + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image = crop_image_from_gray(image)
        image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_csv = pd.read_csv('./input/train.csv')
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))



    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))

    # Data Processing

    train_path = ""./input/train_images/""

    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed)
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.mean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name + '.pt', epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))

if __name__ == '__main__':
    batch_size_list = [32]
    lr_list = [0.00015]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b0', 'efficientnet-b1', 'efficientnet-b2', 'efficientnet-b3']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,
2019.08.08 19:30:39,new_comp_quadratic_kappa,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model



",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.00015
    weight_decay: 0
)",8.0,5320317.0,17.0,32.0,1.131912,1.052329,0.0224,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            self.df_logger = Logger(self.logsFileName + '.csv', 'df')
            self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.data_type = 'new_comp_quadratic_kappa'

        self.img_size = 256

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = None
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 8
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = ""pytorch"" #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip

        ## PRINT FREQUENCY
        self.print_frequency = 25",324.11,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_2015 + p + '.png'):
    #     return train_2015 + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image = crop_image_from_gray(image)
        image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_csv = pd.read_csv('./input/train.csv')
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))



    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))

    # Data Processing

    train_path = ""./input/train_images/""

    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed)
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.mean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name + '.pt', epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))

if __name__ == '__main__':
    batch_size_list = [32]
    lr_list = [0.00015]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b0', 'efficientnet-b1', 'efficientnet-b2', 'efficientnet-b3']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,
2019.08.09 00:29:38,new_old_mixed,efficientnet-b5 with one FC,,"Adam, lr = 0.001",5.0,28342833.0,9.0,16.0,0.419599,0.423766,,,,,,,19910.0,
2019.08.09 17:40:21,new_old_mixed,efficientnet-b5 with one FC,MSELoss(),"Adam, lr = 0.001",5.0,30442033.0,25.0,16.0,0.43094399999999994,0.42818900000000004,,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            self.df_logger = Logger(self.logsFileName + '.csv', 'df')
            self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
            self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'new_comp_quadratic_kappa'

        self.img_size = 256

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = None
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 5
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = ""pytorch"" #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 25
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.3
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new_old_mixed'

        ## PRINT FREQUENCY
        self.print_frequency = 50",21231.47,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image = crop_image_from_gray(image)
        image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))

    # Data Processing


    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed)
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.mean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name + '.pt', epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()


if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [0.001]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b5']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",35557.0,
2019.08.10 04:36:38,new_old_mixed,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.BatchNorm1d(in_features),
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1, bias=True),
            )
            model.cuda()
            self.model = model



",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0
)",5.0,28346929.0,15.0,16.0,0.516158,0.449281,,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            self.df_logger = Logger(self.logsFileName + '.csv', 'df')
            self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
            self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'new_comp_quadratic_kappa'

        self.img_size = 256

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = None
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 5
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = ""pytorch"" #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 25
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.3
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new_old_mixed'

        ## PRINT FREQUENCY
        self.print_frequency = 50",21231.47,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image = crop_image_from_gray(image)
        image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))

    # Data Processing


    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed)
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.mean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name + '.pt', epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()


if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [0.001]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b5']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,
2019.08.10 12:28:42,new_old_mixed,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.BatchNorm1d(in_features),
                nn.Linear(in_features=in_features, out_features=1, bias=True),
            )
            model.cuda()
            self.model = model



",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0
)",5.0,28346929.0,25.0,16.0,0.856866,0.839957,0.4322,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            self.df_logger = Logger(self.logsFileName + '.csv', 'df')
            self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile) + 1) + '_end_epoch'
            self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'new_comp_quadratic_kappa'

        self.img_size = 256

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = None
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 5
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = ""pytorch"" #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 25
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.3
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new_old_mixed'

        ## PRINT FREQUENCY
        self.print_frequency = 50",,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image = crop_image_from_gray(image)
        image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))

    # Data Processing


    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed)
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.mean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name + '.pt', epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()


if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [0.001]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b5']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,
2019.08.10 21:12:27,new_old_mixed,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
            )
            model.cuda()
            self.model = model



",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.006
    lr: 0.006
    weight_decay: 0
)",5.0,28342833.0,12.0,16.0,0.543704,0.5195960000000001,,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            self.df_logger = Logger(self.logsFileName + '.csv', 'df')
            self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
            self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'new_comp_quadratic_kappa'

        self.img_size = 256

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(self.optimizer, 5)
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 5
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = ""pytorch"" #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 25
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.3
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new_old_mixed'

        ## PRINT FREQUENCY
        self.print_frequency = 50",17024.91,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image = crop_image_from_gray(image)
        image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))

    # Data Processing


    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed)
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.mean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            cfg.scheduler.step()

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name + '.pt', epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()


if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [0.006]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b5']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,<torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7efb95f979e8>
2019.08.11 15:06:48,new_old_mixed,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model



",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.003
    weight_decay: 0
)",7.0,28342833.0,25.0,16.0,0.5447029999999999,0.575419,,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'mixed_data_finetuning_43th_model'

        self.img_size = 256

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 7
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = ""pytorch"" #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 25
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.3
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new_old_mixed'

        ## PRINT FREQUENCY
        self.print_frequency = 50",,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image = crop_image_from_gray(image)
        image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))

    # Data Processing


    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed)
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.mean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            cfg.scheduler.step(valid_loss_epoch)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()


if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [0.003]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b5']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb975bfac8>
2019.08.11 17:45:02,new_old_mixed_ben_preprocessing,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model



",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.003
    weight_decay: 0
)",7.0,28342833.0,25.0,16.0,0.577896,0.685317,0.3629,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'mixed_data_finetuning_43th_model'

        self.img_size = 256

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 7
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = ""pytorch"" #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 25
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.3
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new_old_mixed_ben_preprocessing'

        ## PRINT FREQUENCY
        self.print_frequency = 50",,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        # image = crop_image_from_gray(image)
        # image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        # image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))

    # Data Processing


    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed)
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.mean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            cfg.scheduler.step(valid_loss_epoch)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()


if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [0.003]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b5']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb95e8fe80>
2019.08.11 17:56:08,new_old_mixed_ben_preprocessing,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model



",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.003
    weight_decay: 0
)",8.0,28342833.0,30.0,16.0,0.577896,0.685317,0.3629,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'mixed_data_finetuning_43th_model'

        self.img_size = 224

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 8
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = ""pytorch"" #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 30
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.3
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new_old_mixed_ben_preprocessing'

        ## PRINT FREQUENCY
        self.print_frequency = 50",,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        # image = crop_image_from_gray(image)
        # image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        # image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))

    # Data Processing


    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed)
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.mean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            cfg.scheduler.step(valid_loss_epoch)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()


if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [0.003, 0.01, 0.1]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b5']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb95e8fe48>
2019.08.11 18:06:15,new_old_mixed_ben_preprocessing,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model



",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.003
    weight_decay: 0
)",8.0,28342833.0,30.0,16.0,0.297677,0.344714,0.6718,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'mixed_data_finetuning_43th_model'

        self.img_size = 224

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 8
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = ""pytorch"" #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 30
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.3
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new_old_mixed_ben_preprocessing'

        ## PRINT FREQUENCY
        self.print_frequency = 50",13852.03,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        # image = crop_image_from_gray(image)
        # image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        # image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))

    # Data Processing


    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed)
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            cfg.scheduler.step(valid_loss_epoch)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()


if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [0.003, 0.01, 0.1]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b5']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb95e8feb8>
2019.08.12 00:10:31,new_old_mixed_ben_preprocessing,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model



",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0003
    weight_decay: 0
)",8.0,28342833.0,30.0,16.0,0.280158,0.350274,0.7403,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'mixed_data_finetuning_51th_model_img_size_180'

        self.img_size = 180

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 8
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = ""pytorch"" #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 30
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.3
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new_old_mixed_ben_preprocessing'

        ## PRINT FREQUENCY
        self.print_frequency = 50",,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        # image = crop_image_from_gray(image)
        # image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        # image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))

    # Data Processing


    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed)
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            cfg.scheduler.step(valid_loss_epoch)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()


if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [0.0003]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b5']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb95e8fe80>
