date,data-type,net-architecture,loss-func,optim,early-stopping-patience,parameters-amount,n-epochs,batch-size,best-train-loss,best-valid-loss,best-kappa,cfg,time_estimated,lb-kappa-score,dataset,trainloop,time-estimated,scheduler
2019.08.05 23:20:37,,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models

import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024, bias=True),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            self.model = model

",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.00015
    weight_decay: 0
)",8.0,44599361.0,10.0,32.0,0.34075300000000003,0.329401,0.6176,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

class Config:
    def __init__(self):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.seed = 13
        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            self.df_logger = Logger(self.logsFileName + '.csv', 'df')
            self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'



        ## MODEL PARAMETERS
        self.model_type = 'ResNet'

        self.model = MainModel(model_type=self.model_type).model
        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = 0.00015
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = None
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 8
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 12
        self.n_epochs = 15
        self.batch_size = 32
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = 0.4

        ## PRINT FREQUENCY
        self.print_frequency = 25
",327.82,,,,,
2019.08.05 23:29:59,,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models

import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024, bias=True),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            self.model = model

",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.00015
    weight_decay: 0
)",8.0,44599361.0,15.0,32.0,0.12393399999999999,0.298256,0.6095,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

class Config:
    def __init__(self):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.seed = 13
        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            self.df_logger = Logger(self.logsFileName + '.csv', 'df')
            self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'



        ## MODEL PARAMETERS
        self.model_type = 'ResNet'

        self.model = MainModel(model_type=self.model_type).model
        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = 0.00015
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = None
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 8
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'kappa' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 12
        self.n_epochs = 15
        self.batch_size = 32
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = 0.4

        ## PRINT FREQUENCY
        self.print_frequency = 25
",,,,,,
2019.08.05 23:43:43,new_comp,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models

import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024, bias=True),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            self.model = model

",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.00015
    weight_decay: 0
)",10.0,44599361.0,20.0,32.0,0.073034,0.316579,0.637,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

class Config:
    def __init__(self):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.seed = 13
        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            self.df_logger = Logger(self.logsFileName + '.csv', 'df')
            self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.data_type = 'new_comp'


        ## MODEL PARAMETERS
        self.model_type = 'ResNet'

        self.model = MainModel(model_type=self.model_type).model
        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = 0.00015
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = None
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 10
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #pytorch

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 12
        self.n_epochs = 50
        self.batch_size = 32
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = 0.4

        ## PRINT FREQUENCY
        self.print_frequency = 25
",690.71,,,,,
2019.08.05 23:55:42,new_comp,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models

import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024, bias=True),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            self.model = model

",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.00015
    weight_decay: 0
)",10.0,44599361.0,21.0,32.0,0.136981,0.303939,0.6071,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

class Config:
    def __init__(self):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.seed = 13
        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            self.df_logger = Logger(self.logsFileName + '.csv', 'df')
            self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.data_type = 'new_comp'


        ## MODEL PARAMETERS
        self.model_type = 'ResNet'

        self.model = MainModel(model_type=self.model_type).model
        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = 0.00015
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = None
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 10
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'kappa' #pytorch

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 12
        self.n_epochs = 50
        self.batch_size = 32
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = 0.4

        ## PRINT FREQUENCY
        self.print_frequency = 25
",717.39,,,,,
2019.08.06 08:11:27,new_comp,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models

import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024, bias=True),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            self.model = model

",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.00015
    weight_decay: 0
)",15.0,44599361.0,26.0,1.0,0.337197,1.329923,,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            self.df_logger = Logger(self.logsFileName + '.csv', 'df')
            self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.data_type = 'new_comp'


        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.model_type = 'ResNet'

        self.model = MainModel(model_type=self.model_type).model
        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = 0.00015
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = None
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 15
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 12
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = 0.4

        ## PRINT FREQUENCY
        self.print_frequency = 25
",5222.55,,,,,
2019.08.06 09:38:31,new_comp,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models

import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024, bias=True),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            self.model = model

",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.00015
    weight_decay: 0
)",15.0,44599361.0,24.0,2.0,0.30700900000000003,0.430778,,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            self.df_logger = Logger(self.logsFileName + '.csv', 'df')
            self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.data_type = 'new_comp'


        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.model_type = 'ResNet'

        self.model = MainModel(model_type=self.model_type).model
        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = 0.00015
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = None
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 15
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 12
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = 0.4

        ## PRINT FREQUENCY
        self.print_frequency = 25
",2654.21,,,,,
2019.08.06 10:22:46,new_comp,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models

import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024, bias=True),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            self.model = model

",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.00015
    weight_decay: 0
)",15.0,44599361.0,28.0,4.0,0.164537,0.325475,,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            self.df_logger = Logger(self.logsFileName + '.csv', 'df')
            self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.data_type = 'new_comp'


        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.model_type = 'ResNet'

        self.model = MainModel(model_type=self.model_type).model
        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = 0.00015
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = None
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 15
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 12
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = 0.4

        ## PRINT FREQUENCY
        self.print_frequency = 25
",1850.02,,,,,
2019.08.06 10:53:37,new_comp,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models

import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024, bias=True),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            self.model = model

",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.00015
    weight_decay: 0
)",15.0,44599361.0,69.0,8.0,0.032534,0.287978,,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            self.df_logger = Logger(self.logsFileName + '.csv', 'df')
            self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.data_type = 'new_comp'


        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.model_type = 'ResNet'

        self.model = MainModel(model_type=self.model_type).model
        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = 0.00015
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = None
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 15
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 12
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = 0.4

        ## PRINT FREQUENCY
        self.print_frequency = 25
",3120.4,,,,,
2019.08.06 11:45:38,new_comp,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models

import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024, bias=True),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            self.model = model

",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.00015
    weight_decay: 0
)",15.0,44599361.0,38.0,16.0,0.046238,0.29125700000000004,0.6244,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            self.df_logger = Logger(self.logsFileName + '.csv', 'df')
            self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.data_type = 'new_comp'


        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.model_type = 'ResNet'

        self.model = MainModel(model_type=self.model_type).model
        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = 0.00015
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = None
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 15
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 12
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = 0.4

        ## PRINT FREQUENCY
        self.print_frequency = 25
",1351.18,,,,,
2019.08.06 12:08:10,new_comp,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models

import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024, bias=True),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            self.model = model

",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.00015
    weight_decay: 0
)",15.0,44599361.0,37.0,32.0,0.031294,0.27269099999999996,0.6325,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            self.df_logger = Logger(self.logsFileName + '.csv', 'df')
            self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.data_type = 'new_comp'


        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.model_type = 'ResNet'

        self.model = MainModel(model_type=self.model_type).model
        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = 0.00015
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = None
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 15
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 12
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = 0.4

        ## PRINT FREQUENCY
        self.print_frequency = 25
",1212.47,,,,,
2019.08.06 12:28:23,new_comp,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models

import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024, bias=True),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            self.model = model

",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.00015
    weight_decay: 0
)",15.0,44599361.0,36.0,48.0,0.124594,0.304088,0.5833,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            self.df_logger = Logger(self.logsFileName + '.csv', 'df')
            self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.data_type = 'new_comp'


        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.model_type = 'ResNet'

        self.model = MainModel(model_type=self.model_type).model
        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = 0.00015
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = None
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 15
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 12
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = 0.4

        ## PRINT FREQUENCY
        self.print_frequency = 25
",1226.16,,,,,
2019.08.06 12:48:50,new_comp,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models

import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024, bias=True),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            self.model = model

",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.00015
    weight_decay: 0
)",15.0,44599361.0,39.0,64.0,0.021981,0.263108,0.6509999999999999,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            self.df_logger = Logger(self.logsFileName + '.csv', 'df')
            self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.data_type = 'new_comp'


        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.model_type = 'ResNet'

        self.model = MainModel(model_type=self.model_type).model
        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = 0.00015
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = None
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 15
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 12
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = 0.4

        ## PRINT FREQUENCY
        self.print_frequency = 25
",1294.01,,,,,
2019.08.06 17:06:42,new_comp,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models

import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024, bias=True),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            self.model = model

",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.1
    weight_decay: 0
)",15.0,44599361.0,20.0,64.0,271.764497,89.030222,0.0029,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            self.df_logger = Logger(self.logsFileName + '.csv', 'df')
            self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.data_type = 'new_comp'


        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.model_type = 'ResNet'

        self.model = MainModel(model_type=self.model_type).model
        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = None
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 15
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 12
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = 0.4

        ## PRINT FREQUENCY
        self.print_frequency = 25
",662.72,,,,,
2019.08.06 17:17:45,new_comp,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models

import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024, bias=True),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            self.model = model

",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.01
    weight_decay: 0
)",15.0,44599361.0,63.0,64.0,0.466971,0.53052,0.2794,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            self.df_logger = Logger(self.logsFileName + '.csv', 'df')
            self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.data_type = 'new_comp'


        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.model_type = 'ResNet'

        self.model = MainModel(model_type=self.model_type).model
        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = None
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 15
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 12
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = 0.4

        ## PRINT FREQUENCY
        self.print_frequency = 25
",2089.43,,,,,
2019.08.06 17:52:35,new_comp,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models

import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024, bias=True),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            self.model = model

",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0
)",15.0,44599361.0,61.0,64.0,0.05952,0.324779,0.5353,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            self.df_logger = Logger(self.logsFileName + '.csv', 'df')
            self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.data_type = 'new_comp'


        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.model_type = 'ResNet'

        self.model = MainModel(model_type=self.model_type).model
        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = None
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 15
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 12
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = 0.4

        ## PRINT FREQUENCY
        self.print_frequency = 25
",2023.55,,,,,
2019.08.06 18:26:20,new_comp,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models

import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024, bias=True),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            self.model = model

",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)",15.0,44599361.0,54.0,64.0,0.043727999999999996,0.305871,0.588,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            self.df_logger = Logger(self.logsFileName + '.csv', 'df')
            self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.data_type = 'new_comp'


        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.model_type = 'ResNet'

        self.model = MainModel(model_type=self.model_type).model
        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = None
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 15
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 12
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = 0.4

        ## PRINT FREQUENCY
        self.print_frequency = 25
",1793.97,,,,,
2019.08.06 18:56:14,new_comp,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models

import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024, bias=True),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            self.model = model

",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",15.0,44599361.0,61.0,64.0,0.019651,0.259806,0.6612,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            self.df_logger = Logger(self.logsFileName + '.csv', 'df')
            self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.data_type = 'new_comp'


        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.model_type = 'ResNet'

        self.model = MainModel(model_type=self.model_type).model
        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = None
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 15
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 12
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = 0.4

        ## PRINT FREQUENCY
        self.print_frequency = 25
",2032.91,,,,,
2019.08.06 19:30:08,new_comp,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models

import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024, bias=True),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            self.model = model

",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    weight_decay: 0
)",15.0,44599361.0,61.0,64.0,0.023965,0.279988,0.6435,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            self.df_logger = Logger(self.logsFileName + '.csv', 'df')
            self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.data_type = 'new_comp'


        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.model_type = 'ResNet'

        self.model = MainModel(model_type=self.model_type).model
        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = None
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 15
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 12
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = 0.4

        ## PRINT FREQUENCY
        self.print_frequency = 25
",2028.09,,,,,
2019.08.06 20:03:57,new_comp,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models

import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024, bias=True),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            self.model = model

",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 1e-05
    weight_decay: 0
)",15.0,44599361.0,23.0,64.0,0.079316,0.331172,0.6062,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            self.df_logger = Logger(self.logsFileName + '.csv', 'df')
            self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.data_type = 'new_comp'


        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.model_type = 'ResNet'

        self.model = MainModel(model_type=self.model_type).model
        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = None
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 15
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 12
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = 0.4

        ## PRINT FREQUENCY
        self.print_frequency = 25
",766.31,,,,,
2019.08.06 21:08:37,new_comp,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models

import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024, bias=True),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            self.model = model

",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.00015
    weight_decay: 0
)",15.0,44599361.0,39.0,64.0,0.021981,0.263108,0.6509999999999999,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            self.df_logger = Logger(self.logsFileName + '.csv', 'df')
            self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.data_type = 'new_comp'


        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.model_type = 'ResNet'

        self.model = MainModel(model_type=self.model_type).model
        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = None
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 15
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 12
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip

        ## PRINT FREQUENCY
        self.print_frequency = 25
",1318.98,,,,,
2019.08.06 21:30:37,new_comp,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models

import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024, bias=True),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            self.model = model

",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.00015
    weight_decay: 0
)",15.0,44599361.0,39.0,64.0,0.021981,0.263108,0.6509999999999999,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            self.df_logger = Logger(self.logsFileName + '.csv', 'df')
            self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.data_type = 'new_comp'


        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.model_type = 'ResNet'

        self.model = MainModel(model_type=self.model_type).model
        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = None
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 15
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 12
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip

        ## PRINT FREQUENCY
        self.print_frequency = 25
",1320.6,,,,,
2019.08.06 22:00:46,new_comp_quadratic_kappa,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models

import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024, bias=True),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            self.model = model

",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.00015
    weight_decay: 0
)",15.0,44599361.0,39.0,64.0,0.021981,0.263108,0.8797,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            self.df_logger = Logger(self.logsFileName + '.csv', 'df')
            self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.data_type = 'new_comp_quadratic_kappa'


        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.model_type = 'ResNet'

        self.model = MainModel(model_type=self.model_type).model
        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = None
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 15
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 12
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip

        ## PRINT FREQUENCY
        self.print_frequency = 25
",1318.7,,,,,
2019.08.06 22:28:37,new_comp_quadratic_kappa,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models

import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024, bias=True),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            self.model = model

",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.00015
    weight_decay: 0
)",15.0,44599361.0,100.0,64.0,0.803977,1.546088,0.3728,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            self.df_logger = Logger(self.logsFileName + '.csv', 'df')
            self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.data_type = 'new_comp_quadratic_kappa'

        self.img_size = 224

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.model_type = 'ResNet'

        self.model = MainModel(model_type=self.model_type).model
        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = None
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 15
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 12
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip

        ## PRINT FREQUENCY
        self.print_frequency = 25
",,,,,,
2019.08.06 22:35:19,new_comp_quadratic_kappa,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models

import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

def _resnext(path, block, layers, pretrained, progress, **kwargs):
    model = ResNet(block, layers, **kwargs)
    model.load_state_dict(torch.load(path))
    return model

def resnext101_32x8d_wsl(path, progress=True, **kwargs):
    """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
    and finetuned on ImageNet from Figure 5 in
    `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
    Args:
        progress (bool): If True, displays a progress bar of the download to stderr.
    """"""
    kwargs['groups'] = 32
    kwargs['width_per_group'] = 8
    return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            for param in model.parameters():
                param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024, bias=True),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            self.model = model
        elif model_type == 'ResNext':
            model = resnext101_32x8d_wsl(path='./input/pretrained_models/ig_resnext101_32x8-c38310e5.pth')

            for param in model.parameters():
                param.requires_grad = False

            model.fc = torch.nn.Linear(2048, 1)


",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.00015
    weight_decay: 0
)",15.0,2099201.0,59.0,64.0,0.390875,0.448205,0.7998,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            self.df_logger = Logger(self.logsFileName + '.csv', 'df')
            self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.data_type = 'new_comp_quadratic_kappa'

        self.img_size = 224

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.model_type = 'ResNet'

        self.model = MainModel(model_type=self.model_type).model
        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = None
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 15
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 12
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip

        ## PRINT FREQUENCY
        self.print_frequency = 25
",1802.03,,,,,
2019.08.06 23:37:58,new_comp_quadratic_kappa,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models

import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            for param in model.parameters():
                param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024, bias=True),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            self.model = model
        elif model_type == 'ResNext101_32x16d':
            self.model = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x16d_wsl')

    def get_model(self):
        return self.model

",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.00015
    weight_decay: 0
)",15.0,2049.0,100.0,64.0,0.509555,0.551465,0.7007,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            self.df_logger = Logger(self.logsFileName + '.csv', 'df')
            self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.data_type = 'new_comp_quadratic_kappa'

        self.img_size = 256

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.model_type = 'ResNext101_32x16d'

        self.model = MainModel(model_type=self.model_type).get_model()
        if self.model_type == 'ResNext101_32x16d':
            for param in self.model.parameters():
                param.requires_grad = False
            self.model.fc = torch.nn.Linear(2048, 1)

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = None
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 15
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = ""kappa"" #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 12
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip

        ## PRINT FREQUENCY
        self.print_frequency = 25
",,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from config import Config


cfg = Config()

transforms_train = transforms.Compose([
    transforms.ToPILImage(),
    transforms.Resize((cfg.img_size, cfg.img_size)),
    transforms.RandomHorizontalFlip(p=cfg.p_horizontalflip),
    #transforms.ColorJitter(brightness=2, contrast=2),
    transforms.ToTensor(),
    transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))
])

transforms_valid = transforms.Compose([
    transforms.ToPILImage(),
    transforms.Resize((cfg.img_size, cfg.img_size)),
    transforms.ToTensor(),
    transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))
])


class CreateDataset(Dataset):
    def __init__(self, df_data, data_dir='./input/', transform=None):
        super().__init__()
        self.df = df_data.values
        self.data_dir = data_dir
        self.transform = transform

    def __len__(self):
        return len(self.df)

    def __getitem__(self, index):
        img_name, label = self.df[index]
        img_path = os.path.join(self.data_dir, img_name + '.png')
        image = cv2.imread(img_path)
        if self.transform is not None:
            image = self.transform(image)
        return image, label","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from config import Config
from train_dataset import transforms_train, transforms_valid, CreateDataset
from model import MainModel
from logger import Logger

# Open source libs


def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_csv = pd.read_csv('./input/train.csv')
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))



    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))

    # Data Processing

    train_path = ""./input/train_images/""

    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed)
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.mean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## LOGGINS LOSSES
        if valid_loss_best > valid_loss_epoch:
            valid_loss_best  = valid_loss_epoch
            train_loss_best = train_loss_epoch
            kappa_best = valid_kappa
            add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
            add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
            add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name + '.pt', epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))

if __name__ == '__main__':
    batch_size_list = [64]
    lr_list = [0.00015]
    p_horizontalflip_list = [0.4]
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}\n\n\n"".format(batch_size, lr, p_horizontalflip)
                print(info)
                main(batch_size, lr, p_horizontalflip, info)
",,
2019.08.06 23:52:23,new_comp_quadratic_kappa,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models

import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            for param in model.parameters():
                param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024, bias=True),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            self.model = model
        elif model_type == 'ResNext101_32x16d':
            self.model = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x16d_wsl')

    def get_model(self):
        return self.model

",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.00015
    weight_decay: 0
)",15.0,2049.0,78.0,64.0,0.35261,0.49415,0.7726,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            self.df_logger = Logger(self.logsFileName + '.csv', 'df')
            self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.data_type = 'new_comp_quadratic_kappa'

        self.img_size = 256

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.model_type = 'ResNext101_32x16d'

        self.model = MainModel(model_type=self.model_type).get_model()
        if self.model_type == 'ResNext101_32x16d':
            for param in self.model.parameters():
                param.requires_grad = False
            self.model.fc = torch.nn.Linear(2048, 1)

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = None
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 15
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = ""pytorch"" #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 12
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip

        ## PRINT FREQUENCY
        self.print_frequency = 25
",3647.06,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from config import Config


cfg = Config()

transforms_train = transforms.Compose([
    transforms.ToPILImage(),
    transforms.Resize((cfg.img_size, cfg.img_size)),
    # transforms.RandomHorizontalFlip(p=cfg.p_horizontalflip),
    #transforms.ColorJitter(brightness=2, contrast=2),
    transforms.ToTensor(),
    transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))
])

transforms_valid = transforms.Compose([
    transforms.ToPILImage(),
    transforms.Resize((cfg.img_size, cfg.img_size)),
    transforms.ToTensor(),
    transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))
])


class CreateDataset(Dataset):
    def __init__(self, df_data, data_dir='./input/', transform=None):
        super().__init__()
        self.df = df_data.values
        self.data_dir = data_dir
        self.transform = transform

    def __len__(self):
        return len(self.df)

    def __getitem__(self, index):
        img_name, label = self.df[index]
        img_path = os.path.join(self.data_dir, img_name + '.png')
        image = cv2.imread(img_path)
        if self.transform is not None:
            image = self.transform(image)
        return image, label","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from config import Config
from train_dataset import transforms_train, transforms_valid, CreateDataset
from model import MainModel
from logger import Logger

# Open source libs


def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_csv = pd.read_csv('./input/train.csv')
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))



    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))

    # Data Processing

    train_path = ""./input/train_images/""

    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed)
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.mean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## LOGGINS LOSSES
        if valid_loss_best > valid_loss_epoch:
            valid_loss_best  = valid_loss_epoch
            train_loss_best = train_loss_epoch
            kappa_best = valid_kappa
            add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
            add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
            add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name + '.pt', epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))

if __name__ == '__main__':
    batch_size_list = [64]
    lr_list = [0.00015]
    p_horizontalflip_list = [0.4]
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}\n\n\n"".format(batch_size, lr, p_horizontalflip)
                print(info)
                main(batch_size, lr, p_horizontalflip, info)
",,
2019.08.07 01:40:35,new_comp_quadratic_kappa,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models

import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            for param in model.parameters():
                param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024, bias=True),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            self.model = model
        elif model_type == 'ResNext101_32x16d':
            self.model = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x16d_wsl')

    def get_model(self):
        return self.model

",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.00015
    weight_decay: 0
)",15.0,2099201.0,100.0,64.0,0.56591,0.529179,0.7258,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            self.df_logger = Logger(self.logsFileName + '.csv', 'df')
            self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.data_type = 'new_comp_quadratic_kappa'

        self.img_size = 256

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.model_type = 'ResNet101'

        self.model = MainModel(model_type=self.model_type).get_model()
        if self.model_type == 'ResNext101_32x16d':
            for param in self.model.parameters():
                param.requires_grad = False
            self.model.fc = torch.nn.Linear(2048, 1)

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = None
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 15
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = ""pytorch"" #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 12
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip

        ## PRINT FREQUENCY
        self.print_frequency = 25
",,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_2015 + p + '.png'):
    #     return train_2015 + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image = crop_image_from_gray(image)
        image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])
","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from config import Config
from train_dataset import transforms_train, transforms_valid, CreateDataset
from model import MainModel
from logger import Logger

# Open source libs


def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_csv = pd.read_csv('./input/train.csv')
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))



    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))

    # Data Processing

    train_path = ""./input/train_images/""

    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed)
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.mean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## LOGGINS LOSSES
        if valid_loss_best > valid_loss_epoch:
            valid_loss_best  = valid_loss_epoch
            train_loss_best = train_loss_epoch
            kappa_best = valid_kappa
            add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
            add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
            add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name + '.pt', epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))

if __name__ == '__main__':
    batch_size_list = [64]
    lr_list = [0.00015]
    p_horizontalflip_list = [0.4]
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}\n\n\n"".format(batch_size, lr, p_horizontalflip)
                print(info)
                main(batch_size, lr, p_horizontalflip, info)
",,
2019.08.07 02:19:27,new_comp_quadratic_kappa,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models

import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            for param in model.parameters():
                param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024, bias=True),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            self.model = model
        elif model_type == 'ResNext101_32x16d':
            self.model = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x16d_wsl')

    def get_model(self):
        return self.model

",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.00015
    weight_decay: 0
)",15.0,2099201.0,100.0,128.0,,0.838298,0.5452,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            self.df_logger = Logger(self.logsFileName + '.csv', 'df')
            self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.data_type = 'new_comp_quadratic_kappa'

        self.img_size = 256

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.model_type = 'ResNet101'

        self.model = MainModel(model_type=self.model_type).get_model()
        if self.model_type == 'ResNext101_32x16d':
            for param in self.model.parameters():
                param.requires_grad = False
            self.model.fc = torch.nn.Linear(2048, 1)

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = None
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 15
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = ""pytorch"" #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip

        ## PRINT FREQUENCY
        self.print_frequency = 25
",,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_2015 + p + '.png'):
    #     return train_2015 + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image = crop_image_from_gray(image)
        image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])
","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from config import Config
from train_dataset import transforms_train, transforms_valid, CreateDataset
from model import MainModel
from logger import Logger

# Open source libs


def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_csv = pd.read_csv('./input/train.csv')
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))



    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))

    # Data Processing

    train_path = ""./input/train_images/""

    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed)
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.mean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## LOGGINS LOSSES
        if valid_loss_best > valid_loss_epoch:
            valid_loss_best  = valid_loss_epoch
            train_loss_best = train_loss_epoch
            kappa_best = valid_kappa
            add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
            add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
            add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name + '.pt', epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))

if __name__ == '__main__':
    batch_size_list = [128]
    lr_list = [0.00015]
    p_horizontalflip_list = [0.4]
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}\n\n\n"".format(batch_size, lr, p_horizontalflip)
                print(info)
                main(batch_size, lr, p_horizontalflip, info)
",,
2019.08.07 02:23:27,new_comp_quadratic_kappa,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models

import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            for param in model.parameters():
                param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024, bias=True),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            self.model = model
        elif model_type == 'ResNext101_32x16d':
            self.model = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x16d_wsl')

    def get_model(self):
        return self.model

",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.00015
    weight_decay: 0
)",15.0,2099201.0,72.0,32.0,0.485065,0.485729,0.7666,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            self.df_logger = Logger(self.logsFileName + '.csv', 'df')
            self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.data_type = 'new_comp_quadratic_kappa'

        self.img_size = 256

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.model_type = 'ResNet101'

        self.model = MainModel(model_type=self.model_type).get_model()
        if self.model_type == 'ResNext101_32x16d':
            for param in self.model.parameters():
                param.requires_grad = False
            self.model.fc = torch.nn.Linear(2048, 1)

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = None
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 15
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = ""pytorch"" #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip

        ## PRINT FREQUENCY
        self.print_frequency = 25
",11631.53,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_2015 + p + '.png'):
    #     return train_2015 + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image = crop_image_from_gray(image)
        image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])
","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from config import Config
from train_dataset import transforms_train, transforms_valid, CreateDataset
from model import MainModel
from logger import Logger

# Open source libs


def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_csv = pd.read_csv('./input/train.csv')
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))



    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))

    # Data Processing

    train_path = ""./input/train_images/""

    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed)
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.mean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## LOGGINS LOSSES
        if valid_loss_best > valid_loss_epoch:
            valid_loss_best  = valid_loss_epoch
            train_loss_best = train_loss_epoch
            kappa_best = valid_kappa
            add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
            add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
            add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name + '.pt', epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))

if __name__ == '__main__':
    batch_size_list = [32]
    lr_list = [0.00015]
    p_horizontalflip_list = [0.4]
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}\n\n\n"".format(batch_size, lr, p_horizontalflip)
                print(info)
                main(batch_size, lr, p_horizontalflip, info)
",,
2019.08.07 08:44:40,new_comp_quadratic_kappa,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models

import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            for param in model.parameters():
                param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024, bias=True),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            self.model = model
        elif model_type == 'ResNext101_32x16d':
            self.model = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x16d_wsl')

    def get_model(self):
        return self.model

",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.00015
    weight_decay: 0
)",8.0,2099201.0,50.0,512.0,2.464634,2.461174,0.0038,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            self.df_logger = Logger(self.logsFileName + '.csv', 'df')
            self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.data_type = 'new_comp_quadratic_kappa'

        self.img_size = 256

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.model_type = 'ResNet101'

        self.model = MainModel(model_type=self.model_type).get_model()
        if self.model_type == 'ResNext101_32x16d':
            for param in self.model.parameters():
                param.requires_grad = False
            self.model.fc = torch.nn.Linear(2048, 1)

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = None
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 8
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = ""pytorch"" #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 50
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip

        ## PRINT FREQUENCY
        self.print_frequency = 25
",,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_2015 + p + '.png'):
    #     return train_2015 + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image = crop_image_from_gray(image)
        image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])
","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from config import Config
from train_dataset import transforms_train, transforms_valid, CreateDataset
from model import MainModel
from logger import Logger

# Open source libs


def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_csv = pd.read_csv('./input/train.csv')
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))



    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))

    # Data Processing

    train_path = ""./input/train_images/""

    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed)
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.mean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## LOGGINS LOSSES
        if valid_loss_best > valid_loss_epoch:
            valid_loss_best  = valid_loss_epoch
            train_loss_best = train_loss_epoch
            kappa_best = valid_kappa
            add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
            add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
            add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name + '.pt', epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))

if __name__ == '__main__':
    batch_size_list = [512]
    lr_list = [0.00015]
    p_horizontalflip_list = [0.4]
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}\n\n\n"".format(batch_size, lr, p_horizontalflip)
                print(info)
                main(batch_size, lr, p_horizontalflip, info)
",,
2019.08.07 08:49:26,new_comp_quadratic_kappa,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models

import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            for param in model.parameters():
                param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024, bias=True),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            self.model = model
        elif model_type == 'ResNext101_32x16d':
            self.model = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x16d_wsl')

    def get_model(self):
        return self.model

",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.00015
    weight_decay: 0
)",8.0,2099201.0,50.0,64.0,0.658462,0.602823,0.6959,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            self.df_logger = Logger(self.logsFileName + '.csv', 'df')
            self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.data_type = 'new_comp_quadratic_kappa'

        self.img_size = 256

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.model_type = 'ResNet101'

        self.model = MainModel(model_type=self.model_type).get_model()
        if self.model_type == 'ResNext101_32x16d':
            for param in self.model.parameters():
                param.requires_grad = False
            self.model.fc = torch.nn.Linear(2048, 1)

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = None
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 8
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = ""pytorch"" #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 50
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip

        ## PRINT FREQUENCY
        self.print_frequency = 25
",,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_2015 + p + '.png'):
    #     return train_2015 + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image = crop_image_from_gray(image)
        image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])
","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from config import Config
from train_dataset import transforms_train, transforms_valid, CreateDataset
from model import MainModel
from logger import Logger

# Open source libs


def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_csv = pd.read_csv('./input/train.csv')
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))



    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))

    # Data Processing

    train_path = ""./input/train_images/""

    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed)
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.mean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## LOGGINS LOSSES
        if valid_loss_best > valid_loss_epoch:
            valid_loss_best  = valid_loss_epoch
            train_loss_best = train_loss_epoch
            kappa_best = valid_kappa
            add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
            add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
            add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name + '.pt', epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))

if __name__ == '__main__':
    batch_size_list = [64]
    lr_list = [0.00015]
    p_horizontalflip_list = [0.4]
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}\n\n\n"".format(batch_size, lr, p_horizontalflip)
                print(info)
                main(batch_size, lr, p_horizontalflip, info)
",,
2019.08.07 09:02:11,new_comp_quadratic_kappa,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models

import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024, bias=True),
                # nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            self.model = model
        elif model_type == 'ResNext101_32x16d':
            self.model = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x16d_wsl')

    def get_model(self):
        return self.model

",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.00015
    weight_decay: 0
)",8.0,44599361.0,27.0,32.0,0.195692,0.25443899999999997,0.8735,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            self.df_logger = Logger(self.logsFileName + '.csv', 'df')
            self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.data_type = 'new_comp_quadratic_kappa'

        self.img_size = 256

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.model_type = 'ResNet101'

        self.model = MainModel(model_type=self.model_type).get_model()
        if self.model_type == 'ResNext101_32x16d':
            for param in self.model.parameters():
                param.requires_grad = False
            self.model.fc = torch.nn.Linear(2048, 1)

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = None
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 8
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = ""pytorch"" #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 50
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip

        ## PRINT FREQUENCY
        self.print_frequency = 25
",4407.88,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_2015 + p + '.png'):
    #     return train_2015 + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image = crop_image_from_gray(image)
        image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])
","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from config import Config
from train_dataset import transforms_train, transforms_valid, CreateDataset
from model import MainModel
from logger import Logger

# Open source libs


def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_csv = pd.read_csv('./input/train.csv')
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))



    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))

    # Data Processing

    train_path = ""./input/train_images/""

    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed)
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.mean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## LOGGINS LOSSES
        if valid_loss_best > valid_loss_epoch:
            valid_loss_best  = valid_loss_epoch
            train_loss_best = train_loss_epoch
            kappa_best = valid_kappa
            add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
            add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
            add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name + '.pt', epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))

if __name__ == '__main__':
    batch_size_list = [32]
    lr_list = [0.00015]
    p_horizontalflip_list = [0.4]
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}\n\n\n"".format(batch_size, lr, p_horizontalflip)
                print(info)
                main(batch_size, lr, p_horizontalflip, info)
",,
2019.08.07 15:19:31,new_comp_quadratic_kappa,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models

import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            self.model = model
        elif model_type == 'ResNext101_32x16d':
            self.model = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x16d_wsl')

    def get_model(self):
        return self.model

",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.00015
    weight_decay: 0
)",8.0,44599361.0,27.0,32.0,0.186261,0.255677,0.8759999999999999,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            self.df_logger = Logger(self.logsFileName + '.csv', 'df')
            self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.data_type = 'new_comp_quadratic_kappa'

        self.img_size = 256

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.model_type = 'ResNet101'

        self.model = MainModel(model_type=self.model_type).get_model()
        if self.model_type == 'ResNext101_32x16d':
            for param in self.model.parameters():
                param.requires_grad = False
            self.model.fc = torch.nn.Linear(2048, 1)

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = None
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 8
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = ""pytorch"" #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 50
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip

        ## PRINT FREQUENCY
        self.print_frequency = 25
",4404.76,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_2015 + p + '.png'):
    #     return train_2015 + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image = crop_image_from_gray(image)
        image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])
","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from config import Config
from train_dataset import transforms_train, transforms_valid, CreateDataset
from model import MainModel
from logger import Logger

# Open source libs


def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_csv = pd.read_csv('./input/train.csv')
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))



    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))

    # Data Processing

    train_path = ""./input/train_images/""

    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed)
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.mean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## LOGGINS LOSSES
        if valid_loss_best > valid_loss_epoch:
            valid_loss_best  = valid_loss_epoch
            train_loss_best = train_loss_epoch
            kappa_best = valid_kappa
            add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
            add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
            add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name + '.pt', epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))

if __name__ == '__main__':
    batch_size_list = [32]
    lr_list = [0.00015]
    p_horizontalflip_list = [0.4]
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}\n\n\n"".format(batch_size, lr, p_horizontalflip)
                print(info)
                main(batch_size, lr, p_horizontalflip, info)
",,
2019.08.07 16:36:39,new_comp_quadratic_kappa,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models

import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            self.model = model
        elif model_type == 'ResNext101_32x16d':
            self.model = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x16d_wsl')

    def get_model(self):
        return self.model

",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.00015
    weight_decay: 0
)",8.0,44599361.0,14.0,32.0,0.310639,0.284727,0.8585,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            self.df_logger = Logger(self.logsFileName + '.csv', 'df')
            self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.data_type = 'new_comp_quadratic_kappa'

        self.img_size = 224

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.model_type = 'ResNet101'

        self.model = MainModel(model_type=self.model_type).get_model()
        if self.model_type == 'ResNext101_32x16d':
            for param in self.model.parameters():
                param.requires_grad = False
            self.model.fc = torch.nn.Linear(2048, 1)

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = None
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 8
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = ""pytorch"" #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 50
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip

        ## PRINT FREQUENCY
        self.print_frequency = 25
",2217.21,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_2015 + p + '.png'):
    #     return train_2015 + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image = crop_image_from_gray(image)
        image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])
","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from config import Config
from train_dataset import transforms_train, transforms_valid, CreateDataset
from model import MainModel
from logger import Logger

# Open source libs


def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_csv = pd.read_csv('./input/train.csv')
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))



    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))

    # Data Processing

    train_path = ""./input/train_images/""

    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed)
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.mean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## LOGGINS LOSSES
        if valid_loss_best > valid_loss_epoch:
            valid_loss_best  = valid_loss_epoch
            train_loss_best = train_loss_epoch
            kappa_best = valid_kappa
            add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
            add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
            add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name + '.pt', epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))

if __name__ == '__main__':
    batch_size_list = [32]
    lr_list = [0.00015]
    p_horizontalflip_list = [0.4]
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}\n\n\n"".format(batch_size, lr, p_horizontalflip)
                print(info)
                main(batch_size, lr, p_horizontalflip, info)
",,
2019.08.07 19:09:56,new_comp_quadratic_kappa,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models

import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            self.model = model
        elif model_type == 'ResNext101_32x16d':
            self.model = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x16d_wsl')

    def get_model(self):
        return self.model

",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.00015
    weight_decay: 0
)",8.0,44599361.0,73.0,32.0,0.186261,0.255677,0.8759999999999999,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            self.df_logger = Logger(self.logsFileName + '.csv', 'df')
            self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.data_type = 'new_comp_quadratic_kappa'

        self.img_size = 256

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.model_type = 'ResNet101'

        self.model = MainModel(model_type=self.model_type).get_model()
        if self.model_type == 'ResNext101_32x16d':
            for param in self.model.parameters():
                param.requires_grad = False
            self.model.fc = torch.nn.Linear(2048, 1)

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = None
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 8
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = ""kappa"" #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip

        ## PRINT FREQUENCY
        self.print_frequency = 25
",11959.07,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_2015 + p + '.png'):
    #     return train_2015 + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image = crop_image_from_gray(image)
        image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])
","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from config import Config
from train_dataset import transforms_train, transforms_valid, CreateDataset
from model import MainModel
from logger import Logger

# Open source libs


def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_csv = pd.read_csv('./input/train.csv')
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))



    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))

    # Data Processing

    train_path = ""./input/train_images/""

    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed)
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.mean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## LOGGINS LOSSES
        if valid_loss_best > valid_loss_epoch:
            valid_loss_best  = valid_loss_epoch
            train_loss_best = train_loss_epoch
            kappa_best = valid_kappa
            add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
            add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
            add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name + '.pt', epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))

if __name__ == '__main__':
    batch_size_list = [32]
    lr_list = [0.00015]
    p_horizontalflip_list = [0.4]
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}\n\n\n"".format(batch_size, lr, p_horizontalflip)
                print(info)
                main(batch_size, lr, p_horizontalflip, info)
",,
2019.08.08 01:02:16,new_comp_quadratic_kappa,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model



",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.00015
    weight_decay: 0
)",8.0,5320317.0,29.0,32.0,0.147883,0.285261,0.8643,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            self.df_logger = Logger(self.logsFileName + '.csv', 'df')
            self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.data_type = 'new_comp_quadratic_kappa'

        self.img_size = 224

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = None
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 8
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = ""pytorch"" #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip

        ## PRINT FREQUENCY
        self.print_frequency = 25",4415.18,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_2015 + p + '.png'):
    #     return train_2015 + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image = crop_image_from_gray(image)
        image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_csv = pd.read_csv('./input/train.csv')
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))



    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))

    # Data Processing

    train_path = ""./input/train_images/""

    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed)
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.mean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name + '.pt', epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))

if __name__ == '__main__':
    batch_size_list = [32]
    lr_list = [0.00015]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b0', 'efficientnet-b1', 'efficientnet-b2', 'efficientnet-b3', 'efficientnet-b4', 'efficientnet-b5']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,
2019.08.08 02:15:52,new_comp_quadratic_kappa,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model



",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.00015
    weight_decay: 0
)",8.0,7825953.0,24.0,32.0,0.188381,0.31049499999999997,0.8032,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            self.df_logger = Logger(self.logsFileName + '.csv', 'df')
            self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.data_type = 'new_comp_quadratic_kappa'

        self.img_size = 224

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = None
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 8
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = ""pytorch"" #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip

        ## PRINT FREQUENCY
        self.print_frequency = 25",3660.66,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_2015 + p + '.png'):
    #     return train_2015 + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image = crop_image_from_gray(image)
        image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_csv = pd.read_csv('./input/train.csv')
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))



    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))

    # Data Processing

    train_path = ""./input/train_images/""

    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed)
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.mean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name + '.pt', epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))

if __name__ == '__main__':
    batch_size_list = [32]
    lr_list = [0.00015]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b0', 'efficientnet-b1', 'efficientnet-b2', 'efficientnet-b3', 'efficientnet-b4', 'efficientnet-b5']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,
2019.08.08 03:16:52,new_comp_quadratic_kappa,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model



",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.00015
    weight_decay: 0
)",8.0,9144835.0,30.0,32.0,0.129978,0.284721,0.8528,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            self.df_logger = Logger(self.logsFileName + '.csv', 'df')
            self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.data_type = 'new_comp_quadratic_kappa'

        self.img_size = 224

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = None
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 8
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = ""pytorch"" #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip

        ## PRINT FREQUENCY
        self.print_frequency = 25",4578.67,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_2015 + p + '.png'):
    #     return train_2015 + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image = crop_image_from_gray(image)
        image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_csv = pd.read_csv('./input/train.csv')
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))



    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))

    # Data Processing

    train_path = ""./input/train_images/""

    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed)
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.mean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name + '.pt', epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))

if __name__ == '__main__':
    batch_size_list = [32]
    lr_list = [0.00015]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b0', 'efficientnet-b1', 'efficientnet-b2', 'efficientnet-b3', 'efficientnet-b4', 'efficientnet-b5']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,
2019.08.08 08:03:14,new_comp_quadratic_kappa,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model



",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.00015
    weight_decay: 0
)",8.0,12271145.0,44.0,32.0,0.081768,0.288204,0.8522,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            self.df_logger = Logger(self.logsFileName + '.csv', 'df')
            self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.data_type = 'new_comp_quadratic_kappa'

        self.img_size = 224

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = None
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 8
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = ""pytorch"" #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip

        ## PRINT FREQUENCY
        self.print_frequency = 25",6829.12,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_2015 + p + '.png'):
    #     return train_2015 + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image = crop_image_from_gray(image)
        image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_csv = pd.read_csv('./input/train.csv')
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))



    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))

    # Data Processing

    train_path = ""./input/train_images/""

    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed)
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.mean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name + '.pt', epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))

if __name__ == '__main__':
    batch_size_list = [32]
    lr_list = [0.00015]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b3', 'efficientnet-b4', 'efficientnet-b5']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,
2019.08.08 14:12:38,new_comp_quadratic_kappa,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model



",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.00015
    weight_decay: 0
)",8.0,19385673.0,22.0,16.0,0.20179,0.269436,0.8448,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            self.df_logger = Logger(self.logsFileName + '.csv', 'df')
            self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.data_type = 'new_comp_quadratic_kappa'

        self.img_size = 224

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = None
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 8
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = ""pytorch"" #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip

        ## PRINT FREQUENCY
        self.print_frequency = 25",3445.98,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_2015 + p + '.png'):
    #     return train_2015 + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image = crop_image_from_gray(image)
        image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_csv = pd.read_csv('./input/train.csv')
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))



    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))

    # Data Processing

    train_path = ""./input/train_images/""

    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed)
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.mean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name + '.pt', epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))

if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [0.00015]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b4', 'efficientnet-b5']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,
2019.08.08 15:10:05,new_comp_quadratic_kappa,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model



",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.00015
    weight_decay: 0
)",8.0,30439985.0,32.0,16.0,0.095233,0.28225,0.8564,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            self.df_logger = Logger(self.logsFileName + '.csv', 'df')
            self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.data_type = 'new_comp_quadratic_kappa'

        self.img_size = 224

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = None
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 8
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = ""pytorch"" #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip

        ## PRINT FREQUENCY
        self.print_frequency = 25",5032.83,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_2015 + p + '.png'):
    #     return train_2015 + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image = crop_image_from_gray(image)
        image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_csv = pd.read_csv('./input/train.csv')
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))



    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))

    # Data Processing

    train_path = ""./input/train_images/""

    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed)
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.mean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name + '.pt', epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))

if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [0.00015]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b4', 'efficientnet-b5']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,
2019.08.08 19:23:20,new_comp_quadratic_kappa,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model



",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.00015
    weight_decay: 0
)",8.0,5320317.0,100.0,32.0,0.47047,0.452932,0.7693,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            self.df_logger = Logger(self.logsFileName + '.csv', 'df')
            self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.data_type = 'new_comp_quadratic_kappa'

        self.img_size = 224

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = None
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 8
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = ""pytorch"" #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip

        ## PRINT FREQUENCY
        self.print_frequency = 25",,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_2015 + p + '.png'):
    #     return train_2015 + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image = crop_image_from_gray(image)
        image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_csv = pd.read_csv('./input/train.csv')
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))



    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))

    # Data Processing

    train_path = ""./input/train_images/""

    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed)
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.mean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name + '.pt', epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))

if __name__ == '__main__':
    batch_size_list = [32]
    lr_list = [0.00015]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b0', 'efficientnet-b1', 'efficientnet-b2', 'efficientnet-b3']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,
2019.08.08 19:30:39,new_comp_quadratic_kappa,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model



",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.00015
    weight_decay: 0
)",8.0,5320317.0,17.0,32.0,1.131912,1.052329,0.0224,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            self.df_logger = Logger(self.logsFileName + '.csv', 'df')
            self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.data_type = 'new_comp_quadratic_kappa'

        self.img_size = 256

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = None
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 8
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = ""pytorch"" #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip

        ## PRINT FREQUENCY
        self.print_frequency = 25",324.11,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_2015 + p + '.png'):
    #     return train_2015 + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image = crop_image_from_gray(image)
        image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_csv = pd.read_csv('./input/train.csv')
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))



    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))

    # Data Processing

    train_path = ""./input/train_images/""

    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed)
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.mean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name + '.pt', epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))

if __name__ == '__main__':
    batch_size_list = [32]
    lr_list = [0.00015]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b0', 'efficientnet-b1', 'efficientnet-b2', 'efficientnet-b3']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,
2019.08.09 00:29:38,new_old_mixed,efficientnet-b5 with one FC,,"Adam, lr = 0.001",5.0,28342833.0,9.0,16.0,0.419599,0.423766,,,,,,,19910.0,
2019.08.09 17:40:21,new_old_mixed,efficientnet-b5 with one FC,MSELoss(),"Adam, lr = 0.001",5.0,30442033.0,25.0,16.0,0.43094399999999994,0.42818900000000004,,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            self.df_logger = Logger(self.logsFileName + '.csv', 'df')
            self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
            self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'new_comp_quadratic_kappa'

        self.img_size = 256

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = None
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 5
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = ""pytorch"" #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 25
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.3
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new_old_mixed'

        ## PRINT FREQUENCY
        self.print_frequency = 50",21231.47,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image = crop_image_from_gray(image)
        image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))

    # Data Processing


    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed)
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.mean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name + '.pt', epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()


if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [0.001]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b5']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",35557.0,
2019.08.10 04:36:38,new_old_mixed,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.BatchNorm1d(in_features),
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1, bias=True),
            )
            model.cuda()
            self.model = model



",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0
)",5.0,28346929.0,15.0,16.0,0.516158,0.449281,,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            self.df_logger = Logger(self.logsFileName + '.csv', 'df')
            self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
            self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'new_comp_quadratic_kappa'

        self.img_size = 256

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = None
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 5
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = ""pytorch"" #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 25
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.3
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new_old_mixed'

        ## PRINT FREQUENCY
        self.print_frequency = 50",21231.47,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image = crop_image_from_gray(image)
        image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))

    # Data Processing


    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed)
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.mean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name + '.pt', epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()


if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [0.001]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b5']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,
2019.08.10 12:28:42,new_old_mixed,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.BatchNorm1d(in_features),
                nn.Linear(in_features=in_features, out_features=1, bias=True),
            )
            model.cuda()
            self.model = model



",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0
)",5.0,28346929.0,25.0,16.0,0.856866,0.839957,0.4322,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            self.df_logger = Logger(self.logsFileName + '.csv', 'df')
            self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile) + 1) + '_end_epoch'
            self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'new_comp_quadratic_kappa'

        self.img_size = 256

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = None
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 5
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = ""pytorch"" #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 25
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.3
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new_old_mixed'

        ## PRINT FREQUENCY
        self.print_frequency = 50",,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image = crop_image_from_gray(image)
        image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))

    # Data Processing


    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed)
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.mean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name + '.pt', epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()


if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [0.001]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b5']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,
2019.08.10 21:12:27,new_old_mixed,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
            )
            model.cuda()
            self.model = model



",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.006
    lr: 0.006
    weight_decay: 0
)",5.0,28342833.0,12.0,16.0,0.543704,0.5195960000000001,,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            self.df_logger = Logger(self.logsFileName + '.csv', 'df')
            self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
            self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'new_comp_quadratic_kappa'

        self.img_size = 256

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(self.optimizer, 5)
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 5
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = ""pytorch"" #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 25
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.3
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new_old_mixed'

        ## PRINT FREQUENCY
        self.print_frequency = 50",17024.91,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image = crop_image_from_gray(image)
        image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))

    # Data Processing


    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed)
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.mean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            cfg.scheduler.step()

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name + '.pt', epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()


if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [0.006]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b5']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,<torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7efb95f979e8>
2019.08.11 15:06:48,new_old_mixed,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model



",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.003
    weight_decay: 0
)",7.0,28342833.0,25.0,16.0,0.5447029999999999,0.575419,,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'mixed_data_finetuning_43th_model'

        self.img_size = 256

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 7
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = ""pytorch"" #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 25
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.3
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new_old_mixed'

        ## PRINT FREQUENCY
        self.print_frequency = 50",,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image = crop_image_from_gray(image)
        image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))

    # Data Processing


    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed)
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.mean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            cfg.scheduler.step(valid_loss_epoch)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()


if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [0.003]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b5']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb975bfac8>
2019.08.11 17:45:02,new_old_mixed_ben_preprocessing,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model



",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.003
    weight_decay: 0
)",7.0,28342833.0,25.0,16.0,0.577896,0.685317,0.3629,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'mixed_data_finetuning_43th_model'

        self.img_size = 256

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 7
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = ""pytorch"" #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 25
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.3
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new_old_mixed_ben_preprocessing'

        ## PRINT FREQUENCY
        self.print_frequency = 50",,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        # image = crop_image_from_gray(image)
        # image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        # image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))

    # Data Processing


    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed)
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.mean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            cfg.scheduler.step(valid_loss_epoch)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()


if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [0.003]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b5']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb95e8fe80>
2019.08.11 17:56:08,new_old_mixed_ben_preprocessing,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model



",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.003
    weight_decay: 0
)",8.0,28342833.0,30.0,16.0,0.577896,0.685317,0.3629,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'mixed_data_finetuning_43th_model'

        self.img_size = 224

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 8
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = ""pytorch"" #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 30
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.3
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new_old_mixed_ben_preprocessing'

        ## PRINT FREQUENCY
        self.print_frequency = 50",,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        # image = crop_image_from_gray(image)
        # image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        # image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))

    # Data Processing


    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed)
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.mean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            cfg.scheduler.step(valid_loss_epoch)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()


if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [0.003, 0.01, 0.1]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b5']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb95e8fe48>
2019.08.11 18:06:15,new_old_mixed_ben_preprocessing,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model



",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.003
    weight_decay: 0
)",8.0,28342833.0,30.0,16.0,0.297677,0.344714,0.6718,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'mixed_data_finetuning_43th_model'

        self.img_size = 224

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 8
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = ""pytorch"" #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 30
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.3
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new_old_mixed_ben_preprocessing'

        ## PRINT FREQUENCY
        self.print_frequency = 50",13852.03,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        # image = crop_image_from_gray(image)
        # image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        # image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))

    # Data Processing


    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed)
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            cfg.scheduler.step(valid_loss_epoch)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()


if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [0.003, 0.01, 0.1]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b5']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb95e8feb8>
2019.08.12 00:10:31,new_old_mixed_ben_preprocessing,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model



",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0003
    weight_decay: 0
)",8.0,28342833.0,30.0,16.0,0.254533,0.343003,0.7446,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'mixed_data_finetuning_51th_model_img_size_180'

        self.img_size = 180

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 8
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = ""pytorch"" #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 30
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.3
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new_old_mixed_ben_preprocessing'

        ## PRINT FREQUENCY
        self.print_frequency = 50",14136.31,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        # image = crop_image_from_gray(image)
        # image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        # image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))

    # Data Processing


    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed)
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            cfg.scheduler.step(valid_loss_epoch)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()


if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [0.0003]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b5']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb95e8fe80>
2019.08.12 08:40:11,new,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1)
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1)
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model



",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.001
    lr: 0.001
    weight_decay: 0
)",8.0,4008829.0,17.0,32.0,0.20402,0.244385,0.8656,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'mixed_data_finetuning_51th_model_img_size_180'

        self.img_size = 256

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=5, gamma=0.1)
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 8
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = ""pytorch"" #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 30
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.3
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50",2711.86,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image = crop_image_from_gray(image)
        image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))

    # Data Processing


    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed)
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            cfg.scheduler.step()

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()


if __name__ == '__main__':
    batch_size_list = [32]
    lr_list = [1e-3, 1e-2, 1e-1, 1e-4, 1e-5]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b0']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,<torch.optim.lr_scheduler.StepLR object at 0x7efb9604e198>
2019.08.12 09:25:23,new,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1)
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1)
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model



",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.01
    lr: 0.01
    weight_decay: 0
)",8.0,4008829.0,28.0,32.0,0.31125,0.302265,0.7202,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'mixed_data_finetuning_51th_model_img_size_180'

        self.img_size = 256

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=5, gamma=0.1)
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 8
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = ""pytorch"" #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 30
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.3
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50",4491.31,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image = crop_image_from_gray(image)
        image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))

    # Data Processing


    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed)
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            cfg.scheduler.step()

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()


if __name__ == '__main__':
    batch_size_list = [32]
    lr_list = [1e-3, 1e-2, 1e-1, 1e-4, 1e-5]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b0']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,<torch.optim.lr_scheduler.StepLR object at 0x7efb9604e0b8>
2019.08.12 10:40:14,new,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1)
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1)
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model



",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.1
    lr: 0.1
    weight_decay: 0
)",8.0,4008829.0,30.0,32.0,0.7421220000000001,0.589543,0.6151,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'mixed_data_finetuning_51th_model_img_size_180'

        self.img_size = 256

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=5, gamma=0.1)
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 8
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = ""pytorch"" #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 30
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.3
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50",4780.65,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image = crop_image_from_gray(image)
        image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))

    # Data Processing


    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed)
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            cfg.scheduler.step()

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()


if __name__ == '__main__':
    batch_size_list = [32]
    lr_list = [1e-3, 1e-2, 1e-1, 1e-4, 1e-5]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b0']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,<torch.optim.lr_scheduler.StepLR object at 0x7efb8c06ee10>
2019.08.12 11:59:55,new,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1)
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1)
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model



",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.0001
    lr: 0.0001
    weight_decay: 0
)",8.0,4008829.0,14.0,32.0,0.336932,0.357646,0.7955,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'mixed_data_finetuning_51th_model_img_size_180'

        self.img_size = 256

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=5, gamma=0.1)
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 8
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = ""pytorch"" #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 30
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.3
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50",2231.8,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image = crop_image_from_gray(image)
        image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))

    # Data Processing


    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed)
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            cfg.scheduler.step()

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()


if __name__ == '__main__':
    batch_size_list = [32]
    lr_list = [1e-3, 1e-2, 1e-1, 1e-4, 1e-5]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b0']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,<torch.optim.lr_scheduler.StepLR object at 0x7efb8c11ca20>
2019.08.12 12:37:07,new,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1)
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1)
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model



",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 1e-05
    lr: 1e-05
    weight_decay: 0
)",8.0,4008829.0,30.0,32.0,0.654287,0.860943,0.6136,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'mixed_data_finetuning_51th_model_img_size_180'

        self.img_size = 256

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=5, gamma=0.1)
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 8
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = ""pytorch"" #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 30
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.3
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50",4782.32,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image = crop_image_from_gray(image)
        image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))

    # Data Processing


    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed)
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            cfg.scheduler.step()

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()


if __name__ == '__main__':
    batch_size_list = [32]
    lr_list = [1e-3, 1e-2, 1e-1, 1e-4, 1e-5]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b0']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,<torch.optim.lr_scheduler.StepLR object at 0x7efb8c11c860>
2019.08.12 16:37:43,new,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1)
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1)
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model



",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.001
    lr: 0.001
    weight_decay: 1e-05
)",8.0,4008829.0,14.0,32.0,0.25650500000000004,0.23856,0.8806,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'mixed_data_finetuning_43h_model_img_size_256'

        self.img_size = 256

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=5, gamma=0.1)
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 8
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = ""pytorch"" #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 30
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.1
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50",2203.67,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image = crop_image_from_gray(image)
        image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))

    # Data Processing


    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            cfg.scheduler.step()

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()


if __name__ == '__main__':
    batch_size_list = [32]
    lr_list = [1e-3, 1e-2, 1e-1, 1e-4, 1e-5]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b0']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,<torch.optim.lr_scheduler.StepLR object at 0x7efb9604a240>
2019.08.12 17:14:27,new,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1)
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1)
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model



",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.01
    lr: 0.01
    weight_decay: 1e-05
)",8.0,4008829.0,29.0,32.0,0.270807,0.251306,0.7879999999999999,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'mixed_data_finetuning_43h_model_img_size_256'

        self.img_size = 256

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=5, gamma=0.1)
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 8
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = ""pytorch"" #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 30
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.1
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50",4569.66,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image = crop_image_from_gray(image)
        image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))

    # Data Processing


    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            cfg.scheduler.step()

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()


if __name__ == '__main__':
    batch_size_list = [32]
    lr_list = [1e-3, 1e-2, 1e-1, 1e-4, 1e-5]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b0']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,<torch.optim.lr_scheduler.StepLR object at 0x7efb9604a160>
2019.08.12 18:30:37,new,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1)
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1)
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model



",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.1
    lr: 0.1
    weight_decay: 1e-05
)",8.0,4008829.0,30.0,32.0,0.733196,0.624957,0.6544,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'mixed_data_finetuning_43h_model_img_size_256'

        self.img_size = 256

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=5, gamma=0.1)
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 8
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = ""pytorch"" #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 30
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.1
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50",4743.16,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image = crop_image_from_gray(image)
        image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))

    # Data Processing


    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            cfg.scheduler.step()

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()


if __name__ == '__main__':
    batch_size_list = [32]
    lr_list = [1e-3, 1e-2, 1e-1, 1e-4, 1e-5]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b0']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,<torch.optim.lr_scheduler.StepLR object at 0x7efb96021940>
2019.08.12 19:49:40,new,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1)
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1)
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model



",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.0001
    lr: 0.0001
    weight_decay: 1e-05
)",8.0,4008829.0,30.0,32.0,0.26542,0.315985,0.8538,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'mixed_data_finetuning_43h_model_img_size_256'

        self.img_size = 256

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=5, gamma=0.1)
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 8
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = ""pytorch"" #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 30
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.1
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50",,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image = crop_image_from_gray(image)
        image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))

    # Data Processing


    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            cfg.scheduler.step()

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()


if __name__ == '__main__':
    batch_size_list = [32]
    lr_list = [1e-3, 1e-2, 1e-1, 1e-4, 1e-5]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b0']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,<torch.optim.lr_scheduler.StepLR object at 0x7efb960219b0>
2019.08.12 20:43:02,new,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1)
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1)
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model



",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 1e-05
)",8.0,28342833.0,16.0,16.0,0.228527,0.251551,0.8465,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'mixed_data_imgsize_256'

        self.img_size = 256

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 8
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = ""pytorch"" #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 30
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50",2635.93,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image = crop_image_from_gray(image)
        image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))

    # Data Processing


    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            cfg.scheduler.step(valid_loss_epoch)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()


if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [1e-3]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b5']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb95f9aba8>
2019.08.12 21:28:28,new,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1)
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1)
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model



",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 1e-05
)",8.0,28342833.0,22.0,16.0,0.20565999999999998,0.25797,0.8582,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'mixed_data_imgsize_256'

        self.img_size = 256

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 8
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = ""pytorch"" #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 30
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50",3620.78,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image = crop_image_from_gray(image)
        image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))

    # Data Processing


    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            cfg.scheduler.step(valid_loss_epoch)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()


if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [1e-3]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b5']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb9d107320>
2019.08.12 22:35:51,new,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1)
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1)
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model



",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 1e-05
)",8.0,28342833.0,100.0,16.0,0.17770999999999998,0.262639,0.8644,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'mixed_data_imgsize_256'

        self.img_size = 256

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 8
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = ""kappa"" #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50",,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image = crop_image_from_gray(image)
        image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))

    # Data Processing


    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()


if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [1e-3]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b5']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb9d101780>
2019.08.12 23:09:31,new,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1)
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1)
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model



",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 1e-05
)",8.0,28342833.0,100.0,16.0,0.053742,0.299521,0.8821,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'mixed_data_imgsize_256'

        self.img_size = 256

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 8
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = ""kappa"" #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50",16423.5,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image = crop_image_from_gray(image)
        image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))

    # Data Processing


    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()


if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [1e-3]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b5']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb9d101780>
2019.08.13 08:23:37,new_old_mixed_ben_preprocessing,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1)
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1)
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                # nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model



",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 1e-05
)",8.0,28342833.0,100.0,16.0,0.7235560000000001,0.659867,0.4645,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'mixed_data_imgsize_256'

        self.img_size = 256

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 8
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = ""kappa"" #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new_old_mixed_ben_preprocessing'

        ## PRINT FREQUENCY
        self.print_frequency = 50",,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image = crop_image_from_gray(image)
        image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))

    # Data Processing


    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()


if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [1e-3]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b5']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb95f97c50>
2019.08.13 08:49:36,new_old_mixed_ben_preprocessing,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1)
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1)
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                # nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model



",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 1e-05
)",8.0,28342833.0,100.0,16.0,0.320098,0.339406,0.6905,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'mixed_data_imgsize_256'

        self.img_size = 256

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 8
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = ""kappa"" #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new_old_mixed_ben_preprocessing'

        ## PRINT FREQUENCY
        self.print_frequency = 50",,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        # image = crop_image_from_gray(image)
        # image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        # image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))

    # Data Processing


    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()


if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [1e-3]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b5']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb95f97cf8>
2019.08.13 15:30:56,new_old_mixed_ben_preprocessing,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1)
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1)
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                # nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model



",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 1e-05
)",8.0,28342833.0,34.0,16.0,0.282988,0.33096,0.695,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'mixed_data_imgsize_256'

        self.img_size = 256

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 8
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new_old_mixed_ben_preprocessing'

        ## PRINT FREQUENCY
        self.print_frequency = 50",17463.05,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        # image = crop_image_from_gray(image)
        # image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        # image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))

    # Data Processing


    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()


if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [1e-3]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b5']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb95f97cc0>
2019.08.13 20:44:06,new_old_mixed_ben_preprocessing,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1)
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1)
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model



",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 1e-05
)",8.0,28342833.0,38.0,16.0,0.26889,0.32443,0.7033,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'mixed_data_imgsize_256'

        self.img_size = 256

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 8
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new_old_mixed_ben_preprocessing'

        ## PRINT FREQUENCY
        self.print_frequency = 50",19559.33,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        # image = crop_image_from_gray(image)
        # image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        # image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))

    # Data Processing


    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()


if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [1e-3]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b5']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb9d1066d8>
2019.08.14 02:29:18,new_old_balanced,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1)
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1)
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                # nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model



",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 1e-05
)",8.0,28342833.0,34.0,16.0,0.153031,0.318249,0.8564,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'mixed_data_imgsize_256'

        self.img_size = 256

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 8
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new_old_balanced'

        ## PRINT FREQUENCY
        self.print_frequency = 50",2423.59,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        # image = crop_image_from_gray(image)
        # image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        # image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_BEN_preprocessing/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))

    # Data Processing


    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()


if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [1e-3, 5e-3, 1e-2, 1e-1]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b5']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb982c3c50>
2019.08.14 08:22:11,new_old_balanced,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1)
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1)
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                # nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model



",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.005
    weight_decay: 1e-05
)",8.0,28342833.0,36.0,16.0,0.26405300000000004,0.352941,0.7979999999999999,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'mixed_data_imgsize_256'

        self.img_size = 256

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 8
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new_old_balanced'

        ## PRINT FREQUENCY
        self.print_frequency = 50",2540.56,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        # image = crop_image_from_gray(image)
        # image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        # image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_BEN_preprocessing/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))

    # Data Processing


    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()


if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [5e-3, 1e-2, 1e-1]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b5']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb95f9ab00>
2019.08.14 09:04:32,new_old_balanced,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1)
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1)
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                # nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model



",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.01
    weight_decay: 1e-05
)",8.0,28342833.0,100.0,16.0,1.902853,90.885605,-0.0067,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'mixed_data_imgsize_256'

        self.img_size = 256

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 8
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new_old_balanced'

        ## PRINT FREQUENCY
        self.print_frequency = 50",,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        # image = crop_image_from_gray(image)
        # image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        # image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_BEN_preprocessing/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))

    # Data Processing


    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()


if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [5e-3, 1e-2, 1e-1]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b5']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb95e92eb8>
2019.08.14 14:01:40,new_old_balanced,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1)
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1)
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model



",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 1e-05
)",8.0,4008829.0,32.0,16.0,0.177518,0.379637,0.8279,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'mixed_data_imgsize_256'

        self.img_size = 256

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 8
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new_old_balanced'

        ## PRINT FREQUENCY
        self.print_frequency = 50",698.79,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        # image = crop_image_from_gray(image)
        # image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        # image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_BEN_preprocessing/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))

    # Data Processing


    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [1e-3, 5e-3, 1e-2, 1e-1]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b0', 'efficientnet-b5']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb9604b320>
2019.08.14 14:13:19,new_old_balanced,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1)
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1)
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model



",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 1e-05
)",8.0,28342833.0,40.0,16.0,0.137881,0.340661,0.8581,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'mixed_data_imgsize_256'

        self.img_size = 256

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 8
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new_old_balanced'

        ## PRINT FREQUENCY
        self.print_frequency = 50",2690.31,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        # image = crop_image_from_gray(image)
        # image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        # image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_BEN_preprocessing/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))

    # Data Processing


    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [1e-3, 5e-3, 1e-2, 1e-1]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b0', 'efficientnet-b5']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb8c0d5358>
2019.08.14 14:58:09,new_old_balanced,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1)
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1)
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model



",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.005
    weight_decay: 1e-05
)",8.0,4008829.0,50.0,16.0,0.30801,0.362663,0.7942,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'mixed_data_imgsize_256'

        self.img_size = 256

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 8
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new_old_balanced'

        ## PRINT FREQUENCY
        self.print_frequency = 50",1074.19,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        # image = crop_image_from_gray(image)
        # image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        # image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_BEN_preprocessing/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))

    # Data Processing


    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [1e-3, 5e-3, 1e-2, 1e-1]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b0', 'efficientnet-b5']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb7cca54a8>
2019.08.14 15:16:04,new_old_balanced,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1)
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1)
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model



",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.005
    weight_decay: 1e-05
)",8.0,28342833.0,57.0,16.0,0.26776300000000003,0.372154,0.7847,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'mixed_data_imgsize_256'

        self.img_size = 256

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 8
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new_old_balanced'

        ## PRINT FREQUENCY
        self.print_frequency = 50",3835.97,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        # image = crop_image_from_gray(image)
        # image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        # image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_BEN_preprocessing/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))

    # Data Processing


    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [1e-3, 5e-3, 1e-2, 1e-1]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b0', 'efficientnet-b5']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb8c083048>
2019.08.14 16:20:00,new_old_balanced,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1)
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1)
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model



",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.01
    weight_decay: 1e-05
)",8.0,4008829.0,45.0,16.0,0.397825,0.396329,0.741,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'mixed_data_imgsize_256'

        self.img_size = 256

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 8
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new_old_balanced'

        ## PRINT FREQUENCY
        self.print_frequency = 50",1000.4,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        # image = crop_image_from_gray(image)
        # image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        # image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_BEN_preprocessing/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))

    # Data Processing


    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [1e-3, 5e-3, 1e-2, 1e-1]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b0', 'efficientnet-b5']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb9606db00>
2019.08.14 16:41:37,new_old_balanced,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1)
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1)
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model



",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 1e-05
)",8.0,4008829.0,32.0,16.0,0.177518,0.379637,0.8279,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'mixed_data_imgsize_224'

        self.img_size = 224

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 8
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new_old_balanced'

        ## PRINT FREQUENCY
        self.print_frequency = 50",687.86,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        # image = crop_image_from_gray(image)
        # image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        # image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_BEN_preprocessing/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))

    # Data Processing


    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [1e-3, 5e-3, 1e-2, 1e-1]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b0', 'efficientnet-b5']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb9604c320>
2019.08.14 16:53:05,new_old_balanced,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1)
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1)
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model



",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 1e-05
)",8.0,28342833.0,40.0,16.0,0.137881,0.340661,0.8581,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'mixed_data_imgsize_224'

        self.img_size = 224

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 8
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new_old_balanced'

        ## PRINT FREQUENCY
        self.print_frequency = 50",2700.29,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        # image = crop_image_from_gray(image)
        # image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        # image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_BEN_preprocessing/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))

    # Data Processing


    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [1e-3, 5e-3, 1e-2, 1e-1]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b0', 'efficientnet-b5']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb95fedb70>
2019.08.14 17:38:05,new_old_balanced,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1)
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1)
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model



",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.005
    weight_decay: 1e-05
)",8.0,4008829.0,50.0,16.0,0.30801,0.362663,0.7942,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'mixed_data_imgsize_224'

        self.img_size = 224

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 8
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new_old_balanced'

        ## PRINT FREQUENCY
        self.print_frequency = 50",1079.93,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        # image = crop_image_from_gray(image)
        # image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        # image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_BEN_preprocessing/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))

    # Data Processing


    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [1e-3, 5e-3, 1e-2, 1e-1]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b0', 'efficientnet-b5']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb95fedef0>
2019.08.14 17:56:06,new_old_balanced,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1)
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1)
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model



",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.005
    weight_decay: 1e-05
)",8.0,28342833.0,57.0,16.0,0.26776300000000003,0.372154,0.7847,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'mixed_data_imgsize_224'

        self.img_size = 224

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 8
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new_old_balanced'

        ## PRINT FREQUENCY
        self.print_frequency = 50",3874.66,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        # image = crop_image_from_gray(image)
        # image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        # image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_BEN_preprocessing/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))

    # Data Processing


    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [1e-3, 5e-3, 1e-2, 1e-1]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b0', 'efficientnet-b5']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb8c04db00>
2019.08.14 19:00:41,new_old_balanced,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1)
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1)
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model



",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.01
    weight_decay: 1e-05
)",8.0,4008829.0,45.0,16.0,0.397825,0.396329,0.741,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'mixed_data_imgsize_224'

        self.img_size = 224

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 8
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new_old_balanced'

        ## PRINT FREQUENCY
        self.print_frequency = 50",976.41,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        # image = crop_image_from_gray(image)
        # image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        # image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_BEN_preprocessing/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))

    # Data Processing


    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [1e-3, 5e-3, 1e-2, 1e-1]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b0', 'efficientnet-b5']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb95ffcdd8>
2019.08.15 00:31:08,new,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1)
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1)
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model



",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 1e-05
)",8.0,4008829.0,22.0,16.0,0.198245,0.287013,0.8538,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'mixed_data_imgsize_256'

        self.img_size = 224

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 8
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50",3405.85,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image = crop_image_from_gray(image)
        image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_BEN_preprocessing/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))

    # Data Processing


    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [1e-3]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b0', 'efficientnet-b5']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb9604d278>
2019.08.15 01:33:50,new,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1)
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model



",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 1e-05
)",8.0,28342833.0,22.0,16.0,0.20565999999999998,0.25797,0.8582,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'mixed_data_imgsize_256'

        self.img_size = 224

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 8
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50",1231.86,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        # image = crop_image_from_gray(image)
        # image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        # image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_mixed_BEN_preprocessing/'           #### NEED TO CHANGED IT!!!!!!!!!!!!!!!!!!!
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_BEN_preprocessing/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))

    # Data Processing


    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [1e-3]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b5']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb9d1037b8>
2019.08.15 01:56:43,new,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model



",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 1e-05
)",8.0,42502209.0,47.0,16.0,0.252737,0.29188800000000004,0.8207,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'mixed_data_imgsize_256'

        self.img_size = 224

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 8
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50",1646.63,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        # image = crop_image_from_gray(image)
        # image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        # image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_mixed_BEN_preprocessing/'           #### NEED TO CHANGED IT!!!!!!!!!!!!!!!!!!!
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_BEN_preprocessing/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))

    # Data Processing


    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [1e-3]
    p_horizontalflip_list = [0.4]
    model_type_list = ['ResNet101']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb9da39390>
2019.08.15 02:25:10,new,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                # nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model



",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 1e-05
)",8.0,42502209.0,47.0,16.0,0.20998000000000008,0.283555,0.8565,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'mixed_data_imgsize_256'

        self.img_size = 224

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 8
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50",1625.21,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        # image = crop_image_from_gray(image)
        # image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        # image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_mixed_BEN_preprocessing/'           #### NEED TO CHANGED IT!!!!!!!!!!!!!!!!!!!
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_BEN_preprocessing/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))

    # Data Processing


    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [1e-3]
    p_horizontalflip_list = [0.4]
    model_type_list = ['ResNet101']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb9d102898>
2019.08.15 03:01:28,new,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        self.model = None
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                # nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'ResNet152':
            model = models.resnet152(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet152.pth""))
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                # nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model



",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 1e-05
)",8.0,58145857.0,47.0,16.0,0.226359,0.285409,0.8527,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'mixed_data_imgsize_256'

        self.img_size = 224

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 8
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50",2208.14,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        # image = crop_image_from_gray(image)
        # image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        # image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_mixed_BEN_preprocessing/'           #### NEED TO CHANGED IT!!!!!!!!!!!!!!!!!!!
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_BEN_preprocessing/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))

    # Data Processing


    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [1e-3, 5e-3, 1e-2, 5e-2]
    p_horizontalflip_list = [0.4]
    model_type_list = ['ResNet152']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb95fde3c8>
2019.08.15 03:38:17,new,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        self.model = None
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                # nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'ResNet152':
            model = models.resnet152(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet152.pth""))
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                # nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model



",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.005
    weight_decay: 1e-05
)",8.0,58145857.0,60.0,16.0,0.27487399999999995,0.309236,0.7947,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'mixed_data_imgsize_256'

        self.img_size = 224

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 8
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50",2663.23,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        # image = crop_image_from_gray(image)
        # image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        # image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_mixed_BEN_preprocessing/'           #### NEED TO CHANGED IT!!!!!!!!!!!!!!!!!!!
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_BEN_preprocessing/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))

    # Data Processing


    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [1e-3, 5e-3, 1e-2, 5e-2]
    p_horizontalflip_list = [0.4]
    model_type_list = ['ResNet152']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb95f800b8>
2019.08.15 04:22:41,new,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        self.model = None
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                # nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'ResNet152':
            model = models.resnet152(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet152.pth""))
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                # nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model



",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.01
    weight_decay: 1e-05
)",8.0,58145857.0,77.0,16.0,0.302414,0.33829000000000004,0.7908,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'mixed_data_imgsize_256'

        self.img_size = 224

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 8
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50",3414.37,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        # image = crop_image_from_gray(image)
        # image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        # image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_mixed_BEN_preprocessing/'           #### NEED TO CHANGED IT!!!!!!!!!!!!!!!!!!!
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_BEN_preprocessing/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))

    # Data Processing


    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [1e-3, 5e-3, 1e-2, 5e-2]
    p_horizontalflip_list = [0.4]
    model_type_list = ['ResNet152']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb8c140710>
2019.08.15 05:19:37,new,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        self.model = None
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                # nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'ResNet152':
            model = models.resnet152(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet152.pth""))
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                # nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model



",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.05
    weight_decay: 1e-05
)",8.0,58145857.0,82.0,16.0,0.32524200000000003,0.349566,0.7275,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'mixed_data_imgsize_256'

        self.img_size = 224

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 8
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50",3638.44,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        # image = crop_image_from_gray(image)
        # image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        # image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_mixed_BEN_preprocessing/'           #### NEED TO CHANGED IT!!!!!!!!!!!!!!!!!!!
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_BEN_preprocessing/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))

    # Data Processing


    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [1e-3, 5e-3, 1e-2, 5e-2]
    p_horizontalflip_list = [0.4]
    model_type_list = ['ResNet152']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb8c150438>
2019.08.15 08:32:51,new,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        self.model = None
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                # nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'ResNet152':
            model = models.resnet152(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet152.pth""))
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                # nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model



",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 1e-05
)",8.0,42502209.0,47.0,16.0,0.20998000000000008,0.283555,0.8565,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'mixed_data_imgsize_256'

        self.img_size = 256

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 8
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50",1449.84,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        # image = crop_image_from_gray(image)
        # image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        # image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_mixed_BEN_preprocessing/'           #### NEED TO CHANGED IT!!!!!!!!!!!!!!!!!!!
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_BEN_preprocessing/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))

    # Data Processing


    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [1e-3, 5e-3, 1e-2, 5e-2]
    p_horizontalflip_list = [0.4]
    model_type_list = ['ResNet101']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb9d102518>
2019.08.15 09:00:17,new,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        self.model = None
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'ResNet152':
            model = models.resnet152(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet152.pth""))
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                # nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model



",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 1e-05
)",8.0,58145857.0,19.0,16.0,0.4688479999999999,0.381069,0.7526,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'mixed_data_imgsize_256'

        self.img_size = 256

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 8
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50",841.64,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        # image = crop_image_from_gray(image)
        # image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        # image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_mixed_BEN_preprocessing/'           #### NEED TO CHANGED IT!!!!!!!!!!!!!!!!!!!
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_BEN_preprocessing/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))

    # Data Processing


    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [2e-3]
    p_horizontalflip_list = [0.4]
    model_type_list = ['ResNet152', 'ResNet101']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb95fdd390>
2019.08.15 09:14:19,new,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        self.model = None
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'ResNet152':
            model = models.resnet152(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet152.pth""))
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                # nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model



",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 1e-05
)",8.0,42502209.0,51.0,16.0,0.317918,0.348607,0.8174,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'mixed_data_imgsize_256'

        self.img_size = 256

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 8
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50",1581.65,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        # image = crop_image_from_gray(image)
        # image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        # image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_mixed_BEN_preprocessing/'           #### NEED TO CHANGED IT!!!!!!!!!!!!!!!!!!!
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_BEN_preprocessing/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))

    # Data Processing


    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [2e-3]
    p_horizontalflip_list = [0.4]
    model_type_list = ['ResNet152', 'ResNet101']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb8c18c048>
2019.08.15 12:53:23,new,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        self.model = None
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'ResNet152':
            model = models.resnet152(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet152.pth""))
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model



",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 1e-05
)",8.0,58145857.0,28.0,16.0,0.364502,0.338971,0.7070000000000001,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'mixed_data_imgsize_256'

        self.img_size = 256

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 8
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50",1315.51,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        # image = crop_image_from_gray(image)
        # image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        # image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_mixed_BEN_preprocessing/'           #### NEED TO CHANGED IT!!!!!!!!!!!!!!!!!!!
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_BEN_preprocessing/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))

    # Data Processing


    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [2e-3,  1e-3]
    p_horizontalflip_list = [0.4]
    model_type_list = ['ResNet152']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb95fde860>
2019.08.15 13:15:20,new,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        self.model = None
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'ResNet152':
            model = models.resnet152(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet152.pth""))
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model



",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 1e-05
)",8.0,58145857.0,28.0,16.0,0.292928,0.316634,0.8222,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'mixed_data_imgsize_256'

        self.img_size = 256

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 8
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50",1275.24,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        # image = crop_image_from_gray(image)
        # image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        # image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_mixed_BEN_preprocessing/'           #### NEED TO CHANGED IT!!!!!!!!!!!!!!!!!!!
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_BEN_preprocessing/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))

    # Data Processing


    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [2e-3,  1e-3]
    p_horizontalflip_list = [0.4]
    model_type_list = ['ResNet152']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb95fc5f98>
2019.08.15 19:54:21,new,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        self.model = None
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'ResNet152':
            model = models.resnet152(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet152.pth""))
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model



",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 1e-05
)",8.0,28342833.0,100.0,16.0,0.380802,0.344864,0.8515,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'mixed_data_imgsize_256'

        self.img_size = 256

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 8
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50",,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image = crop_image_from_gray(image)
        image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 10), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'           #### NEED TO CHANGED IT!!!!!!!!!!!!!!!!!!!
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_BEN_preprocessing/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))

    # Data Processing


    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [1e-3]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b5']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb9d1086d8>
2019.08.15 20:00:02,new,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        self.model = None
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'ResNet152':
            model = models.resnet152(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet152.pth""))
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model



",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 1e-05
)",8.0,28342833.0,17.0,16.0,0.243047,0.268509,0.8548,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'mixed_data_imgsize_256'

        self.img_size = 256

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 8
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50",1143.19,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image = crop_image_from_gray(image)
        image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 10), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'           #### NEED TO CHANGED IT!!!!!!!!!!!!!!!!!!!
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_BEN_preprocessing/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))
    print(""Training path: {}"".format(train_path))
    # Data Processing


    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [1e-3]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b5']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb9d106668>
2019.08.15 20:20:29,new,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        self.model = None
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'ResNet152':
            model = models.resnet152(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet152.pth""))
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model



",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 1e-05
)",8.0,28342833.0,17.0,16.0,0.234992,0.272334,0.8589,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'mixed_data_imgsize_256'

        self.img_size = 256

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 8
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50",1306.84,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image = crop_image_from_gray(image)
        image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 15), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'           #### NEED TO CHANGED IT!!!!!!!!!!!!!!!!!!!
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_BEN_preprocessing/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))
    print(""Training path: {}"".format(train_path))
    # Data Processing


    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [1e-3]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b5']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb9d109668>
2019.08.15 20:42:46,new,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        self.model = None
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'ResNet152':
            model = models.resnet152(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet152.pth""))
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model



",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 1e-05
)",8.0,28342833.0,100.0,16.0,0.433382,0.589005,0.7821,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'mixed_data_imgsize_256'

        self.img_size = 256

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 8
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50",,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image = crop_image_from_gray(image)
        image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'           #### NEED TO CHANGED IT!!!!!!!!!!!!!!!!!!!
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_BEN_preprocessing/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))
    print(""Training path: {}"".format(train_path))
    # Data Processing


    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [1e-3]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b5']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb9d1085c0>
2019.08.15 20:49:15,new,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        self.model = None
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'ResNet152':
            model = models.resnet152(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet152.pth""))
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model



",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 1e-05
)",8.0,28342833.0,22.0,16.0,0.20234100000000002,0.25605500000000003,0.8605,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'mixed_data_imgsize_256'

        self.img_size = 256

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 8
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50",2048.37,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image = crop_image_from_gray(image)
        image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 20), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'           #### NEED TO CHANGED IT!!!!!!!!!!!!!!!!!!!
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_BEN_preprocessing/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))
    print(""Training path: {}"".format(train_path))
    # Data Processing


    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [1e-3]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b5']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb9d1076d8>
2019.08.15 21:23:53,new_old_balanced,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        self.model = None
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'ResNet152':
            model = models.resnet152(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet152.pth""))
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model



",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.001
    lr: 0.001
    weight_decay: 1e-05
)",250.0,58145857.0,536.0,16.0,0.341773,0.431253,0.8137,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'mixed_data_imgsize_256'

        self.img_size = 256

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=35, eta_min=0)
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 250
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 1000
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.1
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new_old_balanced'

        ## PRINT FREQUENCY
        self.print_frequency = 50",34809.31,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        # image = crop_image_from_gray(image)
        # image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        # image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'           #### NEED TO CHANGED IT!!!!!!!!!!!!!!!!!!!
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_BEN_preprocessing/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))
    print(""Training path: {}"".format(train_path))
    # Data Processing


    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step()
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [1e-3]
    p_horizontalflip_list = [0.4]
    model_type_list = ['ResNet152']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,<torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7efb95fde828>
2019.08.16 08:20:18,new,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        self.model = None
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'ResNet152':
            model = models.resnet152(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet152.pth""))
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model



",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.001
    lr: 0.001
    weight_decay: 1e-05
)",250.0,28342833.0,385.0,16.0,0.093787,0.204284,0.8981,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'mixed_data_imgsize_256'

        self.img_size = 256

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=35, eta_min=0)
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 250
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 1000
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.1
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50",20601.64,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        # image = crop_image_from_gray(image)
        # image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        # image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_mixed_BEN_preprocessing/'           #### NEED TO CHANGED IT!!!!!!!!!!!!!!!!!!!
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_BEN_preprocessing/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))
    print(""Training path: {}"".format(train_path))
    # Data Processing


    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step()
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [1e-3]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b5']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,<torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7efb9d102780>
2019.08.17 03:32:00,new_old_mixed_ben_preprocessing,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        self.model = None
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'ResNet152':
            model = models.resnet152(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet152.pth""))
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model



",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.001
    lr: 0.001
    weight_decay: 1e-05
)",250.0,28342833.0,284.0,16.0,0.260083,0.315942,0.7002,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'mixed_data_imgsize_256'

        self.img_size = 256

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=35, eta_min=0)
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 250
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 1000
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.1
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new_old_mixed_ben_preprocessing'

        ## PRINT FREQUENCY
        self.print_frequency = 50",157651.52,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        # image = crop_image_from_gray(image)
        # image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        # image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_mixed_BEN_preprocessing/'           #### NEED TO CHANGED IT!!!!!!!!!!!!!!!!!!!
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_BEN_preprocessing/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))
    print(""Training path: {}"".format(train_path))
    # Data Processing


    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step()
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [1e-3]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b5']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,<torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7efb9d1028d0>
2019.08.19 03:14:56,new_old_balanced,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        self.model = None
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'ResNet152':
            model = models.resnet152(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet152.pth""))
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model



",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.001
    lr: 0.001
    weight_decay: 1e-05
)",250.0,28342833.0,277.0,16.0,0.148127,0.360742,0.8448,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'mixed_data_imgsize_256'

        self.img_size = 256

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=35, eta_min=0)
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 250
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 1000
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.1
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new_old_balanced'

        ## PRINT FREQUENCY
        self.print_frequency = 50",20226.7,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        # image = crop_image_from_gray(image)
        # image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        # image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_mixed_BEN_preprocessing/'           #### NEED TO CHANGED IT!!!!!!!!!!!!!!!!!!!
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_BEN_preprocessing/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))
    print(""Training path: {}"".format(train_path))
    # Data Processing


    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step()
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [1e-3]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b5']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,<torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7efb9d102470>
2019.08.19 11:57:54,new,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        self.model = None
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'ResNet152':
            model = models.resnet152(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet152.pth""))
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model



",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 1e-05
)",8.0,28342833.0,17.0,16.0,0.250266,0.245164,0.8621,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'mixed_data_imgsize_256'

        self.img_size = 256

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 8
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50",6190.96,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image = crop_image_from_gray(image)
        image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 45), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'           #### NEED TO CHANGED IT!!!!!!!!!!!!!!!!!!!
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_BEN_preprocessing/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))
    print(""Training path: {}"".format(train_path))
    # Data Processing


    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [1e-3]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b5']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb9d1066a0>
2019.08.19 19:16:52,new,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        self.model = None
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'ResNet152':
            model = models.resnet152(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet152.pth""))
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model



",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: True
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 1e-05
)",8.0,28342833.0,22.0,16.0,0.18742,0.258331,0.8591,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'mixed_data_imgsize_256'

        self.img_size = 256

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5, amsgrad=True)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 8
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50",3575.39,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image = crop_image_from_gray(image)
        image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'           #### NEED TO CHANGED IT!!!!!!!!!!!!!!!!!!!
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_BEN_preprocessing/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))
    print(""Training path: {}"".format(train_path))
    # Data Processing


    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [1e-3]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b5']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb9d109320>
2019.08.19 22:43:08,new,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        self.model = None
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'ResNet152':
            model = models.resnet152(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet152.pth""))
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model



",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: True
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 1e-05
)",8.0,6514465.0,21.0,16.0,0.160671,0.27978000000000003,0.8662,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'mixed_data_imgsize_256'

        self.img_size = 256

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5, amsgrad=True)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 8
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50",3406.88,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image = crop_image_from_gray(image)
        image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'           #### NEED TO CHANGED IT!!!!!!!!!!!!!!!!!!!
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_BEN_preprocessing/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))
    print(""Training path: {}"".format(train_path))
    # Data Processing


    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [1e-3]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b1']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb960046a0>
2019.08.19 23:47:05,new,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        self.model = None
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'ResNet152':
            model = models.resnet152(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet152.pth""))
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model



",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 1e-05
)",8.0,6514465.0,29.0,16.0,0.133184,0.276302,0.8707,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'mixed_data_imgsize_256'

        self.img_size = 256

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 8
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50",643.62,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        # image = crop_image_from_gray(image)
        # image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        # image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_mixed_BEN_preprocessing/'           #### NEED TO CHANGED IT!!!!!!!!!!!!!!!!!!!
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_BEN_preprocessing/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))
    print(""Training path: {}"".format(train_path))
    # Data Processing


    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [1e-3]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b1']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb96002908>
2019.08.20 01:14:06,new,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        self.model = None
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'ResNet152':
            model = models.resnet152(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet152.pth""))
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1024),
                # nn.BatchNorm1d(1024),
                nn.Dropout(0.4),
                nn.Linear(1024, 1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model



",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 1e-05
)",8.0,30439985.0,22.0,16.0,0.22435,0.259116,0.8567,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'mixed_data_imgsize_256'

        self.img_size = 256

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 8
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50",1091.85,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        # image = crop_image_from_gray(image)
        # image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        # image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_mixed_BEN_preprocessing/'           #### NEED TO CHANGED IT!!!!!!!!!!!!!!!!!!!
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_BEN_preprocessing/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))
    print(""Training path: {}"".format(train_path))
    # Data Processing


    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [1e-3]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b5']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb9d102780>
2019.08.20 01:33:34,new,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        self.model = None
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'ResNet152':
            model = models.resnet152(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet152.pth""))
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=512),
                # nn.BatchNorm1d(1024),
                nn.Dropout(0.1),
                nn.Linear(512, 1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model



",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 1e-05
)",8.0,29390385.0,22.0,16.0,0.17160699999999998,0.274634,0.8534,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'mixed_data_imgsize_256'

        self.img_size = 256

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 8
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50",1094.76,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        # image = crop_image_from_gray(image)
        # image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        # image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_mixed_BEN_preprocessing/'           #### NEED TO CHANGED IT!!!!!!!!!!!!!!!!!!!
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_BEN_preprocessing/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))
    print(""Training path: {}"".format(train_path))
    # Data Processing


    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [1e-3]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b5']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb9d1037b8>
2019.08.20 01:55:21,new,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        self.model = None
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'ResNet152':
            model = models.resnet152(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet152.pth""))
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.8),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model



",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 1e-05
)",8.0,28342833.0,22.0,16.0,0.24824000000000002,0.234105,0.8558,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'mixed_data_imgsize_256'

        self.img_size = 256

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 8
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 100
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50",1082.39,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        # image = crop_image_from_gray(image)
        # image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        # image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_mixed_BEN_preprocessing/'           #### NEED TO CHANGED IT!!!!!!!!!!!!!!!!!!!
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_BEN_preprocessing/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))
    print(""Training path: {}"".format(train_path))
    # Data Processing


    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [1e-3]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b5']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb9d102630>
2019.08.20 14:14:31,new,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        self.model = None
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'ResNet152':
            model = models.resnet152(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet152.pth""))
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model



",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 1e-05
)",10.0,28342833.0,32.0,16.0,0.084013,0.262158,0.8895,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'mixed_data_imgsize_256'

        self.img_size = 260

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 10
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 150
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50",5222.15,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image = crop_image_from_gray(image)
        image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'           #### NEED TO CHANGED IT!!!!!!!!!!!!!!!!!!!
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_BEN_preprocessing/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))
    print(""Training path: {}"".format(train_path))
    # Data Processing


    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [1e-3]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b5']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb9d106668>
2019.08.20 16:36:04,new,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        self.model = None
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'ResNet152':
            model = models.resnet152(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet152.pth""))
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model



",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 1e-05
)",10.0,7702403.0,21.0,16.0,0.276091,0.26455100000000004,0.8615,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'mixed_data_imgsize_260'

        self.img_size = 260

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 10
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 150
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50",3377.42,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image = crop_image_from_gray(image)
        image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'           #### NEED TO CHANGED IT!!!!!!!!!!!!!!!!!!!
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_BEN_preprocessing/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))
    print(""Training path: {}"".format(train_path))
    # Data Processing


    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [1e-3]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b2']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb9d1015f8>
2019.08.20 18:39:33,new,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        self.model = None
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'ResNet152':
            model = models.resnet152(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet152.pth""))
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model



",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 1e-05
)",10.0,10697769.0,28.0,16.0,0.106745,0.263581,0.8812,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'mixed_data_imgsize_260'

        self.img_size = 300

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 10
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 150
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50",4847.1,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image = crop_image_from_gray(image)
        image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'           #### NEED TO CHANGED IT!!!!!!!!!!!!!!!!!!!
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_BEN_preprocessing/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))
    print(""Training path: {}"".format(train_path))
    # Data Processing


    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [1e-3]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b3']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb9d103828>
2019.08.20 20:10:25,new,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        self.model = None
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'ResNet152':
            model = models.resnet152(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet152.pth""))
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model



",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 1e-05
)",10.0,28342833.0,150.0,4.0,0.384089,0.324736,0.742,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'mixed_data_imgsize_260'

        self.img_size = 460

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 10
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 150
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50",,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image = crop_image_from_gray(image)
        image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'           #### NEED TO CHANGED IT!!!!!!!!!!!!!!!!!!!
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_BEN_preprocessing/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))
    print(""Training path: {}"".format(train_path))
    # Data Processing


    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [4]
    lr_list = [1e-3]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b5']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb9d108780>
2019.08.20 20:38:41,new,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        self.model = None
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'ResNet152':
            model = models.resnet152(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet152.pth""))
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model



",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 1e-05
)",10.0,28342833.0,35.0,4.0,0.176984,0.241886,0.7909999999999999,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'mixed_data_imgsize_260'

        self.img_size = 456

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 10
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 150
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50",8790.93,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image = crop_image_from_gray(image)
        image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'           #### NEED TO CHANGED IT!!!!!!!!!!!!!!!!!!!
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_BEN_preprocessing/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))
    print(""Training path: {}"".format(train_path))
    # Data Processing


    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [4]
    lr_list = [1e-3]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b5']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb9d102748>
2019.08.20 23:28:36,new,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        self.model = None
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'ResNet152':
            model = models.resnet152(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet152.pth""))
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b6':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b6-c76e70fd.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b7':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b7-dcc49843.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model


",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 1e-05
)",10.0,40738009.0,31.0,2.0,0.265694,0.35191,0.5924,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'mixed_data_imgsize_260'

        self.img_size = 528

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 10
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 150
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50",12406.79,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image = crop_image_from_gray(image)
        image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'           #### NEED TO CHANGED IT!!!!!!!!!!!!!!!!!!!
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_BEN_preprocessing/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))
    print(""Training path: {}"".format(train_path))
    # Data Processing


    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [2]
    lr_list = [1e-3]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b6']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb9d107748>
2019.08.21 10:42:52,new,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        self.model = None
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'ResNet152':
            model = models.resnet152(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet152.pth""))
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b6':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b6-c76e70fd.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b7':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b7-dcc49843.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model


",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 1e-05
)",10.0,63789521.0,150.0,1.0,0.767351,4.013189,0.0,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'mixed_data_imgsize_260'

        self.img_size = 600

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 10
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 150
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50",,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image = crop_image_from_gray(image)
        image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'           #### NEED TO CHANGED IT!!!!!!!!!!!!!!!!!!!
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_BEN_preprocessing/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))
    print(""Training path: {}"".format(train_path))
    # Data Processing


    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [1]
    lr_list = [1e-3]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b7']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb9d107588>
2019.08.21 14:26:12,new,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        self.model = None
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'ResNet152':
            model = models.resnet152(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet152.pth""))
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b6':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b6-c76e70fd.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b7':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b7-dcc49843.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model


",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 1e-05
)",10.0,10697769.0,18.0,16.0,0.23050500000000002,0.264461,0.8613,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'mixed_data_imgsize_260'

        self.img_size = 300

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 10
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 150
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50",9177.49,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image = crop_image_from_gray(image)
        image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 50), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'           #### NEED TO CHANGED IT!!!!!!!!!!!!!!!!!!!
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_BEN_preprocessing/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))
    print(""Training path: {}"".format(train_path))
    # Data Processing


    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [1e-3]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b3']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb9d1dac50>
2019.08.21 19:15:09,new,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        self.model = None
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'ResNet152':
            model = models.resnet152(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet152.pth""))
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b6':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b6-c76e70fd.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b7':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b7-dcc49843.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model


",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 1e-05
)",10.0,10697769.0,20.0,16.0,0.249148,0.272864,0.8734,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'mixed_data_imgsize_300'
        self.img_size = 300

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 10
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 150
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50",856.93,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        # image = crop_image_from_gray(image)
        image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        # image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'           #### NEED TO CHANGED IT!!!!!!!!!!!!!!!!!!!
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_BEN_preprocessing/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))
    print(""Training path: {}"".format(train_path))
    # Data Processing


    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [1e-3]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b3']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb9d103828>
2019.08.21 19:35:31,new_old_mixed,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        self.model = None
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'ResNet152':
            model = models.resnet152(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet152.pth""))
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b6':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b6-c76e70fd.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b7':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b7-dcc49843.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model


",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 1e-05
)",10.0,10697769.0,40.0,16.0,0.259898,0.30827,0.7169,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'mixed_data_imgsize_300'
        self.img_size = 300

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 10
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 150
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new_old_mixed'

        ## PRINT FREQUENCY
        self.print_frequency = 50",15102.3,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        # image = crop_image_from_gray(image)
        image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        # image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'           #### NEED TO CHANGED IT!!!!!!!!!!!!!!!!!!!
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_BEN_preprocessing/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))
    print(""Training path: {}"".format(train_path))
    # Data Processing


    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [1e-3]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b3']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb9d1017b8>
2019.08.23 00:39:42,new,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        self.model = None
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'ResNet152':
            model = models.resnet152(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet152.pth""))
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b6':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b6-c76e70fd.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b7':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b7-dcc49843.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model


",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 1e-05
)",10.0,10697769.0,150.0,16.0,0.6800149999999999,0.4173560000000001,0.8422,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'mixed_data_imgsize_300'
        self.img_size = 300

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 10
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 150
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50",,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config
import math

# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p



def subtract_median_bg_image(im):
    k = np.max(im.shape)//20*2+1
    bg = cv2.medianBlur(im, k)
    return cv2.addWeighted (im, 4, bg, -4, 128)

PARAM = 96
def Radius_Reduction(img,PARAM):
    h,w,c=img.shape
    Frame=np.zeros((h,w,c),dtype=np.uint8)
    cv2.circle(Frame,(int(math.floor(w/2)),int(math.floor(h/2))),int(math.floor((h*PARAM)/float(2*100))), (255,255,255), -1)
    Frame1=cv2.cvtColor(Frame, cv2.COLOR_BGR2GRAY)
    img1 =cv2.bitwise_and(img,img,mask=Frame1)
    return img1

class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        # image = crop_image_from_gray(image)
        image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        sub_med = subtract_median_bg_image(image)
        img_rad_red = Radius_Reduction(sub_med, PARAM)
        image = transforms.ToPILImage()(img_rad_red)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'           #### NEED TO CHANGED IT!!!!!!!!!!!!!!!!!!!
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_BEN_preprocessing/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))
    print(""Training path: {}"".format(train_path))
    # Data Processing


    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [1e-3]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b3']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb9d10d710>
2019.08.23 00:41:53,new,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        self.model = None
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'ResNet152':
            model = models.resnet152(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet152.pth""))
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b6':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b6-c76e70fd.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b7':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b7-dcc49843.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model


",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 1e-05
)",10.0,28342833.0,150.0,16.0,0.711697,1.025648,0.7814,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'mixed_data_imgsize_300'
        self.img_size = 256

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 10
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 150
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50",,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config
import math

# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p



def subtract_median_bg_image(im):
    k = np.max(im.shape)//20*2+1
    bg = cv2.medianBlur(im, k)
    return cv2.addWeighted (im, 4, bg, -4, 128)

PARAM = 96
def Radius_Reduction(img,PARAM):
    h,w,c=img.shape
    Frame=np.zeros((h,w,c),dtype=np.uint8)
    cv2.circle(Frame,(int(math.floor(w/2)),int(math.floor(h/2))),int(math.floor((h*PARAM)/float(2*100))), (255,255,255), -1)
    Frame1=cv2.cvtColor(Frame, cv2.COLOR_BGR2GRAY)
    img1 =cv2.bitwise_and(img,img,mask=Frame1)
    return img1

class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        # image = crop_image_from_gray(image)
        image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        sub_med = subtract_median_bg_image(image)
        img_rad_red = Radius_Reduction(sub_med, PARAM)
        image = transforms.ToPILImage()(img_rad_red)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'           #### NEED TO CHANGED IT!!!!!!!!!!!!!!!!!!!
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_BEN_preprocessing/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))
    print(""Training path: {}"".format(train_path))
    # Data Processing


    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [1e-3]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b5']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb9d10c518>
2019.08.23 00:44:55,new,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        self.model = None
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'ResNet152':
            model = models.resnet152(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet152.pth""))
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b6':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b6-c76e70fd.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b7':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b7-dcc49843.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model


",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 1e-05
)",10.0,28342833.0,150.0,4.0,0.90671,0.8359979999999999,0.6547,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'mixed_data_imgsize_300'
        self.img_size = 456

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 10
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 150
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50",,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config
import math

# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p



def subtract_median_bg_image(im):
    k = np.max(im.shape)//20*2+1
    bg = cv2.medianBlur(im, k)
    return cv2.addWeighted (im, 4, bg, -4, 128)

PARAM = 96
def Radius_Reduction(img,PARAM):
    h,w,c=img.shape
    Frame=np.zeros((h,w,c),dtype=np.uint8)
    cv2.circle(Frame,(int(math.floor(w/2)),int(math.floor(h/2))),int(math.floor((h*PARAM)/float(2*100))), (255,255,255), -1)
    Frame1=cv2.cvtColor(Frame, cv2.COLOR_BGR2GRAY)
    img1 =cv2.bitwise_and(img,img,mask=Frame1)
    return img1

class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        # image = crop_image_from_gray(image)
        image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        sub_med = subtract_median_bg_image(image)
        img_rad_red = Radius_Reduction(sub_med, PARAM)
        image = transforms.ToPILImage()(img_rad_red)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'           #### NEED TO CHANGED IT!!!!!!!!!!!!!!!!!!!
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_BEN_preprocessing/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))
    print(""Training path: {}"".format(train_path))
    # Data Processing


    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [4]
    lr_list = [1e-3]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b5']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb9d10c6a0>
2019.08.23 00:48:36,new,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        self.model = None
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'ResNet152':
            model = models.resnet152(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet152.pth""))
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b6':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b6-c76e70fd.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b7':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b7-dcc49843.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model


",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 1e-05
)",10.0,10697769.0,25.0,16.0,0.163141,0.28434899999999996,0.8605,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'mixed_data_imgsize_300'
        self.img_size = 300

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 10
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 150
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50",1039.63,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config
import math

# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p



def subtract_median_bg_image(im):
    k = np.max(im.shape)//20*2+1
    bg = cv2.medianBlur(im, k)
    return cv2.addWeighted (im, 4, bg, -4, 128)

PARAM = 96
def Radius_Reduction(img,PARAM):
    h,w,c=img.shape
    Frame=np.zeros((h,w,c),dtype=np.uint8)
    cv2.circle(Frame,(int(math.floor(w/2)),int(math.floor(h/2))),int(math.floor((h*PARAM)/float(2*100))), (255,255,255), -1)
    Frame1=cv2.cvtColor(Frame, cv2.COLOR_BGR2GRAY)
    img1 =cv2.bitwise_and(img,img,mask=Frame1)
    return img1

class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        # image = crop_image_from_gray(image)
        image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        sub_med = subtract_median_bg_image(image)
        img_rad_red = Radius_Reduction(sub_med, PARAM)
        image = transforms.ToPILImage()(img_rad_red)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'           #### NEED TO CHANGED IT!!!!!!!!!!!!!!!!!!!
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_BEN_preprocessing/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))
    print(""Training path: {}"".format(train_path))
    # Data Processing


    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [1e-3]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b3']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb9d10c6a0>
2019.08.23 01:07:55,new,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        self.model = None
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'ResNet152':
            model = models.resnet152(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet152.pth""))
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b6':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b6-c76e70fd.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b7':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b7-dcc49843.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model


",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 1e-05
)",10.0,7702403.0,24.0,16.0,0.200474,0.26541,0.866,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'mixed_data_imgsize_300'
        self.img_size = 260

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 10
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 150
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50",755.25,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config
import math

# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p



def subtract_median_bg_image(im):
    k = np.max(im.shape)//20*2+1
    bg = cv2.medianBlur(im, k)
    return cv2.addWeighted (im, 4, bg, -4, 128)

PARAM = 96
def Radius_Reduction(img,PARAM):
    h,w,c=img.shape
    Frame=np.zeros((h,w,c),dtype=np.uint8)
    cv2.circle(Frame,(int(math.floor(w/2)),int(math.floor(h/2))),int(math.floor((h*PARAM)/float(2*100))), (255,255,255), -1)
    Frame1=cv2.cvtColor(Frame, cv2.COLOR_BGR2GRAY)
    img1 =cv2.bitwise_and(img,img,mask=Frame1)
    return img1

class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        # image = crop_image_from_gray(image)
        image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        sub_med = subtract_median_bg_image(image)
        img_rad_red = Radius_Reduction(sub_med, PARAM)
        image = transforms.ToPILImage()(img_rad_red)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'           #### NEED TO CHANGED IT!!!!!!!!!!!!!!!!!!!
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_BEN_preprocessing/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))
    print(""Training path: {}"".format(train_path))
    # Data Processing


    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [1e-3]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b2']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb9d10b710>
2019.08.23 01:37:15,new_old_mixed,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        self.model = None
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'ResNet152':
            model = models.resnet152(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet152.pth""))
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b6':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b6-c76e70fd.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b7':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b7-dcc49843.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model


",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 1e-05
)",10.0,7702403.0,49.0,16.0,0.268542,0.347943,0.6754,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'mixed_data_imgsize_300'
        self.img_size = 260

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 10
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 150
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new_old_mixed'

        ## PRINT FREQUENCY
        self.print_frequency = 50",12469.81,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config
import math

# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p



def subtract_median_bg_image(im):
    k = np.max(im.shape)//20*2+1
    bg = cv2.medianBlur(im, k)
    return cv2.addWeighted (im, 4, bg, -4, 128)

PARAM = 96
def Radius_Reduction(img,PARAM):
    h,w,c=img.shape
    Frame=np.zeros((h,w,c),dtype=np.uint8)
    cv2.circle(Frame,(int(math.floor(w/2)),int(math.floor(h/2))),int(math.floor((h*PARAM)/float(2*100))), (255,255,255), -1)
    Frame1=cv2.cvtColor(Frame, cv2.COLOR_BGR2GRAY)
    img1 =cv2.bitwise_and(img,img,mask=Frame1)
    return img1

class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        # image = crop_image_from_gray(image)
        image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        sub_med = subtract_median_bg_image(image)
        img_rad_red = Radius_Reduction(sub_med, PARAM)
        image = transforms.ToPILImage()(img_rad_red)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'           #### NEED TO CHANGED IT!!!!!!!!!!!!!!!!!!!
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_BEN_preprocessing/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))
    print(""Training path: {}"".format(train_path))
    # Data Processing


    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [1e-3]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b2']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb9d10e668>
2019.08.23 09:29:07,new_old_mixed,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        self.model = None
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'ResNet152':
            model = models.resnet152(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet152.pth""))
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b6':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b6-c76e70fd.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b7':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b7-dcc49843.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model


",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 1e-05
)",10.0,10697769.0,58.0,16.0,0.25281,0.31452800000000003,0.7221,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'mixed_data_imgsize_300'
        self.img_size = 300

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 10
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 150
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new_old_mixed'

        ## PRINT FREQUENCY
        self.print_frequency = 50",21598.51,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config
import math

# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p



def subtract_median_bg_image(im):
    k = np.max(im.shape)//20*2+1
    bg = cv2.medianBlur(im, k)
    return cv2.addWeighted (im, 4, bg, -4, 128)

PARAM = 96
def Radius_Reduction(img,PARAM):
    h,w,c=img.shape
    Frame=np.zeros((h,w,c),dtype=np.uint8)
    cv2.circle(Frame,(int(math.floor(w/2)),int(math.floor(h/2))),int(math.floor((h*PARAM)/float(2*100))), (255,255,255), -1)
    Frame1=cv2.cvtColor(Frame, cv2.COLOR_BGR2GRAY)
    img1 =cv2.bitwise_and(img,img,mask=Frame1)
    return img1

class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        # image = crop_image_from_gray(image)
        image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        sub_med = subtract_median_bg_image(image)
        img_rad_red = Radius_Reduction(sub_med, PARAM)
        image = transforms.ToPILImage()(img_rad_red)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'           #### NEED TO CHANGED IT!!!!!!!!!!!!!!!!!!!
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_BEN_preprocessing/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))
    print(""Training path: {}"".format(train_path))
    # Data Processing


    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [1e-3]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b3']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb9d10c6a0>
2019.08.23 16:30:13,new,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        self.model = None
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'ResNet152':
            model = models.resnet152(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet152.pth""))
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b6':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b6-c76e70fd.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b7':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b7-dcc49843.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model


",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 1e-05
)",10.0,28342833.0,26.0,16.0,0.20659499999999997,0.291451,0.8579,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'mixed_data_imgsize_300'
        self.img_size = 256

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 10
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 150
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50",1426.43,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config
import math

# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p



def subtract_median_bg_image(im):
    k = np.max(im.shape)//20*2+1
    bg = cv2.medianBlur(im, k)
    return cv2.addWeighted (im, 4, bg, -4, 128)

PARAM = 96
def Radius_Reduction(img,PARAM):
    h,w,c=img.shape
    Frame=np.zeros((h,w,c),dtype=np.uint8)
    cv2.circle(Frame,(int(math.floor(w/2)),int(math.floor(h/2))),int(math.floor((h*PARAM)/float(2*100))), (255,255,255), -1)
    Frame1=cv2.cvtColor(Frame, cv2.COLOR_BGR2GRAY)
    img1 =cv2.bitwise_and(img,img,mask=Frame1)
    return img1

class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        # image = crop_image_from_gray(image)
        image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        sub_med = subtract_median_bg_image(image)
        img_rad_red = Radius_Reduction(sub_med, PARAM)
        image = transforms.ToPILImage()(img_rad_red)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'           #### NEED TO CHANGED IT!!!!!!!!!!!!!!!!!!!
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_BEN_preprocessing/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))
    print(""Training path: {}"".format(train_path))
    # Data Processing


    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [1e-3]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b5']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb9d10e630>
2019.08.23 18:36:04,new,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        self.model = None
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'ResNet152':
            model = models.resnet152(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet152.pth""))
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b6':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b6-c76e70fd.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b7':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b7-dcc49843.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model


",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 1e-05
)",10.0,28342833.0,150.0,16.0,0.563883,0.35315100000000005,0.7729,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'mixed_data_imgsize_300'
        self.img_size = 400
        self.img_size_crop = 256

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 10
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 150
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50",,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config
import math

# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p



def subtract_median_bg_image(im):
    k = np.max(im.shape)//20*2+1
    bg = cv2.medianBlur(im, k)
    return cv2.addWeighted (im, 4, bg, -4, 128)

PARAM = 96
def Radius_Reduction(img,PARAM):
    h,w,c=img.shape
    Frame=np.zeros((h,w,c),dtype=np.uint8)
    cv2.circle(Frame,(int(math.floor(w/2)),int(math.floor(h/2))),int(math.floor((h*PARAM)/float(2*100))), (255,255,255), -1)
    Frame1=cv2.cvtColor(Frame, cv2.COLOR_BGR2GRAY)
    img1 =cv2.bitwise_and(img,img,mask=Frame1)
    return img1

class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image = crop_image_from_gray(image)
        image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.RandomResizedCrop(cfg.img_size_crop),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'           #### NEED TO CHANGED IT!!!!!!!!!!!!!!!!!!!
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_BEN_preprocessing/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))
    print(""Training path: {}"".format(train_path))
    # Data Processing


    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [1e-3]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b5']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb9d110668>
2019.08.23 18:52:31,new,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        self.model = None
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'ResNet152':
            model = models.resnet152(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet152.pth""))
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b6':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b6-c76e70fd.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b7':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b7-dcc49843.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model


",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 1e-05
)",10.0,28342833.0,30.0,16.0,0.268614,0.293101,0.8467,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'mixed_data_imgsize_300'
        self.img_size = 400
        self.img_size_crop = 256

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 10
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 150
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50",5627.15,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config
import math

# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p



def subtract_median_bg_image(im):
    k = np.max(im.shape)//20*2+1
    bg = cv2.medianBlur(im, k)
    return cv2.addWeighted (im, 4, bg, -4, 128)

PARAM = 96
def Radius_Reduction(img,PARAM):
    h,w,c=img.shape
    Frame=np.zeros((h,w,c),dtype=np.uint8)
    cv2.circle(Frame,(int(math.floor(w/2)),int(math.floor(h/2))),int(math.floor((h*PARAM)/float(2*100))), (255,255,255), -1)
    Frame1=cv2.cvtColor(Frame, cv2.COLOR_BGR2GRAY)
    img1 =cv2.bitwise_and(img,img,mask=Frame1)
    return img1

class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image = crop_image_from_gray(image)
        image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.RandomResizedCrop(cfg.img_size_crop),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'           #### NEED TO CHANGED IT!!!!!!!!!!!!!!!!!!!
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_BEN_preprocessing/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))
    print(""Training path: {}"".format(train_path))
    # Data Processing


    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_train) # need to change It!!!!!

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [1e-3]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b5']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb9d110588>
2019.08.23 20:43:23,new,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        self.model = None
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'ResNet152':
            model = models.resnet152(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet152.pth""))
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b6':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b6-c76e70fd.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b7':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b7-dcc49843.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model


",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 1e-05
)",10.0,7702403.0,40.0,16.0,0.254927,0.302655,0.8432,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'mixed_data_imgsize_300'
        self.img_size = 300
        self.img_size_crop = 260

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 10
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 150
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50",6776.53,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config
import math

# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p



def subtract_median_bg_image(im):
    k = np.max(im.shape)//20*2+1
    bg = cv2.medianBlur(im, k)
    return cv2.addWeighted (im, 4, bg, -4, 128)

PARAM = 96
def Radius_Reduction(img,PARAM):
    h,w,c=img.shape
    Frame=np.zeros((h,w,c),dtype=np.uint8)
    cv2.circle(Frame,(int(math.floor(w/2)),int(math.floor(h/2))),int(math.floor((h*PARAM)/float(2*100))), (255,255,255), -1)
    Frame1=cv2.cvtColor(Frame, cv2.COLOR_BGR2GRAY)
    img1 =cv2.bitwise_and(img,img,mask=Frame1)
    return img1

class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image = crop_image_from_gray(image)
        image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.RandomResizedCrop(cfg.img_size_crop),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'           #### NEED TO CHANGED IT!!!!!!!!!!!!!!!!!!!
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_BEN_preprocessing/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))
    print(""Training path: {}"".format(train_path))
    # Data Processing


    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_train) # need to change It!!!!!

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [1e-3]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b2']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb9d1dab38>
2019.08.24 14:29:02,new,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        self.model = None
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'ResNet152':
            model = models.resnet152(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet152.pth""))
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b6':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b6-c76e70fd.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b7':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b7-dcc49843.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model


",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 1e-05
)",10.0,7702403.0,43.0,16.0,0.207753,0.239033,0.8884,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'mixed_data_imgsize_300'
        self.img_size = 256
        self.img_size_crop = 300

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 10
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 150
        self.batch_size = batch_size
        self.valid_type = 'holdout' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50",816.13,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config
import math

# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p



def subtract_median_bg_image(im):
    k = np.max(im.shape)//20*2+1
    bg = cv2.medianBlur(im, k)
    return cv2.addWeighted (im, 4, bg, -4, 128)

PARAM = 96
def Radius_Reduction(img,PARAM):
    h,w,c=img.shape
    Frame=np.zeros((h,w,c),dtype=np.uint8)
    cv2.circle(Frame,(int(math.floor(w/2)),int(math.floor(h/2))),int(math.floor((h*PARAM)/float(2*100))), (255,255,255), -1)
    Frame1=cv2.cvtColor(Frame, cv2.COLOR_BGR2GRAY)
    img1 =cv2.bitwise_and(img,img,mask=Frame1)
    return img1

class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        # image = crop_image_from_gray(image)
        # image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        # image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        # transforms.RandomResizedCrop(cfg.img_size_crop),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedKFold

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_mixed_BEN_preprocessing/'           #### NEED TO CHANGED IT!!!!!!!!!!!!!!!!!!!
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_BEN_preprocessing/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))
    print(""Training path: {}"".format(train_path))
    # Data Processing


    ## SHUFFLE DATA
    skf = None
    # if cfg.valid_type == 'holdout':
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True,
                                                                   random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid) # need to change It!!!!!
    # elif cfg.valid_type == 'cv':
    #     skf = StratifiedKFold(n_splits=cfg.n_folds, shuffle=True, random_state=cfg.seed)



    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model
    cfg.model.load_state_dict(
        torch.load('./Model_weights_finetuning/pretrained_on_old_img256_b2_without_crop.pth')['model'])

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()

    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [1e-3]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b2']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb9600b860>
2019.08.24 23:19:36,new,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        self.model = None
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'ResNet152':
            model = models.resnet152(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet152.pth""))
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b6':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b6-c76e70fd.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b7':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b7-dcc49843.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model


",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 1e-05
)",10.0,28342833.0,35.0,16.0,0.225246,0.24684099999999998,0.8813,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'mixed_data_imgsize_300'
        self.img_size = 256
        self.img_size_crop = 300

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 10
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 150
        self.batch_size = batch_size
        self.valid_type = 'holdout' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50",1408.04,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config
import math

# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p



def subtract_median_bg_image(im):
    k = np.max(im.shape)//20*2+1
    bg = cv2.medianBlur(im, k)
    return cv2.addWeighted (im, 4, bg, -4, 128)

PARAM = 96
def Radius_Reduction(img,PARAM):
    h,w,c=img.shape
    Frame=np.zeros((h,w,c),dtype=np.uint8)
    cv2.circle(Frame,(int(math.floor(w/2)),int(math.floor(h/2))),int(math.floor((h*PARAM)/float(2*100))), (255,255,255), -1)
    Frame1=cv2.cvtColor(Frame, cv2.COLOR_BGR2GRAY)
    img1 =cv2.bitwise_and(img,img,mask=Frame1)
    return img1

class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        # image = crop_image_from_gray(image)
        # image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        # image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        # transforms.RandomResizedCrop(cfg.img_size_crop),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedKFold

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_mixed_BEN_preprocessing/'           #### NEED TO CHANGED IT!!!!!!!!!!!!!!!!!!!
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_BEN_preprocessing/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))
    print(""Training path: {}"".format(train_path))
    # Data Processing


    ## SHUFFLE DATA
    skf = None
    # if cfg.valid_type == 'holdout':
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True,
                                                                   random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid) # need to change It!!!!!
    # elif cfg.valid_type == 'cv':
    #     skf = StratifiedKFold(n_splits=cfg.n_folds, shuffle=True, random_state=cfg.seed)



    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model
    cfg.model.load_state_dict(
        torch.load('./Model_weights_finetuning/pretrained_on_old_img256_b5.pth')['model'])

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()

    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [1e-3]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b5']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb95fa4be0>
2019.08.25 01:07:43,new,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        self.model = None
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'ResNet152':
            model = models.resnet152(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet152.pth""))
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=5)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b6':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b6-c76e70fd.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b7':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b7-dcc49843.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model


",CrossEntropyLoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 1e-05
)",10.0,28351029.0,26.0,16.0,0.304179,0.428101,0.3404,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'mixed_data_imgsize_300'
        self.img_size = 256
        self.img_size_crop = 300

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.CrossEntropyLoss()
        # self.num_classes = 5
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 10
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP

        self.num_workers = 16
        self.n_epochs = 150
        self.batch_size = batch_size
        self.valid_type = 'holdout' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50",1041.59,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config
import math

# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p



def subtract_median_bg_image(im):
    k = np.max(im.shape)//20*2+1
    bg = cv2.medianBlur(im, k)
    return cv2.addWeighted (im, 4, bg, -4, 128)

PARAM = 96
def Radius_Reduction(img,PARAM):
    h,w,c=img.shape
    Frame=np.zeros((h,w,c),dtype=np.uint8)
    cv2.circle(Frame,(int(math.floor(w/2)),int(math.floor(h/2))),int(math.floor((h*PARAM)/float(2*100))), (255,255,255), -1)
    Frame1=cv2.cvtColor(Frame, cv2.COLOR_BGR2GRAY)
    img1 =cv2.bitwise_and(img,img,mask=Frame1)
    return img1

class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        # image = crop_image_from_gray(image)
        # image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        # image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        # transforms.RandomResizedCrop(cfg.img_size_crop),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedKFold

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_mixed_BEN_preprocessing/'           #### NEED TO CHANGED IT!!!!!!!!!!!!!!!!!!!
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_BEN_preprocessing/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))
    print(""Training path: {}"".format(train_path))
    # Data Processing


    ## SHUFFLE DATA
    skf = None
    # if cfg.valid_type == 'holdout':
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True,
                                                                   random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid) # need to change It!!!!!
    # elif cfg.valid_type == 'cv':
    #     skf = StratifiedKFold(n_splits=cfg.n_folds, shuffle=True, random_state=cfg.seed)



    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model
    # cfg.model.load_state_dict(
        # torch.load('./Model_weights_finetuning/pretrained_on_old_img256_b5.pth')['model'])

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()

    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().long()
            target = target.view(-1)#, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().long()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1)#, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [1e-3]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b5']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb9d10c6a0>
2019.08.25 08:46:13,new,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        self.model = None
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'ResNet152':
            model = models.resnet152(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet152.pth""))
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b6':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b6-c76e70fd.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b7':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b7-dcc49843.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model


",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 1e-05
)",10.0,6514465.0,26.0,12.0,0.203113,0.215811,0.8888,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'mixed_data_imgsize_300'
        self.img_size = 480
        # self.img_size_crop = 300

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        # self.num_classes = 5
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 10
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP

        self.num_workers = 16
        self.n_epochs = 150
        self.batch_size = batch_size
        self.valid_type = 'holdout' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50",1389.88,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config
import math

# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p



def subtract_median_bg_image(im):
    k = np.max(im.shape)//20*2+1
    bg = cv2.medianBlur(im, k)
    return cv2.addWeighted (im, 4, bg, -4, 128)

PARAM = 96
def Radius_Reduction(img,PARAM):
    h,w,c=img.shape
    Frame=np.zeros((h,w,c),dtype=np.uint8)
    cv2.circle(Frame,(int(math.floor(w/2)),int(math.floor(h/2))),int(math.floor((h*PARAM)/float(2*100))), (255,255,255), -1)
    Frame1=cv2.cvtColor(Frame, cv2.COLOR_BGR2GRAY)
    img1 =cv2.bitwise_and(img,img,mask=Frame1)
    return img1

def info_image(im):
    # Compute the center (cx, cy) and radius of the eye
    cy = im.shape[0]//2
    midline = im[cy,:]
    midline = np.where(midline>midline.mean()/3)[0]
    if len(midline)>im.shape[1]//2:
        x_start, x_end = np.min(midline), np.max(midline)
    else: # This actually rarely happens p~1/10000
        x_start, x_end = im.shape[1]//10, 9*im.shape[1]//10
    cx = (x_start + x_end)/2
    r = (x_end - x_start)/2
    return cx, cy, r

def resize_image(im, image_size, augmentation=False):
    # Crops, resizes and potentially augments the image to IMAGE_SIZE
    cx, cy, r = info_image(im)
    scaling = image_size/(2*r)
    rotation = 0
    if augmentation:
        scaling *= 1 + 0.3 * (np.random.rand()-0.5)
        rotation = 360 * np.random.rand()
    M = cv2.getRotationMatrix2D((cx,cy), rotation, scaling)
    M[0,2] -= cx - image_size/2
    M[1,2] -= cy - image_size/2
    return cv2.warpAffine(im,M,(image_size, image_size))

class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        image = resize_image(image, cfg.img_size)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-15, 15)),
        # transforms.RandomResizedCrop(cfg.img_size_crop),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedKFold

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'           #### NEED TO CHANGED IT!!!!!!!!!!!!!!!!!!!
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_BEN_preprocessing/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))
    print(""Training path: {}"".format(train_path))
    # Data Processing


    ## SHUFFLE DATA
    skf = None
    # if cfg.valid_type == 'holdout':
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True,
                                                                   random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid) # need to change It!!!!!
    # elif cfg.valid_type == 'cv':
    #     skf = StratifiedKFold(n_splits=cfg.n_folds, shuffle=True, random_state=cfg.seed)



    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model
    cfg.model.load_state_dict(
        torch.load('./Model_weights_finetuning/pretrained_on_old_img480_with_cropto240_b1.pth')['model'])

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()

    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [12]
    lr_list = [1e-3]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b1']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb96004ac8>
2019.08.25 09:44:40,new,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        self.model = None
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'ResNet152':
            model = models.resnet152(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet152.pth""))
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                # nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b6':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b6-c76e70fd.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b7':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b7-dcc49843.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model


",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 1e-05
)",10.0,6514465.0,150.0,12.0,0.257963,0.23876799999999998,0.8844,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'mixed_data_imgsize_300'
        self.img_size = 480
        # self.img_size_crop = 300

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        # self.num_classes = 5
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 10
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP

        self.num_workers = 16
        self.n_epochs = 150
        self.batch_size = batch_size
        self.valid_type = 'holdout' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50",,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config
import math

# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p



def subtract_median_bg_image(im):
    k = np.max(im.shape)//20*2+1
    bg = cv2.medianBlur(im, k)
    return cv2.addWeighted (im, 4, bg, -4, 128)

PARAM = 96
def Radius_Reduction(img,PARAM):
    h,w,c=img.shape
    Frame=np.zeros((h,w,c),dtype=np.uint8)
    cv2.circle(Frame,(int(math.floor(w/2)),int(math.floor(h/2))),int(math.floor((h*PARAM)/float(2*100))), (255,255,255), -1)
    Frame1=cv2.cvtColor(Frame, cv2.COLOR_BGR2GRAY)
    img1 =cv2.bitwise_and(img,img,mask=Frame1)
    return img1

def info_image(im):
    # Compute the center (cx, cy) and radius of the eye
    cy = im.shape[0]//2
    midline = im[cy,:]
    midline = np.where(midline>midline.mean()/3)[0]
    if len(midline)>im.shape[1]//2:
        x_start, x_end = np.min(midline), np.max(midline)
    else: # This actually rarely happens p~1/10000
        x_start, x_end = im.shape[1]//10, 9*im.shape[1]//10
    cx = (x_start + x_end)/2
    r = (x_end - x_start)/2
    return cx, cy, r

def resize_image(im, image_size, augmentation=False):
    # Crops, resizes and potentially augments the image to IMAGE_SIZE
    cx, cy, r = info_image(im)
    scaling = image_size/(2*r)
    rotation = 0
    if augmentation:
        scaling *= 1 + 0.3 * (np.random.rand()-0.5)
        rotation = 360 * np.random.rand()
    M = cv2.getRotationMatrix2D((cx,cy), rotation, scaling)
    M[0,2] -= cx - image_size/2
    M[1,2] -= cy - image_size/2
    return cv2.warpAffine(im,M,(image_size, image_size))

class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        image = resize_image(image, cfg.img_size)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-15, 15)),
        # transforms.RandomResizedCrop(cfg.img_size_crop),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedKFold

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'           #### NEED TO CHANGED IT!!!!!!!!!!!!!!!!!!!
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_BEN_preprocessing/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))
    print(""Training path: {}"".format(train_path))
    # Data Processing


    ## SHUFFLE DATA
    skf = None
    # if cfg.valid_type == 'holdout':
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True,
                                                                   random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid) # need to change It!!!!!
    # elif cfg.valid_type == 'cv':
    #     skf = StratifiedKFold(n_splits=cfg.n_folds, shuffle=True, random_state=cfg.seed)



    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model
    cfg.model.load_state_dict(
        torch.load('./Model_weights_finetuning/finetune3_pretrained_on_old_img480_with_cropto240_b1.pth')['model'])

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()

    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [12]
    lr_list = [1e-4]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b1']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb960079e8>
2019.08.25 10:47:43,new,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        self.model = None
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'ResNet152':
            model = models.resnet152(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet152.pth""))
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                # nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b6':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b6-c76e70fd.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b7':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b7-dcc49843.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model


",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 1e-05
)",10.0,6514465.0,150.0,12.0,0.23725,0.230245,0.8891,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'mixed_data_imgsize_300'
        self.img_size = 480
        # self.img_size_crop = 300

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        # self.num_classes = 5
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 10
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP

        self.num_workers = 16
        self.n_epochs = 150
        self.batch_size = batch_size
        self.valid_type = 'holdout' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50",,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config
import math

# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p



def subtract_median_bg_image(im):
    k = np.max(im.shape)//20*2+1
    bg = cv2.medianBlur(im, k)
    return cv2.addWeighted (im, 4, bg, -4, 128)

PARAM = 96
def Radius_Reduction(img,PARAM):
    h,w,c=img.shape
    Frame=np.zeros((h,w,c),dtype=np.uint8)
    cv2.circle(Frame,(int(math.floor(w/2)),int(math.floor(h/2))),int(math.floor((h*PARAM)/float(2*100))), (255,255,255), -1)
    Frame1=cv2.cvtColor(Frame, cv2.COLOR_BGR2GRAY)
    img1 =cv2.bitwise_and(img,img,mask=Frame1)
    return img1

def info_image(im):
    # Compute the center (cx, cy) and radius of the eye
    cy = im.shape[0]//2
    midline = im[cy,:]
    midline = np.where(midline>midline.mean()/3)[0]
    if len(midline)>im.shape[1]//2:
        x_start, x_end = np.min(midline), np.max(midline)
    else: # This actually rarely happens p~1/10000
        x_start, x_end = im.shape[1]//10, 9*im.shape[1]//10
    cx = (x_start + x_end)/2
    r = (x_end - x_start)/2
    return cx, cy, r

def resize_image(im, image_size, augmentation=False):
    # Crops, resizes and potentially augments the image to IMAGE_SIZE
    cx, cy, r = info_image(im)
    scaling = image_size/(2*r)
    rotation = 0
    if augmentation:
        scaling *= 1 + 0.3 * (np.random.rand()-0.5)
        rotation = 360 * np.random.rand()
    M = cv2.getRotationMatrix2D((cx,cy), rotation, scaling)
    M[0,2] -= cx - image_size/2
    M[1,2] -= cy - image_size/2
    return cv2.warpAffine(im,M,(image_size, image_size))

class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)
        lab_planes = cv2.split(lab)
        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(16, 16))
        lab_planes[0] = clahe.apply(lab_planes[0])
        lab = cv2.merge(lab_planes)
        clahed = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)
        image = resize_image(clahed, cfg.img_size)

        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-15, 15)),
        # transforms.RandomResizedCrop(cfg.img_size_crop),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedKFold

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'           #### NEED TO CHANGED IT!!!!!!!!!!!!!!!!!!!
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_BEN_preprocessing/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))
    print(""Training path: {}"".format(train_path))
    # Data Processing


    ## SHUFFLE DATA
    skf = None
    # if cfg.valid_type == 'holdout':
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True,
                                                                   random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid) # need to change It!!!!!
    # elif cfg.valid_type == 'cv':
    #     skf = StratifiedKFold(n_splits=cfg.n_folds, shuffle=True, random_state=cfg.seed)



    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model
    cfg.model.load_state_dict(
        torch.load('./Model_weights_finetuning/finetune3_pretrained_on_old_img480_with_cropto240_b1.pth')['model'])

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()

    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [12]
    lr_list = [1e-3]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b1']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb96007a20>
2019.08.25 11:03:40,new,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        self.model = None
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'ResNet152':
            model = models.resnet152(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet152.pth""))
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                # nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b6':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b6-c76e70fd.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b7':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b7-dcc49843.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model


",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 1e-05
)",10.0,6514465.0,41.0,12.0,0.027366,0.183756,0.8937,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'mixed_data_imgsize_300'
        self.img_size = 480
        # self.img_size_crop = 300

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        # self.num_classes = 5
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 10
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP

        self.num_workers = 16
        self.n_epochs = 150
        self.batch_size = batch_size
        self.valid_type = 'holdout' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50",2618.77,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config
import math

# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p



def subtract_median_bg_image(im):
    k = np.max(im.shape)//20*2+1
    bg = cv2.medianBlur(im, k)
    return cv2.addWeighted (im, 4, bg, -4, 128)

PARAM = 96
def Radius_Reduction(img,PARAM):
    h,w,c=img.shape
    Frame=np.zeros((h,w,c),dtype=np.uint8)
    cv2.circle(Frame,(int(math.floor(w/2)),int(math.floor(h/2))),int(math.floor((h*PARAM)/float(2*100))), (255,255,255), -1)
    Frame1=cv2.cvtColor(Frame, cv2.COLOR_BGR2GRAY)
    img1 =cv2.bitwise_and(img,img,mask=Frame1)
    return img1

def info_image(im):
    # Compute the center (cx, cy) and radius of the eye
    cy = im.shape[0]//2
    midline = im[cy,:]
    midline = np.where(midline>midline.mean()/3)[0]
    if len(midline)>im.shape[1]//2:
        x_start, x_end = np.min(midline), np.max(midline)
    else: # This actually rarely happens p~1/10000
        x_start, x_end = im.shape[1]//10, 9*im.shape[1]//10
    cx = (x_start + x_end)/2
    r = (x_end - x_start)/2
    return cx, cy, r

def resize_image(im, image_size, augmentation=False):
    # Crops, resizes and potentially augments the image to IMAGE_SIZE
    cx, cy, r = info_image(im)
    scaling = image_size/(2*r)
    rotation = 0
    if augmentation:
        scaling *= 1 + 0.3 * (np.random.rand()-0.5)
        rotation = 360 * np.random.rand()
    M = cv2.getRotationMatrix2D((cx,cy), rotation, scaling)
    M[0,2] -= cx - image_size/2
    M[1,2] -= cy - image_size/2
    return cv2.warpAffine(im,M,(image_size, image_size))

class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)
        lab_planes = cv2.split(lab)
        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(16, 16))
        lab_planes[0] = clahe.apply(lab_planes[0])
        lab = cv2.merge(lab_planes)
        clahed = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)
        image = resize_image(clahed, cfg.img_size)

        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-15, 15)),
        # transforms.RandomResizedCrop(cfg.img_size_crop),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedKFold

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'           #### NEED TO CHANGED IT!!!!!!!!!!!!!!!!!!!
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_BEN_preprocessing/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))
    print(""Training path: {}"".format(train_path))
    # Data Processing


    ## SHUFFLE DATA
    skf = None
    # if cfg.valid_type == 'holdout':
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True,
                                                                   random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid) # need to change It!!!!!
    # elif cfg.valid_type == 'cv':
    #     skf = StratifiedKFold(n_splits=cfg.n_folds, shuffle=True, random_state=cfg.seed)



    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model
    # cfg.model.load_state_dict(
    #     torch.load('./Model_weights_finetuning/finetune3_pretrained_on_old_img480_with_cropto240_b1.pth')['model'])

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()

    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [12]
    lr_list = [1e-3]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b1']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb96005a58>
2019.08.25 11:48:36,new,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        self.model = None
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'ResNet152':
            model = models.resnet152(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet152.pth""))
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b6':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b6-c76e70fd.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b7':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b7-dcc49843.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model


",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 1e-05
)",10.0,6514465.0,30.0,12.0,0.044465,0.207913,0.8886,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'mixed_data_imgsize_300'
        self.img_size = 480
        # self.img_size_crop = 300

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        # self.num_classes = 5
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 10
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP

        self.num_workers = 16
        self.n_epochs = 150
        self.batch_size = batch_size
        self.valid_type = 'holdout' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50",1930.85,,"from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config
import math

# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p



def subtract_median_bg_image(im):
    k = np.max(im.shape)//20*2+1
    bg = cv2.medianBlur(im, k)
    return cv2.addWeighted (im, 4, bg, -4, 128)

PARAM = 96
def Radius_Reduction(img,PARAM):
    h,w,c=img.shape
    Frame=np.zeros((h,w,c),dtype=np.uint8)
    cv2.circle(Frame,(int(math.floor(w/2)),int(math.floor(h/2))),int(math.floor((h*PARAM)/float(2*100))), (255,255,255), -1)
    Frame1=cv2.cvtColor(Frame, cv2.COLOR_BGR2GRAY)
    img1 =cv2.bitwise_and(img,img,mask=Frame1)
    return img1

def info_image(im):
    # Compute the center (cx, cy) and radius of the eye
    cy = im.shape[0]//2
    midline = im[cy,:]
    midline = np.where(midline>midline.mean()/3)[0]
    if len(midline)>im.shape[1]//2:
        x_start, x_end = np.min(midline), np.max(midline)
    else: # This actually rarely happens p~1/10000
        x_start, x_end = im.shape[1]//10, 9*im.shape[1]//10
    cx = (x_start + x_end)/2
    r = (x_end - x_start)/2
    return cx, cy, r

def resize_image(im, image_size, augmentation=False):
    # Crops, resizes and potentially augments the image to IMAGE_SIZE
    cx, cy, r = info_image(im)
    scaling = image_size/(2*r)
    rotation = 0
    if augmentation:
        scaling *= 1 + 0.3 * (np.random.rand()-0.5)
        rotation = 360 * np.random.rand()
    M = cv2.getRotationMatrix2D((cx,cy), rotation, scaling)
    M[0,2] -= cx - image_size/2
    M[1,2] -= cy - image_size/2
    return cv2.warpAffine(im,M,(image_size, image_size))

class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)
        lab_planes = cv2.split(lab)
        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(16, 16))
        lab_planes[0] = clahe.apply(lab_planes[0])
        lab = cv2.merge(lab_planes)
        clahed = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)
        image = resize_image(clahed, cfg.img_size)

        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-15, 15)),
        # transforms.RandomResizedCrop(cfg.img_size_crop),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedKFold

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'           #### NEED TO CHANGED IT!!!!!!!!!!!!!!!!!!!
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_BEN_preprocessing/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))
    print(""Training path: {}"".format(train_path))
    # Data Processing


    ## SHUFFLE DATA
    skf = None
    # if cfg.valid_type == 'holdout':
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True,
                                                                   random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid) # need to change It!!!!!
    # elif cfg.valid_type == 'cv':
    #     skf = StratifiedKFold(n_splits=cfg.n_folds, shuffle=True, random_state=cfg.seed)



    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model
    # cfg.model.load_state_dict(
    #     torch.load('./Model_weights_finetuning/finetune3_pretrained_on_old_img480_with_cropto240_b1.pth')['model'])

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()

    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [12]
    lr_list = [1e-3]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b1']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",,<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb9d939898>
