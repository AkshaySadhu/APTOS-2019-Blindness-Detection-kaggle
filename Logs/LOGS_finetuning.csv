date,data-type,net-architecture,loss-func,optim,scheduler,early-stopping-patience,parameters-amount,n-epochs,batch-size,best-train-loss,best-valid-loss,best-kappa,lb-kappa-score,cfg,dataset,trainloop,time_estimated
2019.08.11 01:58:43,new_old_mixed_old,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
            )
            model.cuda()
            self.model = model



",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.006
    lr: 0.006
    weight_decay: 0
)",<torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7efb95f97b70>,5.0,28342833.0,11.0,16.0,0.651234,0.631161,0.2751,,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            self.df_logger = Logger(self.logsFileName + '.csv', 'df')
            self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
            self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'new_comp_quadratic_kappa'

        self.img_size = 256

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(self.optimizer, 5)
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 5
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = ""pytorch"" #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 25
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.3
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new_old_mixed'

        ## PRINT FREQUENCY
        self.print_frequency = 50","from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image = crop_image_from_gray(image)
        image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))

    # Data Processing


    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed)
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.mean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            cfg.scheduler.step()

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()


if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [0.006]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b5']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",13907.77
2019.08.11 10:49:33,new_old_mixed_new,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
            )
            model.cuda()
            self.model = model



",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.006
    lr: 0.006
    weight_decay: 0
)",<torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7efb95f96ba8>,8.0,28342833.0,25.0,16.0,0.548863,0.418337,0.768,,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            self.df_logger = Logger(self.logsFileName + '.csv', 'df')
            self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
            self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'new_comp_quadratic_kappa'

        self.img_size = 256

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(self.optimizer, 3)
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 8
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = ""pytorch"" #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 25
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.3
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new_old_mixed'

        ## PRINT FREQUENCY
        self.print_frequency = 50","from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image = crop_image_from_gray(image)
        image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))

    # Data Processing


    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed)
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.mean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            cfg.scheduler.step()

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()


if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [0.003]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b5']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",
2019.08.11 11:17:21,new_old_mixed_new,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
            )
            model.cuda()
            self.model = model



",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.006
    lr: 0.006
    weight_decay: 0
)",<torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7efb9d1d9a20>,8.0,28342833.0,24.0,16.0,0.324063,0.322948,0.7776,,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            self.df_logger = Logger(self.logsFileName + '.csv', 'df')
            self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
            self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'new_comp_quadratic_kappa'

        self.img_size = 256

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(self.optimizer, 3)
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 8
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = ""pytorch"" #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 25
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.3
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new_old_mixed'

        ## PRINT FREQUENCY
        self.print_frequency = 50","from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image = crop_image_from_gray(image)
        image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))

    # Data Processing


    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed)
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.mean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            cfg.scheduler.step()

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()


if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [0.003]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b5']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",3937.36
2019.08.11 12:36:00,new_old_mixed_new,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Linear(in_features=2048, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1024, bias=True),
                nn.Dropout(0.4),
                nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
            )
            model.cuda()
            self.model = model



",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.006
    lr: 0.006
    weight_decay: 0
)",<torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7efb95f97a20>,10.0,28342833.0,26.0,16.0,0.29184499999999997,0.305394,0.8427,,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            self.df_logger = Logger(self.logsFileName + '.csv', 'df')
            self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
            self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'new_comp_quadratic_kappa'

        self.img_size = 256

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(self.optimizer, 3)
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 10
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = ""pytorch"" #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 45
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.3
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new_old_mixed'

        ## PRINT FREQUENCY
        self.print_frequency = 50","from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config


# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p


class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image = crop_image_from_gray(image)
        image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))

    # Data Processing


    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed)
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid)

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.mean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            cfg.scheduler.step()

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()


if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [0.003]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b5']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",4296.42
2019.08.24 09:07:54,old_old,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        self.model = None
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'ResNet152':
            model = models.resnet152(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet152.pth""))
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b6':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b6-c76e70fd.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b7':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b7-dcc49843.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model


",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 1e-05
)",<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb95fc6080>,10.0,10697769.0,150.0,16.0,0.448021,0.434273,0.809,,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'mixed_data_imgsize_300'
        self.img_size = 400
        self.img_size_crop = 300

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 10
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 150
        self.batch_size = batch_size
        self.valid_type = 'HoldOut' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'old'

        ## PRINT FREQUENCY
        self.print_frequency = 50","from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config
import math

# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p



def subtract_median_bg_image(im):
    k = np.max(im.shape)//20*2+1
    bg = cv2.medianBlur(im, k)
    return cv2.addWeighted (im, 4, bg, -4, 128)

PARAM = 96
def Radius_Reduction(img,PARAM):
    h,w,c=img.shape
    Frame=np.zeros((h,w,c),dtype=np.uint8)
    cv2.circle(Frame,(int(math.floor(w/2)),int(math.floor(h/2))),int(math.floor((h*PARAM)/float(2*100))), (255,255,255), -1)
    Frame1=cv2.cvtColor(Frame, cv2.COLOR_BGR2GRAY)
    img1 =cv2.bitwise_and(img,img,mask=Frame1)
    return img1

class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        # image = crop_image_from_gray(image)
        # image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        # image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        #transforms.RandomResizedCrop(cfg.img_size_crop),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'           #### NEED TO CHANGED IT!!!!!!!!!!!!!!!!!!!
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_BEN_preprocessing/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))
    print(""Training path: {}"".format(train_path))
    # Data Processing


    ## SHUFFLE DATA
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_train) # need to change It!!!!!

    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [1e-3]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b2']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",
2019.08.24 10:08:45,old_old,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        self.model = None
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'ResNet152':
            model = models.resnet152(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet152.pth""))
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b6':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b6-c76e70fd.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b7':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b7-dcc49843.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model


",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 1e-05
)",<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb9d092780>,10.0,10697769.0,150.0,16.0,0.5919300000000001,0.42468,0.7793,,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'mixed_data_imgsize_300'
        self.img_size = 400
        self.img_size_crop = 300

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 10
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 150
        self.batch_size = batch_size
        self.valid_type = 'holdout' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'old'

        ## PRINT FREQUENCY
        self.print_frequency = 50","from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config
import math

# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p



def subtract_median_bg_image(im):
    k = np.max(im.shape)//20*2+1
    bg = cv2.medianBlur(im, k)
    return cv2.addWeighted (im, 4, bg, -4, 128)

PARAM = 96
def Radius_Reduction(img,PARAM):
    h,w,c=img.shape
    Frame=np.zeros((h,w,c),dtype=np.uint8)
    cv2.circle(Frame,(int(math.floor(w/2)),int(math.floor(h/2))),int(math.floor((h*PARAM)/float(2*100))), (255,255,255), -1)
    Frame1=cv2.cvtColor(Frame, cv2.COLOR_BGR2GRAY)
    img1 =cv2.bitwise_and(img,img,mask=Frame1)
    return img1

class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        # image = crop_image_from_gray(image)
        # image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        # image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        transforms.RandomResizedCrop(cfg.img_size_crop),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedKFold

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'           #### NEED TO CHANGED IT!!!!!!!!!!!!!!!!!!!
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_BEN_preprocessing/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))
    print(""Training path: {}"".format(train_path))
    # Data Processing


    ## SHUFFLE DATA
    if cfg.valid_type == 'holdout':
        train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
        train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
        valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_train) # need to change It!!!!!
    elif cfg.valid_type == 'cv':



    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [1e-3]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b2']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",
2019.08.24 11:35:33,old_old,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        self.model = None
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'ResNet152':
            model = models.resnet152(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet152.pth""))
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b6':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b6-c76e70fd.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b7':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b7-dcc49843.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model


",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 1e-05
)",<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb9d091668>,10.0,10697769.0,150.0,16.0,0.448021,0.434273,0.809,,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'mixed_data_imgsize_300'
        self.img_size = 300
        self.img_size_crop = 300

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 10
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 150
        self.batch_size = batch_size
        self.valid_type = 'holdout' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'old'

        ## PRINT FREQUENCY
        self.print_frequency = 50","from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config
import math

# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p



def subtract_median_bg_image(im):
    k = np.max(im.shape)//20*2+1
    bg = cv2.medianBlur(im, k)
    return cv2.addWeighted (im, 4, bg, -4, 128)

PARAM = 96
def Radius_Reduction(img,PARAM):
    h,w,c=img.shape
    Frame=np.zeros((h,w,c),dtype=np.uint8)
    cv2.circle(Frame,(int(math.floor(w/2)),int(math.floor(h/2))),int(math.floor((h*PARAM)/float(2*100))), (255,255,255), -1)
    Frame1=cv2.cvtColor(Frame, cv2.COLOR_BGR2GRAY)
    img1 =cv2.bitwise_and(img,img,mask=Frame1)
    return img1

class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        # image = crop_image_from_gray(image)
        # image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        # image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        # transforms.RandomResizedCrop(cfg.img_size_crop),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedKFold

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'           #### NEED TO CHANGED IT!!!!!!!!!!!!!!!!!!!
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_BEN_preprocessing/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))
    print(""Training path: {}"".format(train_path))
    # Data Processing


    ## SHUFFLE DATA
    if cfg.valid_type == 'holdout':
        train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
        train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
        valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_train) # need to change It!!!!!
    elif cfg.valid_type == 'cv':



    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [1e-3]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b2']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",
2019.08.24 12:30:54,old_old,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        self.model = None
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'ResNet152':
            model = models.resnet152(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet152.pth""))
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b6':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b6-c76e70fd.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b7':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b7-dcc49843.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model


",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 1e-05
)",<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb9d10d208>,10.0,7702403.0,18.0,16.0,0.5119319999999999,0.44016099999999997,0.8098,,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'mixed_data_imgsize_300'
        self.img_size = 256
        self.img_size_crop = 300

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 10
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 150
        self.batch_size = batch_size
        self.valid_type = 'holdout' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'old'

        ## PRINT FREQUENCY
        self.print_frequency = 50","from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config
import math

# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p



def subtract_median_bg_image(im):
    k = np.max(im.shape)//20*2+1
    bg = cv2.medianBlur(im, k)
    return cv2.addWeighted (im, 4, bg, -4, 128)

PARAM = 96
def Radius_Reduction(img,PARAM):
    h,w,c=img.shape
    Frame=np.zeros((h,w,c),dtype=np.uint8)
    cv2.circle(Frame,(int(math.floor(w/2)),int(math.floor(h/2))),int(math.floor((h*PARAM)/float(2*100))), (255,255,255), -1)
    Frame1=cv2.cvtColor(Frame, cv2.COLOR_BGR2GRAY)
    img1 =cv2.bitwise_and(img,img,mask=Frame1)
    return img1

class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        # image = crop_image_from_gray(image)
        # image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        # image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        # transforms.RandomResizedCrop(cfg.img_size_crop),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedKFold

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'           #### NEED TO CHANGED IT!!!!!!!!!!!!!!!!!!!
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_BEN_preprocessing/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))
    print(""Training path: {}"".format(train_path))
    # Data Processing


    ## SHUFFLE DATA
    if cfg.valid_type == 'holdout':
        train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
        train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
        valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_train) # need to change It!!!!!
    elif cfg.valid_type == 'cv':
        skf = StratifiedKFold(n_splits=cfg.n_folds, shuffle=True, random_state=13)


    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()
    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [1e-3]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b2']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",4709.69
2019.08.24 16:27:32,new_old,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        self.model = None
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'ResNet152':
            model = models.resnet152(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet152.pth""))
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b6':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b6-c76e70fd.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b7':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b7-dcc49843.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model


",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 1e-05
)",<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb9d10e5c0>,10.0,28342833.0,19.0,16.0,0.440264,0.35058,0.6965,,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'mixed_data_imgsize_300'
        self.img_size = 256
        self.img_size_crop = 300

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 10
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP
        self.num_workers = 16
        self.n_epochs = 150
        self.batch_size = batch_size
        self.valid_type = 'holdout' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50","from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config
import math

# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p



def subtract_median_bg_image(im):
    k = np.max(im.shape)//20*2+1
    bg = cv2.medianBlur(im, k)
    return cv2.addWeighted (im, 4, bg, -4, 128)

PARAM = 96
def Radius_Reduction(img,PARAM):
    h,w,c=img.shape
    Frame=np.zeros((h,w,c),dtype=np.uint8)
    cv2.circle(Frame,(int(math.floor(w/2)),int(math.floor(h/2))),int(math.floor((h*PARAM)/float(2*100))), (255,255,255), -1)
    Frame1=cv2.cvtColor(Frame, cv2.COLOR_BGR2GRAY)
    img1 =cv2.bitwise_and(img,img,mask=Frame1)
    return img1

class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        # image = crop_image_from_gray(image)
        # image = cv2.resize(image, (cfg.img_size, cfg.img_size))
        # image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        # transforms.RandomResizedCrop(cfg.img_size_crop),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedKFold

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_mixed_BEN_preprocessing/'           #### NEED TO CHANGED IT!!!!!!!!!!!!!!!!!!!
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_BEN_preprocessing/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))
    print(""Training path: {}"".format(train_path))
    # Data Processing


    ## SHUFFLE DATA
    skf = None
    # if cfg.valid_type == 'holdout':
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True,
                                                                   random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid) # need to change It!!!!!
    # elif cfg.valid_type == 'cv':
    #     skf = StratifiedKFold(n_splits=cfg.n_folds, shuffle=True, random_state=cfg.seed)



    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model
    cfg.model.load_state_dict(
        torch.load('./Model_weights_finetuning/pretrained_on_old_img256_b2_without_crop.pth')['model'])

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()

    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [1e-3]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b2']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",10573.07
2019.08.25 01:33:07,new_old,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        self.model = None
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'ResNet152':
            model = models.resnet152(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet152.pth""))
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b6':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b6-c76e70fd.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b7':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b7-dcc49843.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model


",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 1e-05
)",<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb9600f860>,10.0,6514465.0,19.0,12.0,0.342063,0.386119,,,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'mixed_data_imgsize_300'
        self.img_size = 480
        # self.img_size_crop = 300

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        # self.num_classes = 5
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 10
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP

        self.num_workers = 16
        self.n_epochs = 150
        self.batch_size = batch_size
        self.valid_type = 'holdout' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50","from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config
import math

# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p



def subtract_median_bg_image(im):
    k = np.max(im.shape)//20*2+1
    bg = cv2.medianBlur(im, k)
    return cv2.addWeighted (im, 4, bg, -4, 128)

PARAM = 96
def Radius_Reduction(img,PARAM):
    h,w,c=img.shape
    Frame=np.zeros((h,w,c),dtype=np.uint8)
    cv2.circle(Frame,(int(math.floor(w/2)),int(math.floor(h/2))),int(math.floor((h*PARAM)/float(2*100))), (255,255,255), -1)
    Frame1=cv2.cvtColor(Frame, cv2.COLOR_BGR2GRAY)
    img1 =cv2.bitwise_and(img,img,mask=Frame1)
    return img1

def info_image(im):
    # Compute the center (cx, cy) and radius of the eye
    cy = im.shape[0]//2
    midline = im[cy,:]
    midline = np.where(midline>midline.mean()/3)[0]
    if len(midline)>im.shape[1]//2:
        x_start, x_end = np.min(midline), np.max(midline)
    else: # This actually rarely happens p~1/10000
        x_start, x_end = im.shape[1]//10, 9*im.shape[1]//10
    cx = (x_start + x_end)/2
    r = (x_end - x_start)/2
    return cx, cy, r

def resize_image(im, image_size, augmentation=False):
    # Crops, resizes and potentially augments the image to IMAGE_SIZE
    cx, cy, r = info_image(im)
    scaling = image_size/(2*r)
    rotation = 0
    if augmentation:
        scaling *= 1 + 0.3 * (np.random.rand()-0.5)
        rotation = 360 * np.random.rand()
    M = cv2.getRotationMatrix2D((cx,cy), rotation, scaling)
    M[0,2] -= cx - image_size/2
    M[1,2] -= cy - image_size/2
    return cv2.warpAffine(im,M,(image_size, image_size))

class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        image = resize_image(image, cfg.img_size)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-15, 15)),
        # transforms.RandomResizedCrop(cfg.img_size_crop),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedKFold

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_mixed_BEN_preprocessing/'           #### NEED TO CHANGED IT!!!!!!!!!!!!!!!!!!!
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_BEN_preprocessing/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))
    print(""Training path: {}"".format(train_path))
    # Data Processing


    ## SHUFFLE DATA
    skf = None
    # if cfg.valid_type == 'holdout':
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True,
                                                                   random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid) # need to change It!!!!!
    # elif cfg.valid_type == 'cv':
    #     skf = StratifiedKFold(n_splits=cfg.n_folds, shuffle=True, random_state=cfg.seed)



    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model
    # cfg.model.load_state_dict(
        # torch.load('./Model_weights_finetuning/pretrained_on_old_img256_b5.pth')['model'])

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()

    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().long()
            target = target.view(-1)#, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().long()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1)#, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [1e-3]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b5']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",13469.38
2019.08.25 12:38:38,new_old,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        self.model = None
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'ResNet152':
            model = models.resnet152(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet152.pth""))
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b6':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b6-c76e70fd.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b7':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b7-dcc49843.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model


",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 1e-05
)",<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb9d10f668>,10.0,6514465.0,28.0,12.0,0.283221,0.340251,,,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'mixed_data_imgsize_300'
        self.img_size = 480
        # self.img_size_crop = 300

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        # self.num_classes = 5
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 10
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP

        self.num_workers = 16
        self.n_epochs = 150
        self.batch_size = batch_size
        self.valid_type = 'holdout' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50","from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config
import math

# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p



def subtract_median_bg_image(im):
    k = np.max(im.shape)//20*2+1
    bg = cv2.medianBlur(im, k)
    return cv2.addWeighted (im, 4, bg, -4, 128)

PARAM = 96
def Radius_Reduction(img,PARAM):
    h,w,c=img.shape
    Frame=np.zeros((h,w,c),dtype=np.uint8)
    cv2.circle(Frame,(int(math.floor(w/2)),int(math.floor(h/2))),int(math.floor((h*PARAM)/float(2*100))), (255,255,255), -1)
    Frame1=cv2.cvtColor(Frame, cv2.COLOR_BGR2GRAY)
    img1 =cv2.bitwise_and(img,img,mask=Frame1)
    return img1

def info_image(im):
    # Compute the center (cx, cy) and radius of the eye
    cy = im.shape[0]//2
    midline = im[cy,:]
    midline = np.where(midline>midline.mean()/3)[0]
    if len(midline)>im.shape[1]//2:
        x_start, x_end = np.min(midline), np.max(midline)
    else: # This actually rarely happens p~1/10000
        x_start, x_end = im.shape[1]//10, 9*im.shape[1]//10
    cx = (x_start + x_end)/2
    r = (x_end - x_start)/2
    return cx, cy, r

def resize_image(im, image_size, augmentation=False):
    # Crops, resizes and potentially augments the image to IMAGE_SIZE
    cx, cy, r = info_image(im)
    scaling = image_size/(2*r)
    rotation = 0
    if augmentation:
        scaling *= 1 + 0.3 * (np.random.rand()-0.5)
        rotation = 360 * np.random.rand()
    M = cv2.getRotationMatrix2D((cx,cy), rotation, scaling)
    M[0,2] -= cx - image_size/2
    M[1,2] -= cy - image_size/2
    return cv2.warpAffine(im,M,(image_size, image_size))

class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)
        lab_planes = cv2.split(lab)
        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(16, 16))
        lab_planes[0] = clahe.apply(lab_planes[0])
        lab = cv2.merge(lab_planes)
        clahed = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)
        image = resize_image(clahed, cfg.img_size)

        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-15, 15)),
        # transforms.RandomResizedCrop(cfg.img_size_crop),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedKFold

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'           #### NEED TO CHANGED IT!!!!!!!!!!!!!!!!!!!
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_BEN_preprocessing/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))
    print(""Training path: {}"".format(train_path))
    # Data Processing


    ## SHUFFLE DATA
    skf = None
    # if cfg.valid_type == 'holdout':
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True,
                                                                   random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid) # need to change It!!!!!
    # elif cfg.valid_type == 'cv':
    #     skf = StratifiedKFold(n_splits=cfg.n_folds, shuffle=True, random_state=cfg.seed)



    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model
    # cfg.model.load_state_dict(
    #     torch.load('./Model_weights_finetuning/finetune3_pretrained_on_old_img480_with_cropto240_b1.pth')['model'])

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()

    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [12]
    lr_list = [1e-3]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b1']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",20863.64
2019.08.25 19:07:40,new_old,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        self.model = None
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'ResNet152':
            model = models.resnet152(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet152.pth""))
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b6':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b6-c76e70fd.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b7':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b7-dcc49843.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model


",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 1e-05
)",<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb9d10b748>,10.0,28342833.0,150.0,4.0,0.545569,1.009435,0.5566,,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'data_imgsize_480'
        self.img_size = 480
        # self.img_size_crop = 300

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        # self.num_classes = 5
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 10
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP

        self.num_workers = 16
        self.n_epochs = 150
        self.batch_size = batch_size
        self.valid_type = 'holdout' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50","from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config
import math

# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p



def subtract_median_bg_image(im):
    k = np.max(im.shape)//20*2+1
    bg = cv2.medianBlur(im, k)
    return cv2.addWeighted (im, 4, bg, -4, 128)

PARAM = 96
def Radius_Reduction(img,PARAM):
    h,w,c=img.shape
    Frame=np.zeros((h,w,c),dtype=np.uint8)
    cv2.circle(Frame,(int(math.floor(w/2)),int(math.floor(h/2))),int(math.floor((h*PARAM)/float(2*100))), (255,255,255), -1)
    Frame1=cv2.cvtColor(Frame, cv2.COLOR_BGR2GRAY)
    img1 =cv2.bitwise_and(img,img,mask=Frame1)
    return img1

def info_image(im):
    # Compute the center (cx, cy) and radius of the eye
    cy = im.shape[0]//2
    midline = im[cy,:]
    midline = np.where(midline>midline.mean()/3)[0]
    if len(midline)>im.shape[1]//2:
        x_start, x_end = np.min(midline), np.max(midline)
    else: # This actually rarely happens p~1/10000
        x_start, x_end = im.shape[1]//10, 9*im.shape[1]//10
    cx = (x_start + x_end)/2
    r = (x_end - x_start)/2
    return cx, cy, r

def resize_image(im, image_size, augmentation=False):
    # Crops, resizes and potentially augments the image to IMAGE_SIZE
    cx, cy, r = info_image(im)
    scaling = image_size/(2*r)
    rotation = 0
    if augmentation:
        scaling *= 1 + 0.3 * (np.random.rand()-0.5)
        rotation = 360 * np.random.rand()
    M = cv2.getRotationMatrix2D((cx,cy), rotation, scaling)
    M[0,2] -= cx - image_size/2
    M[1,2] -= cy - image_size/2
    return cv2.warpAffine(im,M,(image_size, image_size))

class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)
        lab_planes = cv2.split(lab)
        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(16, 16))
        lab_planes[0] = clahe.apply(lab_planes[0])
        lab = cv2.merge(lab_planes)
        clahed = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)
        image = resize_image(clahed, cfg.img_size)

        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-15, 15)),
        # transforms.RandomResizedCrop(cfg.img_size_crop),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedKFold

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'           #### NEED TO CHANGED IT!!!!!!!!!!!!!!!!!!!
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_BEN_preprocessing/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))
    print(""Training path: {}"".format(train_path))
    # Data Processing


    ## SHUFFLE DATA
    skf = None
    # if cfg.valid_type == 'holdout':
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True,
                                                                   random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid) # need to change It!!!!!
    # elif cfg.valid_type == 'cv':
    #     skf = StratifiedKFold(n_splits=cfg.n_folds, shuffle=True, random_state=cfg.seed)



    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model
    cfg.model.load_state_dict(
        torch.load('./Model_weights_finetuning/finetune4_img480_withcropto240_clahe_b1.pth')['model'])

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()

    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [12]
    lr_list = [1e-3]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b1']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",
2019.08.25 22:48:05,new_old,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        self.model = None
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'ResNet152':
            model = models.resnet152(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet152.pth""))
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b6':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b6-c76e70fd.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b7':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b7-dcc49843.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model


",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 1e-05
)",<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb9d10e6a0>,10.0,42502209.0,150.0,12.0,0.973581,1.869637,0.0,,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'data_imgsize_480'
        self.img_size = 480
        # self.img_size_crop = 300

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        # self.num_classes = 5
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 10
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP

        self.num_workers = 16
        self.n_epochs = 150
        self.batch_size = batch_size
        self.valid_type = 'holdout' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50","from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config
import math

# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p



def subtract_median_bg_image(im):
    k = np.max(im.shape)//20*2+1
    bg = cv2.medianBlur(im, k)
    return cv2.addWeighted (im, 4, bg, -4, 128)

PARAM = 96
def Radius_Reduction(img,PARAM):
    h,w,c=img.shape
    Frame=np.zeros((h,w,c),dtype=np.uint8)
    cv2.circle(Frame,(int(math.floor(w/2)),int(math.floor(h/2))),int(math.floor((h*PARAM)/float(2*100))), (255,255,255), -1)
    Frame1=cv2.cvtColor(Frame, cv2.COLOR_BGR2GRAY)
    img1 =cv2.bitwise_and(img,img,mask=Frame1)
    return img1

def info_image(im):
    # Compute the center (cx, cy) and radius of the eye
    cy = im.shape[0]//2
    midline = im[cy,:]
    midline = np.where(midline>midline.mean()/3)[0]
    if len(midline)>im.shape[1]//2:
        x_start, x_end = np.min(midline), np.max(midline)
    else: # This actually rarely happens p~1/10000
        x_start, x_end = im.shape[1]//10, 9*im.shape[1]//10
    cx = (x_start + x_end)/2
    r = (x_end - x_start)/2
    return cx, cy, r

def resize_image(im, image_size, augmentation=False):
    # Crops, resizes and potentially augments the image to IMAGE_SIZE
    cx, cy, r = info_image(im)
    scaling = image_size/(2*r)
    rotation = 0
    if augmentation:
        scaling *= 1 + 0.3 * (np.random.rand()-0.5)
        rotation = 360 * np.random.rand()
    M = cv2.getRotationMatrix2D((cx,cy), rotation, scaling)
    M[0,2] -= cx - image_size/2
    M[1,2] -= cy - image_size/2
    return cv2.warpAffine(im,M,(image_size, image_size))

class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)
        lab_planes = cv2.split(lab)
        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(16, 16))
        lab_planes[0] = clahe.apply(lab_planes[0])
        lab = cv2.merge(lab_planes)
        clahed = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)
        image = resize_image(clahed, cfg.img_size)

        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-15, 15)),
        # transforms.RandomResizedCrop(cfg.img_size_crop),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedKFold

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'           #### NEED TO CHANGED IT!!!!!!!!!!!!!!!!!!!
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_BEN_preprocessing/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))
    print(""Training path: {}"".format(train_path))
    # Data Processing


    ## SHUFFLE DATA
    skf = None
    # if cfg.valid_type == 'holdout':
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True,
                                                                   random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid) # need to change It!!!!!
    # elif cfg.valid_type == 'cv':
    #     skf = StratifiedKFold(n_splits=cfg.n_folds, shuffle=True, random_state=cfg.seed)



    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model
    cfg.model.load_state_dict(
        torch.load('./Model_weights_finetuning/finetune4_img480_withcropto240_clahe_b1.pth')['model'])

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()

    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [12]
    lr_list = [1e-3]
    p_horizontalflip_list = [0.4]
    model_type_list = ['ResNet101']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",
2019.08.26 01:51:44,new_old_balanced_old,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        self.model = None
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'ResNet152':
            model = models.resnet152(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet152.pth""))
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b6':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b6-c76e70fd.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b7':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b7-dcc49843.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model


",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 1e-05
)",<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb9d10e6d8>,10.0,7702403.0,28.0,16.0,0.38382,0.38658899999999996,0.7804,,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'data_imgsize_480'
        self.img_size = 260
        # self.img_size_crop = 300

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        # self.num_classes = 5
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 10
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP

        self.num_workers = 16
        self.n_epochs = 150
        self.batch_size = batch_size
        self.valid_type = 'holdout' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new_old_balanced'

        ## PRINT FREQUENCY
        self.print_frequency = 50","from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config
import math

# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p



def subtract_median_bg_image(im):
    k = np.max(im.shape)//20*2+1
    bg = cv2.medianBlur(im, k)
    return cv2.addWeighted (im, 4, bg, -4, 128)

PARAM = 96
def Radius_Reduction(img,PARAM):
    h,w,c=img.shape
    Frame=np.zeros((h,w,c),dtype=np.uint8)
    cv2.circle(Frame,(int(math.floor(w/2)),int(math.floor(h/2))),int(math.floor((h*PARAM)/float(2*100))), (255,255,255), -1)
    Frame1=cv2.cvtColor(Frame, cv2.COLOR_BGR2GRAY)
    img1 =cv2.bitwise_and(img,img,mask=Frame1)
    return img1

def info_image(im):
    # Compute the center (cx, cy) and radius of the eye
    cy = im.shape[0]//2
    midline = im[cy,:]
    midline = np.where(midline>midline.mean()/3)[0]
    if len(midline)>im.shape[1]//2:
        x_start, x_end = np.min(midline), np.max(midline)
    else: # This actually rarely happens p~1/10000
        x_start, x_end = im.shape[1]//10, 9*im.shape[1]//10
    cx = (x_start + x_end)/2
    r = (x_end - x_start)/2
    return cx, cy, r

def resize_image(im, image_size, augmentation=False):
    # Crops, resizes and potentially augments the image to IMAGE_SIZE
    cx, cy, r = info_image(im)
    scaling = image_size/(2*r)
    rotation = 0
    if augmentation:
        scaling *= 1 + 0.3 * (np.random.rand()-0.5)
        rotation = 360 * np.random.rand()
    M = cv2.getRotationMatrix2D((cx,cy), rotation, scaling)
    M[0,2] -= cx - image_size/2
    M[1,2] -= cy - image_size/2
    return cv2.warpAffine(im,M,(image_size, image_size))

class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)
        lab_planes = cv2.split(lab)
        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(16, 16))
        lab_planes[0] = clahe.apply(lab_planes[0])
        lab = cv2.merge(lab_planes)
        clahed = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)
        image = resize_image(clahed, cfg.img_size)

        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-15, 15)),
        # transforms.RandomResizedCrop(cfg.img_size_crop),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedKFold

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'           #### NEED TO CHANGED IT!!!!!!!!!!!!!!!!!!!
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_images/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))
    print(""Training path: {}"".format(train_path))
    # Data Processing


    ## SHUFFLE DATA
    skf = None
    # if cfg.valid_type == 'holdout':
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True,
                                                                   random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid) # need to change It!!!!!
    # elif cfg.valid_type == 'cv':
    #     skf = StratifiedKFold(n_splits=cfg.n_folds, shuffle=True, random_state=cfg.seed)



    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model
    # cfg.model.load_state_dict(
    #     torch.load('./Model_weights_finetuning/finetune4_img480_withcropto240_clahe_b1.pth')['model'])

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()

    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [12]
    lr_list = [1e-3]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b1']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",9406.3
2019.08.26 14:36:00,new_old,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        self.model = None
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'ResNet152':
            model = models.resnet152(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet152.pth""))
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b6':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b6-c76e70fd.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b7':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b7-dcc49843.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model


",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 1e-05
)",<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb9d10a5c0>,10.0,28342833.0,150.0,16.0,0.495213,0.438913,0.741,,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'data_imgsize_480'
        self.img_size = 260
        # self.img_size_crop = 300

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        # self.num_classes = 5
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 10
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP

        self.num_workers = 16
        self.n_epochs = 150
        self.batch_size = batch_size
        self.valid_type = 'holdout' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50","from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config
import math

# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p



def subtract_median_bg_image(im):
    k = np.max(im.shape)//20*2+1
    bg = cv2.medianBlur(im, k)
    return cv2.addWeighted (im, 4, bg, -4, 128)

PARAM = 96
def Radius_Reduction(img,PARAM):
    h,w,c=img.shape
    Frame=np.zeros((h,w,c),dtype=np.uint8)
    cv2.circle(Frame,(int(math.floor(w/2)),int(math.floor(h/2))),int(math.floor((h*PARAM)/float(2*100))), (255,255,255), -1)
    Frame1=cv2.cvtColor(Frame, cv2.COLOR_BGR2GRAY)
    img1 =cv2.bitwise_and(img,img,mask=Frame1)
    return img1

def info_image(im):
    # Compute the center (cx, cy) and radius of the eye
    cy = im.shape[0]//2
    midline = im[cy,:]
    midline = np.where(midline>midline.mean()/3)[0]
    if len(midline)>im.shape[1]//2:
        x_start, x_end = np.min(midline), np.max(midline)
    else: # This actually rarely happens p~1/10000
        x_start, x_end = im.shape[1]//10, 9*im.shape[1]//10
    cx = (x_start + x_end)/2
    r = (x_end - x_start)/2
    return cx, cy, r

def resize_image(im, image_size, augmentation=False):
    # Crops, resizes and potentially augments the image to IMAGE_SIZE
    cx, cy, r = info_image(im)
    scaling = image_size/(2*r)
    rotation = 0
    if augmentation:
        scaling *= 1 + 0.3 * (np.random.rand()-0.5)
        rotation = 360 * np.random.rand()
    M = cv2.getRotationMatrix2D((cx,cy), rotation, scaling)
    M[0,2] -= cx - image_size/2
    M[1,2] -= cy - image_size/2
    return cv2.warpAffine(im,M,(image_size, image_size))

class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)
        lab_planes = cv2.split(lab)
        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(16, 16))
        lab_planes[0] = clahe.apply(lab_planes[0])
        lab = cv2.merge(lab_planes)
        clahed = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)
        image = resize_image(clahed, cfg.img_size)

        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-15, 15)),
        # transforms.RandomResizedCrop(cfg.img_size_crop),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedKFold

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'           #### NEED TO CHANGED IT!!!!!!!!!!!!!!!!!!!
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_images/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))
    print(""Training path: {}"".format(train_path))
    # Data Processing


    ## SHUFFLE DATA
    skf = None
    # if cfg.valid_type == 'holdout':
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True,
                                                                   random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid) # need to change It!!!!!
    # elif cfg.valid_type == 'cv':
    #     skf = StratifiedKFold(n_splits=cfg.n_folds, shuffle=True, random_state=cfg.seed)



    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model
    cfg.model.load_state_dict(
        torch.load('./Model_weights_finetuning/finetune5_with_cropto260_b2_clahe.pth')['model'])

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()

    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [1e-3]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b2']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",
2019.08.26 16:31:09,new_old,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        self.model = None
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'ResNet152':
            model = models.resnet152(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet152.pth""))
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b6':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b6-c76e70fd.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b7':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b7-dcc49843.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model


",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 1e-05
)",<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb9d10f6a0>,10.0,28342833.0,19.0,16.0,0.44026400000000004,0.35058,0.6965,,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'data_imgsize_260'
        self.img_size = 256
        # self.img_size_crop = 300

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        # self.num_classes = 5
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 10
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP

        self.num_workers = 16
        self.n_epochs = 150
        self.batch_size = batch_size
        self.valid_type = 'holdout' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50","from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config
import math

# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p



def subtract_median_bg_image(im):
    k = np.max(im.shape)//20*2+1
    bg = cv2.medianBlur(im, k)
    return cv2.addWeighted (im, 4, bg, -4, 128)

PARAM = 96
def Radius_Reduction(img,PARAM):
    h,w,c=img.shape
    Frame=np.zeros((h,w,c),dtype=np.uint8)
    cv2.circle(Frame,(int(math.floor(w/2)),int(math.floor(h/2))),int(math.floor((h*PARAM)/float(2*100))), (255,255,255), -1)
    Frame1=cv2.cvtColor(Frame, cv2.COLOR_BGR2GRAY)
    img1 =cv2.bitwise_and(img,img,mask=Frame1)
    return img1

def info_image(im):
    # Compute the center (cx, cy) and radius of the eye
    cy = im.shape[0]//2
    midline = im[cy,:]
    midline = np.where(midline>midline.mean()/3)[0]
    if len(midline)>im.shape[1]//2:
        x_start, x_end = np.min(midline), np.max(midline)
    else: # This actually rarely happens p~1/10000
        x_start, x_end = im.shape[1]//10, 9*im.shape[1]//10
    cx = (x_start + x_end)/2
    r = (x_end - x_start)/2
    return cx, cy, r

def resize_image(im, image_size, augmentation=False):
    # Crops, resizes and potentially augments the image to IMAGE_SIZE
    cx, cy, r = info_image(im)
    scaling = image_size/(2*r)
    rotation = 0
    if augmentation:
        scaling *= 1 + 0.3 * (np.random.rand()-0.5)
        rotation = 360 * np.random.rand()
    M = cv2.getRotationMatrix2D((cx,cy), rotation, scaling)
    M[0,2] -= cx - image_size/2
    M[1,2] -= cy - image_size/2
    return cv2.warpAffine(im,M,(image_size, image_size))

class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        # lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)
        # lab_planes = cv2.split(lab)
        # clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(16, 16))
        # lab_planes[0] = clahe.apply(lab_planes[0])
        # lab = cv2.merge(lab_planes)
        # clahed = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)
        # image = resize_image(clahed, cfg.img_size)

        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-120, 120)),
        # transforms.RandomResizedCrop(cfg.img_size_crop),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedKFold

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_mixed_BEN_preprocessing/'           #### NEED TO CHANGED IT!!!!!!!!!!!!!!!!!!!
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_images/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))
    print(""Training path: {}"".format(train_path))
    # Data Processing


    ## SHUFFLE DATA
    skf = None
    # if cfg.valid_type == 'holdout':
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True,
                                                                   random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid) # need to change It!!!!!
    # elif cfg.valid_type == 'cv':
    #     skf = StratifiedKFold(n_splits=cfg.n_folds, shuffle=True, random_state=cfg.seed)



    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model
    # cfg.model.load_state_dict(
    #     torch.load('./Model_weights_finetuning/finetune5_with_cropto260_b2_clahe.pth')['model'])

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()

    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [1e-3]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b5']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",10580.83
2019.08.27 01:02:59,new_old,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        self.model = None
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'ResNet152':
            model = models.resnet152(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet152.pth""))
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b6':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b6-c76e70fd.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b7':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b7-dcc49843.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model


",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 1e-05
)",<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb9d10c710>,10.0,28342833.0,18.0,16.0,0.464937,0.370867,0.7532,,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'data_imgsize_240'
        self.img_size = 256
        # self.img_size_crop = 300

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        # self.num_classes = 5
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 10
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP

        self.num_workers = 16
        self.n_epochs = 150
        self.batch_size = batch_size
        self.valid_type = 'holdout' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50","from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config
import math

# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p



def subtract_median_bg_image(im):
    k = np.max(im.shape)//20*2+1
    bg = cv2.medianBlur(im, k)
    return cv2.addWeighted (im, 4, bg, -4, 128)

PARAM = 96
def Radius_Reduction(img,PARAM):
    h,w,c=img.shape
    Frame=np.zeros((h,w,c),dtype=np.uint8)
    cv2.circle(Frame,(int(math.floor(w/2)),int(math.floor(h/2))),int(math.floor((h*PARAM)/float(2*100))), (255,255,255), -1)
    Frame1=cv2.cvtColor(Frame, cv2.COLOR_BGR2GRAY)
    img1 =cv2.bitwise_and(img,img,mask=Frame1)
    return img1

def info_image(im):
    # Compute the center (cx, cy) and radius of the eye
    cy = im.shape[0]//2
    midline = im[cy,:]
    midline = np.where(midline>midline.mean()/3)[0]
    if len(midline)>im.shape[1]//2:
        x_start, x_end = np.min(midline), np.max(midline)
    else: # This actually rarely happens p~1/10000
        x_start, x_end = im.shape[1]//10, 9*im.shape[1]//10
    cx = (x_start + x_end)/2
    r = (x_end - x_start)/2
    return cx, cy, r

def resize_image(im, image_size, augmentation=False):
    # Crops, resizes and potentially augments the image to IMAGE_SIZE
    cx, cy, r = info_image(im)
    scaling = image_size/(2*r)
    rotation = 0
    if augmentation:
        scaling *= 1 + 0.3 * (np.random.rand()-0.5)
        rotation = 360 * np.random.rand()
    M = cv2.getRotationMatrix2D((cx,cy), rotation, scaling)
    M[0,2] -= cx - image_size/2
    M[1,2] -= cy - image_size/2
    return cv2.warpAffine(im,M,(image_size, image_size))

class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)
        lab_planes = cv2.split(lab)
        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(16, 16))
        lab_planes[0] = clahe.apply(lab_planes[0])
        lab = cv2.merge(lab_planes)
        clahed = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)
        image = resize_image(clahed, cfg.img_size)

        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-150, 150)),
        transforms.ColorJitter(brightness=0, contrast=0, saturation=0, hue=0),
        # transforms.RandomResizedCrop(cfg.img_size_crop),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
        ])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedKFold

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'           #### NEED TO CHANGED IT!!!!!!!!!!!!!!!!!!!
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_images/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))
    print(""Training path: {}"".format(train_path))
    # Data Processing


    ## SHUFFLE DATA
    skf = None
    # if cfg.valid_type == 'holdout':
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True,
                                                                   random_state=cfg.seed, stratify=train_csv['diagnosis'])
    # train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid) # need to change It!!!!!
    # elif cfg.valid_type == 'cv':
    #     skf = StratifiedKFold(n_splits=cfg.n_folds, shuffle=True, random_state=cfg.seed)



    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model
    # cfg.model.load_state_dict(
    #     torch.load('./Model_weights_finetuning/finetune5_with_cropto260_b2_clahe.pth')['model'])

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()

    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [1e-3]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b5']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",10216.06
2019.08.27 09:29:00,new_old,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        self.model = None
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'ResNet152':
            model = models.resnet152(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet152.pth""))
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b6':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b6-c76e70fd.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b7':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b7-dcc49843.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model


",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 1e-05
)",<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb9d10d7b8>,10.0,28342833.0,27.0,16.0,0.421188,0.35687399999999997,0.7834,,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'data_imgsize_240'
        self.img_size = 256
        # self.img_size_crop = 300

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        # self.num_classes = 5
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 10
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP

        self.num_workers = 16
        self.n_epochs = 150
        self.batch_size = batch_size
        self.valid_type = 'holdout' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50","from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config
import math

# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p



def subtract_median_bg_image(im):
    k = np.max(im.shape)//20*2+1
    bg = cv2.medianBlur(im, k)
    return cv2.addWeighted (im, 4, bg, -4, 128)

PARAM = 96
def Radius_Reduction(img,PARAM):
    h,w,c=img.shape
    Frame=np.zeros((h,w,c),dtype=np.uint8)
    cv2.circle(Frame,(int(math.floor(w/2)),int(math.floor(h/2))),int(math.floor((h*PARAM)/float(2*100))), (255,255,255), -1)
    Frame1=cv2.cvtColor(Frame, cv2.COLOR_BGR2GRAY)
    img1 =cv2.bitwise_and(img,img,mask=Frame1)
    return img1

def info_image(im):
    # Compute the center (cx, cy) and radius of the eye
    cy = im.shape[0]//2
    midline = im[cy,:]
    midline = np.where(midline>midline.mean()/3)[0]
    if len(midline)>im.shape[1]//2:
        x_start, x_end = np.min(midline), np.max(midline)
    else: # This actually rarely happens p~1/10000
        x_start, x_end = im.shape[1]//10, 9*im.shape[1]//10
    cx = (x_start + x_end)/2
    r = (x_end - x_start)/2
    return cx, cy, r

def resize_image(im, image_size, augmentation=False):
    # Crops, resizes and potentially augments the image to IMAGE_SIZE
    cx, cy, r = info_image(im)
    scaling = image_size/(2*r)
    rotation = 0
    if augmentation:
        scaling *= 1 + 0.3 * (np.random.rand()-0.5)
        rotation = 360 * np.random.rand()
    M = cv2.getRotationMatrix2D((cx,cy), rotation, scaling)
    M[0,2] -= cx - image_size/2
    M[1,2] -= cy - image_size/2
    return cv2.warpAffine(im,M,(image_size, image_size))

class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)
        lab_planes = cv2.split(lab)
        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(16, 16))
        lab_planes[0] = clahe.apply(lab_planes[0])
        lab = cv2.merge(lab_planes)
        clahed = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)
        image = resize_image(clahed, cfg.img_size)

        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-150, 150)),
        transforms.RandomVerticalFlip(),
        transforms.ColorJitter(brightness=0, contrast=0, saturation=0, hue=0),
        # transforms.RandomResizedCrop(cfg.img_size_crop),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
        ])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedKFold

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'           #### NEED TO CHANGED IT!!!!!!!!!!!!!!!!!!!
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_images/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))
    print(""Training path: {}"".format(train_path))
    # Data Processing


    ## SHUFFLE DATA
    skf = None
    # if cfg.valid_type == 'holdout':
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True,
                                                                   random_state=cfg.seed, stratify=train_csv['diagnosis'])
    # train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid) # need to change It!!!!!
    # elif cfg.valid_type == 'cv':
    #     skf = StratifiedKFold(n_splits=cfg.n_folds, shuffle=True, random_state=cfg.seed)



    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model
    cfg.model.load_state_dict(
        torch.load('./Model_weights_finetuning/finetune8_256_crop_clahe_b5_colorjitter.pth')['model'])

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()

    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [1e-3]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b5']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",15525.78
2019.08.27 16:25:27,new_old,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        self.model = None
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'ResNet152':
            model = models.resnet152(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet152.pth""))
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b6':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b6-c76e70fd.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b7':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b7-dcc49843.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model


",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 1e-05
)",<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb9d10f668>,10.0,42502209.0,150.0,16.0,0.6831520000000001,0.718974,0.3346,,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'data_imgsize_240'
        self.img_size = 224
        # self.img_size_crop = 300

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        # self.num_classes = 5
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 10
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP

        self.num_workers = 16
        self.n_epochs = 150
        self.batch_size = batch_size
        self.valid_type = 'holdout' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50","from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config
import math

# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p



def subtract_median_bg_image(im):
    k = np.max(im.shape)//20*2+1
    bg = cv2.medianBlur(im, k)
    return cv2.addWeighted (im, 4, bg, -4, 128)

PARAM = 96
def Radius_Reduction(img,PARAM):
    h,w,c=img.shape
    Frame=np.zeros((h,w,c),dtype=np.uint8)
    cv2.circle(Frame,(int(math.floor(w/2)),int(math.floor(h/2))),int(math.floor((h*PARAM)/float(2*100))), (255,255,255), -1)
    Frame1=cv2.cvtColor(Frame, cv2.COLOR_BGR2GRAY)
    img1 =cv2.bitwise_and(img,img,mask=Frame1)
    return img1

def info_image(im):
    # Compute the center (cx, cy) and radius of the eye
    cy = im.shape[0]//2
    midline = im[cy,:]
    midline = np.where(midline>midline.mean()/3)[0]
    if len(midline)>im.shape[1]//2:
        x_start, x_end = np.min(midline), np.max(midline)
    else: # This actually rarely happens p~1/10000
        x_start, x_end = im.shape[1]//10, 9*im.shape[1]//10
    cx = (x_start + x_end)/2
    r = (x_end - x_start)/2
    return cx, cy, r

def resize_image(im, image_size, augmentation=False):
    # Crops, resizes and potentially augments the image to IMAGE_SIZE
    cx, cy, r = info_image(im)
    scaling = image_size/(2*r)
    rotation = 0
    if augmentation:
        scaling *= 1 + 0.3 * (np.random.rand()-0.5)
        rotation = 360 * np.random.rand()
    M = cv2.getRotationMatrix2D((cx,cy), rotation, scaling)
    M[0,2] -= cx - image_size/2
    M[1,2] -= cy - image_size/2
    return cv2.warpAffine(im,M,(image_size, image_size))

class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)
        lab_planes = cv2.split(lab)
        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(16, 16))
        lab_planes[0] = clahe.apply(lab_planes[0])
        lab = cv2.merge(lab_planes)
        clahed = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)
        image = resize_image(clahed, cfg.img_size)

        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-150, 150)),
        transforms.RandomVerticalFlip(),
        transforms.ColorJitter(brightness=0, contrast=0, saturation=0, hue=0),
        # transforms.RandomResizedCrop(cfg.img_size_crop),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
        ])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedKFold

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'           #### NEED TO CHANGED IT!!!!!!!!!!!!!!!!!!!
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_images/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))
    print(""Training path: {}"".format(train_path))
    # Data Processing


    ## SHUFFLE DATA
    skf = None
    # if cfg.valid_type == 'holdout':
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True,
                                                                   random_state=cfg.seed, stratify=train_csv['diagnosis'])
    # train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid) # need to change It!!!!!
    # elif cfg.valid_type == 'cv':
    #     skf = StratifiedKFold(n_splits=cfg.n_folds, shuffle=True, random_state=cfg.seed)



    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model
    cfg.model.load_state_dict(
        torch.load('./Model_weights_finetuning/finetune9_256_crop_clahe_b5_colorjitter_verticalflip.pth')['model'])

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()

    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [1e-3]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b5']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",
2019.08.27 18:33:15,new_old,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        self.model = None
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'ResNet152':
            model = models.resnet152(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet152.pth""))
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            # model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b6':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b6-c76e70fd.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b7':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b7-dcc49843.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model


",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 1e-05
)",<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb95fa6e10>,10.0,28342833.0,37.0,16.0,0.574757,0.504608,0.4914,,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'data_imgsize_240'
        self.img_size = 224
        # self.img_size_crop = 300

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        # self.num_classes = 5
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 10
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP

        self.num_workers = 16
        self.n_epochs = 150
        self.batch_size = batch_size
        self.valid_type = 'holdout' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50","from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config
import math

# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p



def subtract_median_bg_image(im):
    k = np.max(im.shape)//20*2+1
    bg = cv2.medianBlur(im, k)
    return cv2.addWeighted (im, 4, bg, -4, 128)

PARAM = 96
def Radius_Reduction(img,PARAM):
    h,w,c=img.shape
    Frame=np.zeros((h,w,c),dtype=np.uint8)
    cv2.circle(Frame,(int(math.floor(w/2)),int(math.floor(h/2))),int(math.floor((h*PARAM)/float(2*100))), (255,255,255), -1)
    Frame1=cv2.cvtColor(Frame, cv2.COLOR_BGR2GRAY)
    img1 =cv2.bitwise_and(img,img,mask=Frame1)
    return img1

def info_image(im):
    # Compute the center (cx, cy) and radius of the eye
    cy = im.shape[0]//2
    midline = im[cy,:]
    midline = np.where(midline>midline.mean()/3)[0]
    if len(midline)>im.shape[1]//2:
        x_start, x_end = np.min(midline), np.max(midline)
    else: # This actually rarely happens p~1/10000
        x_start, x_end = im.shape[1]//10, 9*im.shape[1]//10
    cx = (x_start + x_end)/2
    r = (x_end - x_start)/2
    return cx, cy, r

def resize_image(im, image_size, augmentation=False):
    # Crops, resizes and potentially augments the image to IMAGE_SIZE
    cx, cy, r = info_image(im)
    scaling = image_size/(2*r)
    rotation = 0
    if augmentation:
        scaling *= 1 + 0.3 * (np.random.rand()-0.5)
        rotation = 360 * np.random.rand()
    M = cv2.getRotationMatrix2D((cx,cy), rotation, scaling)
    M[0,2] -= cx - image_size/2
    M[1,2] -= cy - image_size/2
    return cv2.warpAffine(im,M,(image_size, image_size))

class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)
        lab_planes = cv2.split(lab)
        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(16, 16))
        lab_planes[0] = clahe.apply(lab_planes[0])
        lab = cv2.merge(lab_planes)
        clahed = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)
        image = resize_image(clahed, cfg.img_size)

        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-150, 150)),
        transforms.RandomVerticalFlip(),
        transforms.ColorJitter(brightness=0, contrast=0, saturation=0, hue=0),
        # transforms.RandomResizedCrop(cfg.img_size_crop),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
        ])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedKFold

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'           #### NEED TO CHANGED IT!!!!!!!!!!!!!!!!!!!
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_images/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))
    print(""Training path: {}"".format(train_path))
    # Data Processing


    ## SHUFFLE DATA
    skf = None
    # if cfg.valid_type == 'holdout':
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True,
                                                                   random_state=cfg.seed, stratify=train_csv['diagnosis'])
    # train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid) # need to change It!!!!!
    # elif cfg.valid_type == 'cv':
    #     skf = StratifiedKFold(n_splits=cfg.n_folds, shuffle=True, random_state=cfg.seed)



    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model
    cfg.model.load_state_dict(
        torch.load('./Model_weights_finetuning/finetune9_256_crop_clahe_b5_colorjitter_verticalflip.pth')['model'])

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()

    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [1e-3]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b5']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",17782.93
2019.08.28 01:43:38,new_old,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        self.model = None
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'ResNet152':
            model = models.resnet152(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet152.pth""))
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            # model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b6':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b6-c76e70fd.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b7':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b7-dcc49843.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model


",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 1e-05
)",<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb95faad68>,20.0,28342833.0,47.0,16.0,0.53897,0.494566,0.5432,,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'data_imgsize_240'
        self.img_size = 256
        # self.img_size_crop = 300

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        # self.num_classes = 5
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 20
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP

        self.num_workers = 16
        self.n_epochs = 200
        self.batch_size = batch_size
        self.valid_type = 'holdout' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50","from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config
import math

# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p



def subtract_median_bg_image(im):
    k = np.max(im.shape)//20*2+1
    bg = cv2.medianBlur(im, k)
    return cv2.addWeighted (im, 4, bg, -4, 128)

PARAM = 96
def Radius_Reduction(img,PARAM):
    h,w,c=img.shape
    Frame=np.zeros((h,w,c),dtype=np.uint8)
    cv2.circle(Frame,(int(math.floor(w/2)),int(math.floor(h/2))),int(math.floor((h*PARAM)/float(2*100))), (255,255,255), -1)
    Frame1=cv2.cvtColor(Frame, cv2.COLOR_BGR2GRAY)
    img1 =cv2.bitwise_and(img,img,mask=Frame1)
    return img1

def info_image(im):
    # Compute the center (cx, cy) and radius of the eye
    cy = im.shape[0]//2
    midline = im[cy,:]
    midline = np.where(midline>midline.mean()/3)[0]
    if len(midline)>im.shape[1]//2:
        x_start, x_end = np.min(midline), np.max(midline)
    else: # This actually rarely happens p~1/10000
        x_start, x_end = im.shape[1]//10, 9*im.shape[1]//10
    cx = (x_start + x_end)/2
    r = (x_end - x_start)/2
    return cx, cy, r

def resize_image(im, image_size, augmentation=False):
    # Crops, resizes and potentially augments the image to IMAGE_SIZE
    cx, cy, r = info_image(im)
    scaling = image_size/(2*r)
    rotation = 0
    if augmentation:
        scaling *= 1 + 0.3 * (np.random.rand()-0.5)
        rotation = 360 * np.random.rand()
    M = cv2.getRotationMatrix2D((cx,cy), rotation, scaling)
    M[0,2] -= cx - image_size/2
    M[1,2] -= cy - image_size/2
    return cv2.warpAffine(im,M,(image_size, image_size))

class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)
        lab_planes = cv2.split(lab)
        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(16, 16))
        lab_planes[0] = clahe.apply(lab_planes[0])
        lab = cv2.merge(lab_planes)
        clahed = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)
        image = resize_image(clahed, cfg.img_size)

        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-150, 150)),
        transforms.RandomVerticalFlip(),
        transforms.ColorJitter(brightness=0, contrast=0, saturation=0, hue=0),
        # transforms.RandomResizedCrop(cfg.img_size_crop),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
        ])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedKFold

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'           #### NEED TO CHANGED IT!!!!!!!!!!!!!!!!!!!
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_images/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))
    print(""Training path: {}"".format(train_path))
    # Data Processing


    ## SHUFFLE DATA
    skf = None
    # if cfg.valid_type == 'holdout':
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True,
                                                                   random_state=cfg.seed, stratify=train_csv['diagnosis'])
    # train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid) # need to change It!!!!!
    # elif cfg.valid_type == 'cv':
    #     skf = StratifiedKFold(n_splits=cfg.n_folds, shuffle=True, random_state=cfg.seed)



    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model
    cfg.model.load_state_dict(
        torch.load('./Model_weights_finetuning/finetune10_like_9_img256_not_pretrained.pth')['model'])

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()

    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [1e-3]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b5']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",26647.74
2019.08.28 14:21:09,new_old,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        self.model = None
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'ResNet152':
            model = models.resnet152(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet152.pth""))
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            # model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b6':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b6-c76e70fd.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b7':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b7-dcc49843.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model


",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.01
    weight_decay: 1e-05
)",<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb9d10a748>,20.0,28342833.0,23.0,16.0,1.02047,1.69511,0.0,,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'data_imgsize_240'
        self.img_size = 256
        # self.img_size_crop = 300

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        # self.num_classes = 5
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 20
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP

        self.num_workers = 16
        self.n_epochs = 200
        self.batch_size = batch_size
        self.valid_type = 'holdout' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50","from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config
import math

# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p



def subtract_median_bg_image(im):
    k = np.max(im.shape)//20*2+1
    bg = cv2.medianBlur(im, k)
    return cv2.addWeighted (im, 4, bg, -4, 128)

PARAM = 96
def Radius_Reduction(img,PARAM):
    h,w,c=img.shape
    Frame=np.zeros((h,w,c),dtype=np.uint8)
    cv2.circle(Frame,(int(math.floor(w/2)),int(math.floor(h/2))),int(math.floor((h*PARAM)/float(2*100))), (255,255,255), -1)
    Frame1=cv2.cvtColor(Frame, cv2.COLOR_BGR2GRAY)
    img1 =cv2.bitwise_and(img,img,mask=Frame1)
    return img1

def info_image(im):
    # Compute the center (cx, cy) and radius of the eye
    cy = im.shape[0]//2
    midline = im[cy,:]
    midline = np.where(midline>midline.mean()/3)[0]
    if len(midline)>im.shape[1]//2:
        x_start, x_end = np.min(midline), np.max(midline)
    else: # This actually rarely happens p~1/10000
        x_start, x_end = im.shape[1]//10, 9*im.shape[1]//10
    cx = (x_start + x_end)/2
    r = (x_end - x_start)/2
    return cx, cy, r

def resize_image(im, image_size, augmentation=False):
    # Crops, resizes and potentially augments the image to IMAGE_SIZE
    cx, cy, r = info_image(im)
    scaling = image_size/(2*r)
    rotation = 0
    if augmentation:
        scaling *= 1 + 0.3 * (np.random.rand()-0.5)
        rotation = 360 * np.random.rand()
    M = cv2.getRotationMatrix2D((cx,cy), rotation, scaling)
    M[0,2] -= cx - image_size/2
    M[1,2] -= cy - image_size/2
    return cv2.warpAffine(im,M,(image_size, image_size))

class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)
        lab_planes = cv2.split(lab)
        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(16, 16))
        lab_planes[0] = clahe.apply(lab_planes[0])
        lab = cv2.merge(lab_planes)
        clahed = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)
        image = resize_image(clahed, cfg.img_size)

        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-150, 150)),
        transforms.RandomVerticalFlip(),
        transforms.ColorJitter(brightness=0, contrast=0, saturation=0, hue=0),
        # transforms.RandomResizedCrop(cfg.img_size_crop),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
        ])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedKFold

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'           #### NEED TO CHANGED IT!!!!!!!!!!!!!!!!!!!
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_images/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))
    print(""Training path: {}"".format(train_path))
    # Data Processing


    ## SHUFFLE DATA
    skf = None
    # if cfg.valid_type == 'holdout':
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True,
                                                                   random_state=cfg.seed, stratify=train_csv['diagnosis'])
    # train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid) # need to change It!!!!!
    # elif cfg.valid_type == 'cv':
    #     skf = StratifiedKFold(n_splits=cfg.n_folds, shuffle=True, random_state=cfg.seed)



    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model
    cfg.model.load_state_dict(
        torch.load('./Model_weights_finetuning/finetune11_like_10_imgsize256.pth')['model'])

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()

    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [1e-3]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b5']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",13076.83
2019.08.28 18:54:12,new_old,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        self.model = None
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'ResNet152':
            model = models.resnet152(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet152.pth""))
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            # model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b6':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b6-c76e70fd.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b7':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b7-dcc49843.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            model.cuda()
            self.model = model


",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.005
    weight_decay: 1e-05
)",<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb95faad68>,20.0,28342833.0,200.0,16.0,0.976902,1.826091,0.0,,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'data_imgsize_240'
        self.img_size = 256
        # self.img_size_crop = 300

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        # self.num_classes = 5
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 20
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP

        self.num_workers = 16
        self.n_epochs = 200
        self.batch_size = batch_size
        self.valid_type = 'holdout' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50","from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config
import math

# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p



def subtract_median_bg_image(im):
    k = np.max(im.shape)//20*2+1
    bg = cv2.medianBlur(im, k)
    return cv2.addWeighted (im, 4, bg, -4, 128)

PARAM = 96
def Radius_Reduction(img,PARAM):
    h,w,c=img.shape
    Frame=np.zeros((h,w,c),dtype=np.uint8)
    cv2.circle(Frame,(int(math.floor(w/2)),int(math.floor(h/2))),int(math.floor((h*PARAM)/float(2*100))), (255,255,255), -1)
    Frame1=cv2.cvtColor(Frame, cv2.COLOR_BGR2GRAY)
    img1 =cv2.bitwise_and(img,img,mask=Frame1)
    return img1

def info_image(im):
    # Compute the center (cx, cy) and radius of the eye
    cy = im.shape[0]//2
    midline = im[cy,:]
    midline = np.where(midline>midline.mean()/3)[0]
    if len(midline)>im.shape[1]//2:
        x_start, x_end = np.min(midline), np.max(midline)
    else: # This actually rarely happens p~1/10000
        x_start, x_end = im.shape[1]//10, 9*im.shape[1]//10
    cx = (x_start + x_end)/2
    r = (x_end - x_start)/2
    return cx, cy, r

def resize_image(im, image_size, augmentation=False):
    # Crops, resizes and potentially augments the image to IMAGE_SIZE
    cx, cy, r = info_image(im)
    scaling = image_size/(2*r)
    rotation = 0
    if augmentation:
        scaling *= 1 + 0.3 * (np.random.rand()-0.5)
        rotation = 360 * np.random.rand()
    M = cv2.getRotationMatrix2D((cx,cy), rotation, scaling)
    M[0,2] -= cx - image_size/2
    M[1,2] -= cy - image_size/2
    return cv2.warpAffine(im,M,(image_size, image_size))

class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)
        lab_planes = cv2.split(lab)
        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(16, 16))
        lab_planes[0] = clahe.apply(lab_planes[0])
        lab = cv2.merge(lab_planes)
        clahed = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)
        image = resize_image(clahed, cfg.img_size)

        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-150, 150)),
        transforms.RandomVerticalFlip(),
        transforms.ColorJitter(brightness=0, contrast=0, saturation=0, hue=0),
        # transforms.RandomResizedCrop(cfg.img_size_crop),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
        ])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedKFold

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'           #### NEED TO CHANGED IT!!!!!!!!!!!!!!!!!!!
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_images/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))
    print(""Training path: {}"".format(train_path))
    # Data Processing


    ## SHUFFLE DATA
    skf = None
    # if cfg.valid_type == 'holdout':
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True,
                                                                   random_state=cfg.seed, stratify=train_csv['diagnosis'])
    # train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid) # need to change It!!!!!
    # elif cfg.valid_type == 'cv':
    #     skf = StratifiedKFold(n_splits=cfg.n_folds, shuffle=True, random_state=cfg.seed)



    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model
    cfg.model.load_state_dict(
        torch.load('./Model_weights_finetuning/finetune11_like_10_imgsize256.pth')['model'])

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()

    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [1e-3]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b5']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",
2019.08.29 03:31:26,new_old,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        self.model = None
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'ResNet152':
            model = models.resnet152(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet152.pth""))
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            # model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            # model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))
            model._conv_stem = nn.Conv2d(4, 48, kernel_size=3, stride=2, bias=False)
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            # #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b6':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b6-c76e70fd.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b7':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b7-dcc49843.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            #model.cuda()
            self.model = model


",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 1e-05
)",<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb95fb11d0>,20.0,7702403.0,200.0,16.0,0.618409,0.595683,0.6325,,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'data_imgsize_240'
        self.img_size = 256
        # self.img_size_crop = 300

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        # self.num_classes = 5
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 20
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'kappa' #kappa

        ## TRAINING & VALIDATION SETUP

        self.num_workers = 16
        self.n_epochs = 200
        self.batch_size = batch_size
        self.valid_type = 'holdout' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50","# from retinal_process import *
# from utils_img import *

from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config
import math

# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()
# import tensorflow as tf

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p



def subtract_median_bg_image(im):
    k = np.max(im.shape)//20*2+1
    bg = cv2.medianBlur(im, k)
    return cv2.addWeighted (im, 4, bg, -4, 128)

PARAM = 96
def Radius_Reduction(img,PARAM):
    h,w,c=img.shape
    Frame=np.zeros((h,w,c),dtype=np.uint8)
    cv2.circle(Frame,(int(math.floor(w/2)),int(math.floor(h/2))),int(math.floor((h*PARAM)/float(2*100))), (255,255,255), -1)
    Frame1=cv2.cvtColor(Frame, cv2.COLOR_BGR2GRAY)
    img1 =cv2.bitwise_and(img,img,mask=Frame1)
    return img1

def info_image(im):
    # Compute the center (cx, cy) and radius of the eye
    cy = im.shape[0]//2
    midline = im[cy,:]
    midline = np.where(midline>midline.mean()/3)[0]
    if len(midline)>im.shape[1]//2:
        x_start, x_end = np.min(midline), np.max(midline)
    else: # This actually rarely happens p~1/10000
        x_start, x_end = im.shape[1]//10, 9*im.shape[1]//10
    cx = (x_start + x_end)/2
    r = (x_end - x_start)/2
    return cx, cy, r

def resize_image(im, image_size, augmentation=False):
    # Crops, resizes and potentially augments the image to IMAGE_SIZE
    cx, cy, r = info_image(im)
    scaling = image_size/(2*r)
    rotation = 0
    if augmentation:
        scaling *= 1 + 0.3 * (np.random.rand()-0.5)
        rotation = 360 * np.random.rand()
    M = cv2.getRotationMatrix2D((cx,cy), rotation, scaling)
    M[0,2] -= cx - image_size/2
    M[1,2] -= cy - image_size/2
    return cv2.warpAffine(im,M,(image_size, image_size))

class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)
        # print(tf.test.is_gpu_available())
        lab_planes = cv2.split(lab)
        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(16, 16))
        lab_planes[0] = clahe.apply(lab_planes[0])
        lab = cv2.merge(lab_planes)
        clahed = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)
        image = resize_image(clahed, cfg.img_size)
        # vessel_model = VesselNet('./vessels/')
        # process(lab, vessel_model=vessel_model)
        # print(np.shape(image))
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-150, 150)),
        transforms.RandomVerticalFlip(),
        transforms.ColorJitter(brightness=0, contrast=0, saturation=0, hue=0),
        # transforms.RandomResizedCrop(cfg.img_size_crop),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
        ])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","
import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedKFold

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'           #### NEED TO CHANGED IT!!!!!!!!!!!!!!!!!!!
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_images/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))
    print(""Training path: {}"".format(train_path))
    # Data Processing


    ## SHUFFLE DATA
    skf = None
    # if cfg.valid_type == 'holdout':
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True,
                                                                   random_state=cfg.seed, stratify=train_csv['diagnosis'])
    # train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid) # need to change It!!!!!
    # elif cfg.valid_type == 'cv':
    #     skf = StratifiedKFold(n_splits=cfg.n_folds, shuffle=True, random_state=cfg.seed)



    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model
    # cfg.model.load_state_dict(
    #     torch.load('./Model_weights_finetuning/finetune11_like_10_imgsize256.pth')['model'])

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()

    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                pass
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                pass
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [1e-3]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b5']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",
2019.08.29 08:49:45,new_old,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        self.model = None
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'ResNet152':
            model = models.resnet152(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet152.pth""))
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1, bias=True),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            # model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            # model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))
            model._conv_stem = nn.Conv2d(4, 48, kernel_size=3, stride=2, bias=False)
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            # #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b6':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b6-c76e70fd.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b7':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b7-dcc49843.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            #model.cuda()
            self.model = model


",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 1e-05
)",<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb95fb0198>,20.0,7702403.0,200.0,16.0,0.469759,0.5761539999999999,0.8038,,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'data_imgsize_240'
        self.img_size = 256
        # self.img_size_crop = 300

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        # self.num_classes = 5
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 20
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'kappa' #kappa

        ## TRAINING & VALIDATION SETUP

        self.num_workers = 16
        self.n_epochs = 200
        self.batch_size = batch_size
        self.valid_type = 'holdout' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50","# from retinal_process import *
# from utils_img import *

from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config
import math

# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()
# import tensorflow as tf

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p



def subtract_median_bg_image(im):
    k = np.max(im.shape)//20*2+1
    bg = cv2.medianBlur(im, k)
    return cv2.addWeighted (im, 4, bg, -4, 128)

PARAM = 96
def Radius_Reduction(img,PARAM):
    h,w,c=img.shape
    Frame=np.zeros((h,w,c),dtype=np.uint8)
    cv2.circle(Frame,(int(math.floor(w/2)),int(math.floor(h/2))),int(math.floor((h*PARAM)/float(2*100))), (255,255,255), -1)
    Frame1=cv2.cvtColor(Frame, cv2.COLOR_BGR2GRAY)
    img1 =cv2.bitwise_and(img,img,mask=Frame1)
    return img1

def info_image(im):
    # Compute the center (cx, cy) and radius of the eye
    cy = im.shape[0]//2
    midline = im[cy,:]
    midline = np.where(midline>midline.mean()/3)[0]
    if len(midline)>im.shape[1]//2:
        x_start, x_end = np.min(midline), np.max(midline)
    else: # This actually rarely happens p~1/10000
        x_start, x_end = im.shape[1]//10, 9*im.shape[1]//10
    cx = (x_start + x_end)/2
    r = (x_end - x_start)/2
    return cx, cy, r

def resize_image(im, image_size, augmentation=False):
    # Crops, resizes and potentially augments the image to IMAGE_SIZE
    cx, cy, r = info_image(im)
    scaling = image_size/(2*r)
    rotation = 0
    if augmentation:
        scaling *= 1 + 0.3 * (np.random.rand()-0.5)
        rotation = 360 * np.random.rand()
    M = cv2.getRotationMatrix2D((cx,cy), rotation, scaling)
    M[0,2] -= cx - image_size/2
    M[1,2] -= cy - image_size/2
    return cv2.warpAffine(im,M,(image_size, image_size))

class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)
        # print(tf.test.is_gpu_available())
        lab_planes = cv2.split(lab)
        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(16, 16))
        lab_planes[0] = clahe.apply(lab_planes[0])
        lab = cv2.merge(lab_planes)
        clahed = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)
        image = resize_image(clahed, cfg.img_size)
        # vessel_model = VesselNet('./vessels/')
        # process(lab, vessel_model=vessel_model)
        # print(np.shape(image))
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-150, 150)),
        transforms.RandomVerticalFlip(),
        transforms.ColorJitter(brightness=0, contrast=0, saturation=0, hue=0),
        # transforms.RandomResizedCrop(cfg.img_size_crop),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
        ])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","
import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedKFold

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'           #### NEED TO CHANGED IT!!!!!!!!!!!!!!!!!!!
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_images/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))
    print(""Training path: {}"".format(train_path))
    # Data Processing


    ## SHUFFLE DATA
    skf = None
    # if cfg.valid_type == 'holdout':
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True,
                                                                   random_state=cfg.seed, stratify=train_csv['diagnosis'])
    # train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid) # need to change It!!!!!
    # elif cfg.valid_type == 'cv':
    #     skf = StratifiedKFold(n_splits=cfg.n_folds, shuffle=True, random_state=cfg.seed)



    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model
    # cfg.model.load_state_dict(
    #     torch.load('./Model_weights_finetuning/finetune11_like_10_imgsize256.pth')['model'])

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()

    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                pass
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                pass
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [1e-3]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b5']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",
2019.08.30 00:31:08,new_old,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        self.model = None
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'ResNet152':
            model = models.resnet152(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet152.pth""))
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))
            # model._conv_stem = nn.Conv2d(4, 48, kernel_size=3, stride=2, bias=False)
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            # #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b6':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b6-c76e70fd.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b7':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b7-dcc49843.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            #model.cuda()
            self.model = model


",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 1e-05
)",<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb95faf080>,10.0,6514465.0,200.0,16.0,0.580902,0.510293,0.7875,,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'data_imgsize_240'
        self.img_size = 240
        # self.img_size_crop = 300

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        # self.num_classes = 5
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 10
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP

        self.num_workers = 16
        self.n_epochs = 200
        self.batch_size = batch_size
        self.valid_type = 'holdout' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50","# from retinal_process import *
# from utils_img import *

from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config
import math

# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()
# import tensorflow as tf

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p



def subtract_median_bg_image(im):
    k = np.max(im.shape)//20*2+1
    bg = cv2.medianBlur(im, k)
    return cv2.addWeighted (im, 4, bg, -4, 128)

PARAM = 96
def Radius_Reduction(img,PARAM):
    h,w,c=img.shape
    Frame=np.zeros((h,w,c),dtype=np.uint8)
    cv2.circle(Frame,(int(math.floor(w/2)),int(math.floor(h/2))),int(math.floor((h*PARAM)/float(2*100))), (255,255,255), -1)
    Frame1=cv2.cvtColor(Frame, cv2.COLOR_BGR2GRAY)
    img1 =cv2.bitwise_and(img,img,mask=Frame1)
    return img1

def info_image(im):
    # Compute the center (cx, cy) and radius of the eye
    cy = im.shape[0]//2
    midline = im[cy,:]
    midline = np.where(midline>midline.mean()/3)[0]
    if len(midline)>im.shape[1]//2:
        x_start, x_end = np.min(midline), np.max(midline)
    else: # This actually rarely happens p~1/10000
        x_start, x_end = im.shape[1]//10, 9*im.shape[1]//10
    cx = (x_start + x_end)/2
    r = (x_end - x_start)/2
    return cx, cy, r

def resize_image(im, image_size, augmentation=False):
    # Crops, resizes and potentially augments the image to IMAGE_SIZE
    cx, cy, r = info_image(im)
    scaling = image_size/(2*r)
    rotation = 0
    if augmentation:
        scaling *= 1 + 0.3 * (np.random.rand()-0.5)
        rotation = 360 * np.random.rand()
    M = cv2.getRotationMatrix2D((cx,cy), rotation, scaling)
    M[0,2] -= cx - image_size/2
    M[1,2] -= cy - image_size/2
    return cv2.warpAffine(im,M,(image_size, image_size))

class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)
        lab_planes = cv2.split(lab)
        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(16, 16))
        lab_planes[0] = clahe.apply(lab_planes[0])
        lab = cv2.merge(lab_planes)
        clahed = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)
        image = resize_image(clahed, cfg.img_size)
        # print(tf.test.is_gpu_available())
        # vessel_model = VesselNet('./vessels/')
        # process(lab, vessel_model=vessel_model)
        # print(np.shape(image))
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-150, 150)),
        transforms.RandomVerticalFlip(),
        transforms.ColorJitter(brightness=0, contrast=0, saturation=0, hue=0),
        # transforms.RandomResizedCrop(cfg.img_size_crop),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
        ])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","
import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedKFold

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'           #### NEED TO CHANGED IT!!!!!!!!!!!!!!!!!!!
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_images/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))
    print(""Training path: {}"".format(train_path))
    # Data Processing


    ## SHUFFLE DATA
    skf = None
    # if cfg.valid_type == 'holdout':
    test_805 = pd.read_csv('./input/test_805.csv')
    test_805 = pd.concat([test_805, test_805])
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True,
                                                                   random_state=cfg.seed, stratify=train['diagnosis'])
    # train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid) # need to change It!!!!!
    # elif cfg.valid_type == 'cv':
    #     skf = StratifiedKFold(n_splits=cfg.n_folds, shuffle=True, random_state=cfg.seed)



    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model
    # cfg.model.load_state_dict(
    #     torch.load('./Model_weights_finetuning/finetune12_clahe_crop_256_b2_kappa.pth')['model'])

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()

    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                pass
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                pass
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [1e-3]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b5']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",
2019.08.30 00:44:35,new_old,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        self.model = None
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'ResNet152':
            model = models.resnet152(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet152.pth""))
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            # model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))
            # model._conv_stem = nn.Conv2d(4, 48, kernel_size=3, stride=2, bias=False)
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            # #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b6':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b6-c76e70fd.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b7':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b7-dcc49843.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            #model.cuda()
            self.model = model


",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 1e-05
)",<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb9d100e48>,10.0,6514465.0,32.0,16.0,0.550095,0.490894,0.5339,,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'data_imgsize_240'
        self.img_size = 240
        # self.img_size_crop = 300

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        # self.num_classes = 5
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 10
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP

        self.num_workers = 16
        self.n_epochs = 200
        self.batch_size = batch_size
        self.valid_type = 'holdout' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50","# from retinal_process import *
# from utils_img import *

from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config
import math

# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()
# import tensorflow as tf

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p



def subtract_median_bg_image(im):
    k = np.max(im.shape)//20*2+1
    bg = cv2.medianBlur(im, k)
    return cv2.addWeighted (im, 4, bg, -4, 128)

PARAM = 96
def Radius_Reduction(img,PARAM):
    h,w,c=img.shape
    Frame=np.zeros((h,w,c),dtype=np.uint8)
    cv2.circle(Frame,(int(math.floor(w/2)),int(math.floor(h/2))),int(math.floor((h*PARAM)/float(2*100))), (255,255,255), -1)
    Frame1=cv2.cvtColor(Frame, cv2.COLOR_BGR2GRAY)
    img1 =cv2.bitwise_and(img,img,mask=Frame1)
    return img1

def info_image(im):
    # Compute the center (cx, cy) and radius of the eye
    cy = im.shape[0]//2
    midline = im[cy,:]
    midline = np.where(midline>midline.mean()/3)[0]
    if len(midline)>im.shape[1]//2:
        x_start, x_end = np.min(midline), np.max(midline)
    else: # This actually rarely happens p~1/10000
        x_start, x_end = im.shape[1]//10, 9*im.shape[1]//10
    cx = (x_start + x_end)/2
    r = (x_end - x_start)/2
    return cx, cy, r

def resize_image(im, image_size, augmentation=False):
    # Crops, resizes and potentially augments the image to IMAGE_SIZE
    cx, cy, r = info_image(im)
    scaling = image_size/(2*r)
    rotation = 0
    if augmentation:
        scaling *= 1 + 0.3 * (np.random.rand()-0.5)
        rotation = 360 * np.random.rand()
    M = cv2.getRotationMatrix2D((cx,cy), rotation, scaling)
    M[0,2] -= cx - image_size/2
    M[1,2] -= cy - image_size/2
    return cv2.warpAffine(im,M,(image_size, image_size))

class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)
        lab_planes = cv2.split(lab)
        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(16, 16))
        lab_planes[0] = clahe.apply(lab_planes[0])
        lab = cv2.merge(lab_planes)
        clahed = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)
        image = resize_image(clahed, cfg.img_size)
        # print(tf.test.is_gpu_available())
        # vessel_model = VesselNet('./vessels/')
        # process(lab, vessel_model=vessel_model)
        # print(np.shape(image))
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-150, 150)),
        transforms.RandomVerticalFlip(),
        transforms.ColorJitter(brightness=0, contrast=0, saturation=0, hue=0),
        # transforms.RandomResizedCrop(cfg.img_size_crop),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
        ])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","
import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedKFold

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'           #### NEED TO CHANGED IT!!!!!!!!!!!!!!!!!!!
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_images/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))
    print(""Training path: {}"".format(train_path))
    # Data Processing


    ## SHUFFLE DATA
    skf = None
    # if cfg.valid_type == 'holdout':
    test_805 = pd.read_csv('./input/test_805.csv')
    test_805 = pd.concat([test_805, test_805])
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True,
                                                                   random_state=cfg.seed, stratify=train['diagnosis'])
    # train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid) # need to change It!!!!!
    # elif cfg.valid_type == 'cv':
    #     skf = StratifiedKFold(n_splits=cfg.n_folds, shuffle=True, random_state=cfg.seed)



    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model
    # cfg.model.load_state_dict(
    #     torch.load('./Model_weights_finetuning/finetune12_clahe_crop_256_b2_kappa.pth')['model'])

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()

    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                pass
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                pass
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [1e-3]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b5']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",10051.48
2019.08.30 14:06:32,new_old,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        self.model = None
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'ResNet152':
            model = models.resnet152(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet152.pth""))
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            # model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))
            # model._conv_stem = nn.Conv2d(4, 48, kernel_size=3, stride=2, bias=False)
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            # #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b6':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b6-c76e70fd.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b7':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b7-dcc49843.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            #model.cuda()
            self.model = model


",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 1e-05
)",<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb95fae128>,10.0,6514465.0,11.0,16.0,0.95726,1.94313,0.0,,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'data_imgsize_240'
        self.img_size = 240
        # self.img_size_crop = 300

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        # self.num_classes = 5
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 10
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP

        self.num_workers = 16
        self.n_epochs = 200
        self.batch_size = batch_size
        self.valid_type = 'holdout' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50","# from retinal_process import *
# from utils_img import *

from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config
import math

# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()
# import tensorflow as tf

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p



def subtract_median_bg_image(im):
    k = np.max(im.shape)//20*2+1
    bg = cv2.medianBlur(im, k)
    return cv2.addWeighted (im, 4, bg, -4, 128)

PARAM = 96
def Radius_Reduction(img,PARAM):
    h,w,c=img.shape
    Frame=np.zeros((h,w,c),dtype=np.uint8)
    cv2.circle(Frame,(int(math.floor(w/2)),int(math.floor(h/2))),int(math.floor((h*PARAM)/float(2*100))), (255,255,255), -1)
    Frame1=cv2.cvtColor(Frame, cv2.COLOR_BGR2GRAY)
    img1 =cv2.bitwise_and(img,img,mask=Frame1)
    return img1

def info_image(im):
    # Compute the center (cx, cy) and radius of the eye
    cy = im.shape[0]//2
    midline = im[cy,:]
    midline = np.where(midline>midline.mean()/3)[0]
    if len(midline)>im.shape[1]//2:
        x_start, x_end = np.min(midline), np.max(midline)
    else: # This actually rarely happens p~1/10000
        x_start, x_end = im.shape[1]//10, 9*im.shape[1]//10
    cx = (x_start + x_end)/2
    r = (x_end - x_start)/2
    return cx, cy, r

def resize_image(im, image_size, augmentation=False):
    # Crops, resizes and potentially augments the image to IMAGE_SIZE
    cx, cy, r = info_image(im)
    scaling = image_size/(2*r)
    rotation = 0
    if augmentation:
        scaling *= 1 + 0.3 * (np.random.rand()-0.5)
        rotation = 360 * np.random.rand()
    M = cv2.getRotationMatrix2D((cx,cy), rotation, scaling)
    M[0,2] -= cx - image_size/2
    M[1,2] -= cy - image_size/2
    return cv2.warpAffine(im,M,(image_size, image_size))

class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        # lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)
        # lab_planes = cv2.split(lab)
        # clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(16, 16))
        # lab_planes[0] = clahe.apply(lab_planes[0])
        # lab = cv2.merge(lab_planes)
        # clahed = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)
        image = resize_image(image, cfg.img_size) #CHANGE TO CLAHED!
        # print(tf.test.is_gpu_available())
        # vessel_model = VesselNet('./vessels/')
        # process(lab, vessel_model=vessel_model)
        # print(np.shape(image))
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-150, 150)),
        transforms.RandomVerticalFlip(),
        transforms.ColorJitter(brightness=0, contrast=0, saturation=0, hue=0),
        # transforms.RandomResizedCrop(cfg.img_size_crop),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
        ])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","
import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedKFold

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'           #### NEED TO CHANGED IT!!!!!!!!!!!!!!!!!!!
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_images/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))
    print(""Training path: {}"".format(train_path))
    # Data Processing


    ## SHUFFLE DATA
    skf = None
    # if cfg.valid_type == 'holdout':
    test_805 = pd.read_csv('./input/test_805.csv')
    test_805 = pd.concat([test_805, test_805])
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True,
                                                                   random_state=cfg.seed, stratify=train_csv['diagnosis'])
    # train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid) # need to change It!!!!!
    # elif cfg.valid_type == 'cv':
    #     skf = StratifiedKFold(n_splits=cfg.n_folds, shuffle=True, random_state=cfg.seed)



    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model
    cfg.model.load_state_dict(
        torch.load('./Model_weights_finetuning/finetune13_clahe_crop_240_b1.pth')['model'])

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()

    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                pass
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                pass
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [1e-3]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b1']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",2828.33
2019.08.30 16:16:40,new_old,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        self.model = None
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'ResNet152':
            model = models.resnet152(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet152.pth""))
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            # model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))
            # model._conv_stem = nn.Conv2d(4, 48, kernel_size=3, stride=2, bias=False)
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            # #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b6':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b6-c76e70fd.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b7':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b7-dcc49843.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            #model.cuda()
            self.model = model


",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 1e-05
)",<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb9602d128>,10.0,6514465.0,11.0,16.0,0.955389,1.860145,0.0057,,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'data_imgsize_240'
        self.img_size = 240
        # self.img_size_crop = 300

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        # self.num_classes = 5
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 10
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP

        self.num_workers = 16
        self.n_epochs = 200
        self.batch_size = batch_size
        self.valid_type = 'holdout' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50","# from retinal_process import *
# from utils_img import *

from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config
import math

# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()
# import tensorflow as tf

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p



def subtract_median_bg_image(im):
    k = np.max(im.shape)//20*2+1
    bg = cv2.medianBlur(im, k)
    return cv2.addWeighted (im, 4, bg, -4, 128)

PARAM = 96
def Radius_Reduction(img,PARAM):
    h,w,c=img.shape
    Frame=np.zeros((h,w,c),dtype=np.uint8)
    cv2.circle(Frame,(int(math.floor(w/2)),int(math.floor(h/2))),int(math.floor((h*PARAM)/float(2*100))), (255,255,255), -1)
    Frame1=cv2.cvtColor(Frame, cv2.COLOR_BGR2GRAY)
    img1 =cv2.bitwise_and(img,img,mask=Frame1)
    return img1

def info_image(im):
    # Compute the center (cx, cy) and radius of the eye
    cy = im.shape[0]//2
    midline = im[cy,:]
    midline = np.where(midline>midline.mean()/3)[0]
    if len(midline)>im.shape[1]//2:
        x_start, x_end = np.min(midline), np.max(midline)
    else: # This actually rarely happens p~1/10000
        x_start, x_end = im.shape[1]//10, 9*im.shape[1]//10
    cx = (x_start + x_end)/2
    r = (x_end - x_start)/2
    return cx, cy, r

def resize_image(im, image_size, augmentation=False):
    # Crops, resizes and potentially augments the image to IMAGE_SIZE
    cx, cy, r = info_image(im)
    scaling = image_size/(2*r)
    rotation = 0
    if augmentation:
        scaling *= 1 + 0.3 * (np.random.rand()-0.5)
        rotation = 360 * np.random.rand()
    M = cv2.getRotationMatrix2D((cx,cy), rotation, scaling)
    M[0,2] -= cx - image_size/2
    M[1,2] -= cy - image_size/2
    return cv2.warpAffine(im,M,(image_size, image_size))

class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        # lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)
        # lab_planes = cv2.split(lab)
        # clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(16, 16))
        # lab_planes[0] = clahe.apply(lab_planes[0])
        # lab = cv2.merge(lab_planes)
        # clahed = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)
        image = resize_image(image, cfg.img_size) #CHANGE TO CLAHED!
        # print(tf.test.is_gpu_available())
        # vessel_model = VesselNet('./vessels/')
        # process(lab, vessel_model=vessel_model)
        # print(np.shape(image))
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-150, 150)),
        transforms.RandomVerticalFlip(),
        transforms.ColorJitter(brightness=0, contrast=0.5, saturation=0.5, hue=0.1),
        # transforms.RandomResizedCrop(cfg.img_size_crop),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
        ])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","
import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedKFold

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'           #### NEED TO CHANGED IT!!!!!!!!!!!!!!!!!!!
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_images/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))
    print(""Training path: {}"".format(train_path))
    # Data Processing


    ## SHUFFLE DATA
    skf = None
    # if cfg.valid_type == 'holdout':
    test_805 = pd.read_csv('./input/test_805.csv')
    test_805 = pd.concat([test_805, test_805])
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True,
                                                                   random_state=cfg.seed, stratify=train_csv['diagnosis'])
    # train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid) # need to change It!!!!!
    # elif cfg.valid_type == 'cv':
    #     skf = StratifiedKFold(n_splits=cfg.n_folds, shuffle=True, random_state=cfg.seed)



    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model
    cfg.model.load_state_dict(
        torch.load('./Model_weights_finetuning/finetune13_clahe_crop_240_b1.pth')['model'])

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()

    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                pass
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                pass
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [1e-3]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b1']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",2826.17
2019.08.30 18:52:03,new_old,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        self.model = None
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'ResNet152':
            model = models.resnet152(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet152.pth""))
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            # model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))
            # model._conv_stem = nn.Conv2d(4, 48, kernel_size=3, stride=2, bias=False)
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            # #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b6':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b6-c76e70fd.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b7':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b7-dcc49843.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            #model.cuda()
            self.model = model


",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 1e-05
)",<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb95f44470>,10.0,28342833.0,20.0,16.0,0.43098800000000004,0.42063999999999996,0.7764,,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'data_imgsize_256'
        self.img_size = 256
        # self.img_size_crop = 300

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        # self.num_classes = 5
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 10
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP

        self.num_workers = 16
        self.n_epochs = 200
        self.batch_size = batch_size
        self.valid_type = 'holdout' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50","# from retinal_process import *
# from utils_img import *

from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config
import math

# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()
# import tensorflow as tf

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p



def subtract_median_bg_image(im):
    k = np.max(im.shape)//20*2+1
    bg = cv2.medianBlur(im, k)
    return cv2.addWeighted (im, 4, bg, -4, 128)

PARAM = 96
def Radius_Reduction(img,PARAM):
    h,w,c=img.shape
    Frame=np.zeros((h,w,c),dtype=np.uint8)
    cv2.circle(Frame,(int(math.floor(w/2)),int(math.floor(h/2))),int(math.floor((h*PARAM)/float(2*100))), (255,255,255), -1)
    Frame1=cv2.cvtColor(Frame, cv2.COLOR_BGR2GRAY)
    img1 =cv2.bitwise_and(img,img,mask=Frame1)
    return img1

def info_image(im):
    # Compute the center (cx, cy) and radius of the eye
    cy = im.shape[0]//2
    midline = im[cy,:]
    midline = np.where(midline>midline.mean()/3)[0]
    if len(midline)>im.shape[1]//2:
        x_start, x_end = np.min(midline), np.max(midline)
    else: # This actually rarely happens p~1/10000
        x_start, x_end = im.shape[1]//10, 9*im.shape[1]//10
    cx = (x_start + x_end)/2
    r = (x_end - x_start)/2
    return cx, cy, r

def resize_image(im, image_size, augmentation=False):
    # Crops, resizes and potentially augments the image to IMAGE_SIZE
    cx, cy, r = info_image(im)
    scaling = image_size/(2*r)
    rotation = 0
    if augmentation:
        scaling *= 1 + 0.3 * (np.random.rand()-0.5)
        rotation = 360 * np.random.rand()
    M = cv2.getRotationMatrix2D((cx,cy), rotation, scaling)
    M[0,2] -= cx - image_size/2
    M[1,2] -= cy - image_size/2
    return cv2.warpAffine(im,M,(image_size, image_size))

class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)
        lab_planes = cv2.split(lab)
        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(16, 16))
        lab_planes[0] = clahe.apply(lab_planes[0])
        lab = cv2.merge(lab_planes)
        clahed = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)
        image = resize_image(clahed, cfg.img_size)
        # print(tf.test.is_gpu_available())
        # vessel_model = VesselNet('./vessels/')
        # process(lab, vessel_model=vessel_model)
        # print(np.shape(image))
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-150, 150)),
        transforms.RandomVerticalFlip(),
        transforms.ColorJitter(brightness=0, contrast=0.5, saturation=0.5, hue=0.1),
        # transforms.RandomResizedCrop(cfg.img_size_crop),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
        ])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","
import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedKFold

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'           #### NEED TO CHANGED IT!!!!!!!!!!!!!!!!!!!
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_images/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))
    print(""Training path: {}"".format(train_path))
    # Data Processing


    ## SHUFFLE DATA
    skf = None
    # if cfg.valid_type == 'holdout':
    test_805 = pd.read_csv('./input/test_805.csv')
    test_805 = pd.concat([test_805, test_805])
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True,
                                                                   random_state=cfg.seed, stratify=train_csv['diagnosis'])
    # train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid) # need to change It!!!!!
    # elif cfg.valid_type == 'cv':
    #     skf = StratifiedKFold(n_splits=cfg.n_folds, shuffle=True, random_state=cfg.seed)



    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model
    cfg.model.load_state_dict(
        torch.load('./Model_weights_finetuning/finetune13_clahe_crop_240_b1.pth')['model'])

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()

    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                pass
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                pass
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [1e-3]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b1']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",12441.74
2019.08.30 23:05:50,new_old,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        self.model = None
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'ResNet152':
            model = models.resnet152(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet152.pth""))
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            # model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))
            # model._conv_stem = nn.Conv2d(4, 48, kernel_size=3, stride=2, bias=False)
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            # #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b6':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b6-c76e70fd.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b7':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b7-dcc49843.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            #model.cuda()
            self.model = model


",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 1e-05
)",<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb95f444a8>,10.0,28342833.0,18.0,16.0,0.499034,0.386216,0.8079999999999999,,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'data_imgsize_256'
        self.img_size = 256
        # self.img_size_crop = 300

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        # self.num_classes = 5
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 10
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP

        self.num_workers = 16
        self.n_epochs = 200
        self.batch_size = batch_size
        self.valid_type = 'holdout' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50","# from retinal_process import *
# from utils_img import *

from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config
import math

# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()
# import tensorflow as tf

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p



def subtract_median_bg_image(im):
    k = np.max(im.shape)//20*2+1
    bg = cv2.medianBlur(im, k)
    return cv2.addWeighted (im, 4, bg, -4, 128)

PARAM = 96
def Radius_Reduction(img,PARAM):
    h,w,c=img.shape
    Frame=np.zeros((h,w,c),dtype=np.uint8)
    cv2.circle(Frame,(int(math.floor(w/2)),int(math.floor(h/2))),int(math.floor((h*PARAM)/float(2*100))), (255,255,255), -1)
    Frame1=cv2.cvtColor(Frame, cv2.COLOR_BGR2GRAY)
    img1 =cv2.bitwise_and(img,img,mask=Frame1)
    return img1

def info_image(im):
    # Compute the center (cx, cy) and radius of the eye
    cy = im.shape[0]//2
    midline = im[cy,:]
    midline = np.where(midline>midline.mean()/3)[0]
    if len(midline)>im.shape[1]//2:
        x_start, x_end = np.min(midline), np.max(midline)
    else: # This actually rarely happens p~1/10000
        x_start, x_end = im.shape[1]//10, 9*im.shape[1]//10
    cx = (x_start + x_end)/2
    r = (x_end - x_start)/2
    return cx, cy, r

def resize_image(im, image_size, augmentation=False):
    # Crops, resizes and potentially augments the image to IMAGE_SIZE
    cx, cy, r = info_image(im)
    scaling = image_size/(2*r)
    rotation = 0
    if augmentation:
        scaling *= 1 + 0.3 * (np.random.rand()-0.5)
        rotation = 360 * np.random.rand()
    M = cv2.getRotationMatrix2D((cx,cy), rotation, scaling)
    M[0,2] -= cx - image_size/2
    M[1,2] -= cy - image_size/2
    return cv2.warpAffine(im,M,(image_size, image_size))

class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)
        lab_planes = cv2.split(lab)
        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(16, 16))
        lab_planes[0] = clahe.apply(lab_planes[0])
        lab = cv2.merge(lab_planes)
        clahed = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)
        image = resize_image(clahed, cfg.img_size)
        # print(tf.test.is_gpu_available())
        # vessel_model = VesselNet('./vessels/')
        # process(lab, vessel_model=vessel_model)
        # print(np.shape(image))
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-150, 150)),
        transforms.RandomVerticalFlip(),
        transforms.ColorJitter(brightness=0.4, contrast=0.5, saturation=0.5, hue=0.1),
        # transforms.RandomResizedCrop(cfg.img_size_crop),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
        ])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","
import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedKFold

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'           #### NEED TO CHANGED IT!!!!!!!!!!!!!!!!!!!
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_images/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))
    print(""Training path: {}"".format(train_path))
    # Data Processing


    ## SHUFFLE DATA
    skf = None
    # if cfg.valid_type == 'holdout':
    test_805 = pd.read_csv('./input/test_805.csv')
    test_805 = pd.concat([test_805, test_805])
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True,
                                                                   random_state=cfg.seed, stratify=train_csv['diagnosis'])
    # train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid) # need to change It!!!!!
    # elif cfg.valid_type == 'cv':
    #     skf = StratifiedKFold(n_splits=cfg.n_folds, shuffle=True, random_state=cfg.seed)



    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model
    cfg.model.load_state_dict(
        torch.load('./Model_weights_finetuning/finetune14_like9_but_with_collorjitter_params.pth')['model'])

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()

    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                pass
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                pass
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [1e-3]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b5']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",11172.06
2019.08.31 06:30:07,new_old,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        self.model = None
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'ResNet152':
            model = models.resnet152(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet152.pth""))
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            # model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))
            # model._conv_stem = nn.Conv2d(4, 48, kernel_size=3, stride=2, bias=False)
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            # #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b6':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b6-c76e70fd.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b7':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b7-dcc49843.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            #model.cuda()
            self.model = model


",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 1e-05
)",<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb95f453c8>,10.0,28342833.0,20.0,16.0,0.4150680000000001,0.359604,0.7979,,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'data_imgsize_256'
        self.img_size = 256
        # self.img_size_crop = 300

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        # self.num_classes = 5
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 10
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP

        self.num_workers = 16
        self.n_epochs = 200
        self.batch_size = batch_size
        self.valid_type = 'holdout' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50","# from retinal_process import *
# from utils_img import *

from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config
import math

# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()
# import tensorflow as tf

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p



def subtract_median_bg_image(im):
    k = np.max(im.shape)//20*2+1
    bg = cv2.medianBlur(im, k)
    return cv2.addWeighted (im, 4, bg, -4, 128)

PARAM = 96
def Radius_Reduction(img,PARAM):
    h,w,c=img.shape
    Frame=np.zeros((h,w,c),dtype=np.uint8)
    cv2.circle(Frame,(int(math.floor(w/2)),int(math.floor(h/2))),int(math.floor((h*PARAM)/float(2*100))), (255,255,255), -1)
    Frame1=cv2.cvtColor(Frame, cv2.COLOR_BGR2GRAY)
    img1 =cv2.bitwise_and(img,img,mask=Frame1)
    return img1

def info_image(im):
    # Compute the center (cx, cy) and radius of the eye
    cy = im.shape[0]//2
    midline = im[cy,:]
    midline = np.where(midline>midline.mean()/3)[0]
    if len(midline)>im.shape[1]//2:
        x_start, x_end = np.min(midline), np.max(midline)
    else: # This actually rarely happens p~1/10000
        x_start, x_end = im.shape[1]//10, 9*im.shape[1]//10
    cx = (x_start + x_end)/2
    r = (x_end - x_start)/2
    return cx, cy, r

def resize_image(im, image_size, augmentation=False):
    # Crops, resizes and potentially augments the image to IMAGE_SIZE
    cx, cy, r = info_image(im)
    scaling = image_size/(2*r)
    rotation = 0
    if augmentation:
        scaling *= 1 + 0.3 * (np.random.rand()-0.5)
        rotation = 360 * np.random.rand()
    M = cv2.getRotationMatrix2D((cx,cy), rotation, scaling)
    M[0,2] -= cx - image_size/2
    M[1,2] -= cy - image_size/2
    return cv2.warpAffine(im,M,(image_size, image_size))

class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)
        lab_planes = cv2.split(lab)
        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(16, 16))
        lab_planes[0] = clahe.apply(lab_planes[0])
        lab = cv2.merge(lab_planes)
        clahed = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)
        image = resize_image(clahed, cfg.img_size)
        # print(tf.test.is_gpu_available())
        # vessel_model = VesselNet('./vessels/')
        # process(lab, vessel_model=vessel_model)
        # print(np.shape(image))
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-150, 150)),
        transforms.RandomVerticalFlip(),
        transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),
        # transforms.RandomResizedCrop(cfg.img_size_crop),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
        ])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","
import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedKFold

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'           #### NEED TO CHANGED IT!!!!!!!!!!!!!!!!!!!
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_images/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))
    print(""Training path: {}"".format(train_path))
    # Data Processing


    ## SHUFFLE DATA
    skf = None
    # if cfg.valid_type == 'holdout':
    test_805 = pd.read_csv('./input/test_805.csv')
    test_805 = pd.concat([test_805, test_805])
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True,
                                                                   random_state=cfg.seed, stratify=train_csv['diagnosis'])
    # train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid) # need to change It!!!!!
    # elif cfg.valid_type == 'cv':
    #     skf = StratifiedKFold(n_splits=cfg.n_folds, shuffle=True, random_state=cfg.seed)



    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model
    cfg.model.load_state_dict(
        torch.load('./Model_weights_finetuning/finetuning_15_like14_but_with_brightness_0.4.pth')['model'])

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()

    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                pass
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                pass
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [1e-3]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b5']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",12374.71
2019.08.31 13:23:42,new_old,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        self.model = None
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'ResNet152':
            model = models.resnet152(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet152.pth""))
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            # model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))
            # model._conv_stem = nn.Conv2d(4, 48, kernel_size=3, stride=2, bias=False)
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            # #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b6':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b6-c76e70fd.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b7':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b7-dcc49843.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            #model.cuda()
            self.model = model


",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 1e-05
)",<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb95f42470>,10.0,28342833.0,28.0,16.0,0.406893,0.379268,0.8079,,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'data_imgsize_256'
        self.img_size = 256
        # self.img_size_crop = 300

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        # self.num_classes = 5
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 10
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP

        self.num_workers = 16
        self.n_epochs = 200
        self.batch_size = batch_size
        self.valid_type = 'holdout' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50","# from retinal_process import *
# from utils_img import *

from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config
import math

# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()
# import tensorflow as tf

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p



def subtract_median_bg_image(im):
    k = np.max(im.shape)//20*2+1
    bg = cv2.medianBlur(im, k)
    return cv2.addWeighted (im, 4, bg, -4, 128)

PARAM = 96
def Radius_Reduction(img,PARAM):
    h,w,c=img.shape
    Frame=np.zeros((h,w,c),dtype=np.uint8)
    cv2.circle(Frame,(int(math.floor(w/2)),int(math.floor(h/2))),int(math.floor((h*PARAM)/float(2*100))), (255,255,255), -1)
    Frame1=cv2.cvtColor(Frame, cv2.COLOR_BGR2GRAY)
    img1 =cv2.bitwise_and(img,img,mask=Frame1)
    return img1

def info_image(im):
    # Compute the center (cx, cy) and radius of the eye
    cy = im.shape[0]//2
    midline = im[cy,:]
    midline = np.where(midline>midline.mean()/3)[0]
    if len(midline)>im.shape[1]//2:
        x_start, x_end = np.min(midline), np.max(midline)
    else: # This actually rarely happens p~1/10000
        x_start, x_end = im.shape[1]//10, 9*im.shape[1]//10
    cx = (x_start + x_end)/2
    r = (x_end - x_start)/2
    return cx, cy, r

def resize_image(im, image_size, augmentation=False):
    # Crops, resizes and potentially augments the image to IMAGE_SIZE
    cx, cy, r = info_image(im)
    scaling = image_size/(2*r)
    rotation = 0
    if augmentation:
        scaling *= 1 + 0.3 * (np.random.rand()-0.5)
        rotation = 360 * np.random.rand()
    M = cv2.getRotationMatrix2D((cx,cy), rotation, scaling)
    M[0,2] -= cx - image_size/2
    M[1,2] -= cy - image_size/2
    return cv2.warpAffine(im,M,(image_size, image_size))

class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)
        lab_planes = cv2.split(lab)
        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(16, 16))
        lab_planes[0] = clahe.apply(lab_planes[0])
        lab = cv2.merge(lab_planes)
        clahed = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)
        image = resize_image(clahed, cfg.img_size)
        # print(tf.test.is_gpu_available())
        # vessel_model = VesselNet('./vessels/')
        # process(lab, vessel_model=vessel_model)
        # print(np.shape(image))
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-150, 150)),
        transforms.RandomVerticalFlip(),
        transforms.ColorJitter(brightness=0.1, contrast=1, saturation=1, hue=0.1),
        # transforms.RandomResizedCrop(cfg.img_size_crop),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
        ])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","
import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedKFold

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'           #### NEED TO CHANGED IT!!!!!!!!!!!!!!!!!!!
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_images/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))
    print(""Training path: {}"".format(train_path))
    # Data Processing


    ## SHUFFLE DATA
    skf = None
    # if cfg.valid_type == 'holdout':
    test_805 = pd.read_csv('./input/test_805.csv')
    test_805 = pd.concat([test_805, test_805])
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True,
                                                                   random_state=cfg.seed, stratify=train_csv['diagnosis'])
    # train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid) # need to change It!!!!!
    # elif cfg.valid_type == 'cv':
    #     skf = StratifiedKFold(n_splits=cfg.n_folds, shuffle=True, random_state=cfg.seed)



    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model
    cfg.model.load_state_dict(
        torch.load('./Model_weights_finetuning/finetune16_like14_but_01_01_01_01_colorjitter.pth')['model'])

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()

    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                pass
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                pass
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [1e-3]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b5']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",17892.55
2019.09.01 00:22:48,new_old,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        self.model = None
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'ResNet152':
            model = models.resnet152(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet152.pth""))
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            # model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))
            # model._conv_stem = nn.Conv2d(4, 48, kernel_size=3, stride=2, bias=False)
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            # #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b6':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b6-c76e70fd.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b7':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b7-dcc49843.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            #model.cuda()
            self.model = model


",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 1e-05
)",<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb95f47358>,10.0,28342833.0,18.0,16.0,0.493762,0.406982,0.8013,,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'data_imgsize_256'
        self.img_size = 256
        # self.img_size_crop = 300

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        # self.num_classes = 5
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 10
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP

        self.num_workers = 16
        self.n_epochs = 200
        self.batch_size = batch_size
        self.valid_type = 'holdout' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50","# from retinal_process import *
# from utils_img import *

from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config
import math

# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()
# import tensorflow as tf

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p



def subtract_median_bg_image(im):
    k = np.max(im.shape)//20*2+1
    bg = cv2.medianBlur(im, k)
    return cv2.addWeighted (im, 4, bg, -4, 128)

PARAM = 96
def Radius_Reduction(img,PARAM):
    h,w,c=img.shape
    Frame=np.zeros((h,w,c),dtype=np.uint8)
    cv2.circle(Frame,(int(math.floor(w/2)),int(math.floor(h/2))),int(math.floor((h*PARAM)/float(2*100))), (255,255,255), -1)
    Frame1=cv2.cvtColor(Frame, cv2.COLOR_BGR2GRAY)
    img1 =cv2.bitwise_and(img,img,mask=Frame1)
    return img1

def info_image(im):
    # Compute the center (cx, cy) and radius of the eye
    cy = im.shape[0]//2
    midline = im[cy,:]
    midline = np.where(midline>midline.mean()/3)[0]
    if len(midline)>im.shape[1]//2:
        x_start, x_end = np.min(midline), np.max(midline)
    else: # This actually rarely happens p~1/10000
        x_start, x_end = im.shape[1]//10, 9*im.shape[1]//10
    cx = (x_start + x_end)/2
    r = (x_end - x_start)/2
    return cx, cy, r

def resize_image(im, image_size, augmentation=False):
    # Crops, resizes and potentially augments the image to IMAGE_SIZE
    cx, cy, r = info_image(im)
    scaling = image_size/(2*r)
    rotation = 0
    if augmentation:
        scaling *= 1 + 0.3 * (np.random.rand()-0.5)
        rotation = 360 * np.random.rand()
    M = cv2.getRotationMatrix2D((cx,cy), rotation, scaling)
    M[0,2] -= cx - image_size/2
    M[1,2] -= cy - image_size/2
    return cv2.warpAffine(im,M,(image_size, image_size))

class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)
        lab_planes = cv2.split(lab)
        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(16, 16))
        lab_planes[0] = clahe.apply(lab_planes[0])
        lab = cv2.merge(lab_planes)
        clahed = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)
        image = resize_image(clahed, cfg.img_size)
        # print(tf.test.is_gpu_available())
        # vessel_model = VesselNet('./vessels/')
        # process(lab, vessel_model=vessel_model)
        # print(np.shape(image))
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-180, 180)),
        transforms.RandomVerticalFlip(),
        transforms.ColorJitter(brightness=0.1, contrast=0.5, saturation=0.1, hue=0.1),
        # transforms.RandomResizedCrop(cfg.img_size_crop),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
        ])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","
import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedKFold

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'           #### NEED TO CHANGED IT!!!!!!!!!!!!!!!!!!!
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_images/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))
    print(""Training path: {}"".format(train_path))
    # Data Processing


    ## SHUFFLE DATA
    skf = None
    # if cfg.valid_type == 'holdout':
    test_805 = pd.read_csv('./input/test_805.csv')
    test_805 = pd.concat([test_805, test_805])
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True,
                                                                   random_state=cfg.seed, stratify=train_csv['diagnosis'])
    # train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid) # need to change It!!!!!
    # elif cfg.valid_type == 'cv':
    #     skf = StratifiedKFold(n_splits=cfg.n_folds, shuffle=True, random_state=cfg.seed)



    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model
    cfg.model.load_state_dict(
        torch.load('./Model_weights_finetuning/finetune16_like14_but_01_01_01_01_colorjitter.pth')['model'])

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()

    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                pass
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                pass
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [1e-3]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b5']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",11152.56
2019.09.01 11:38:45,new_old,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        self.model = None
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'ResNet152':
            model = models.resnet152(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet152.pth""))
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            # model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))
            # model._conv_stem = nn.Conv2d(4, 48, kernel_size=3, stride=2, bias=False)
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            # #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b6':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b6-c76e70fd.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b7':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b7-dcc49843.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            #model.cuda()
            self.model = model


",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 1e-05
)",<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb95f42550>,10.0,28342833.0,34.0,16.0,0.445161,0.429182,0.7895,,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'data_imgsize_256'
        self.img_size = 300
        self.img_size_crop = 256

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        # self.num_classes = 5
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 10
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP

        self.num_workers = 16
        self.n_epochs = 200
        self.batch_size = batch_size
        self.valid_type = 'holdout' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50","# from retinal_process import *
# from utils_img import *

from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config
import math

# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()
# import tensorflow as tf

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p



def subtract_median_bg_image(im):
    k = np.max(im.shape)//20*2+1
    bg = cv2.medianBlur(im, k)
    return cv2.addWeighted (im, 4, bg, -4, 128)

PARAM = 96
def Radius_Reduction(img,PARAM):
    h,w,c=img.shape
    Frame=np.zeros((h,w,c),dtype=np.uint8)
    cv2.circle(Frame,(int(math.floor(w/2)),int(math.floor(h/2))),int(math.floor((h*PARAM)/float(2*100))), (255,255,255), -1)
    Frame1=cv2.cvtColor(Frame, cv2.COLOR_BGR2GRAY)
    img1 =cv2.bitwise_and(img,img,mask=Frame1)
    return img1

def info_image(im):
    # Compute the center (cx, cy) and radius of the eye
    cy = im.shape[0]//2
    midline = im[cy,:]
    midline = np.where(midline>midline.mean()/3)[0]
    if len(midline)>im.shape[1]//2:
        x_start, x_end = np.min(midline), np.max(midline)
    else: # This actually rarely happens p~1/10000
        x_start, x_end = im.shape[1]//10, 9*im.shape[1]//10
    cx = (x_start + x_end)/2
    r = (x_end - x_start)/2
    return cx, cy, r

def resize_image(im, image_size, augmentation=False):
    # Crops, resizes and potentially augments the image to IMAGE_SIZE
    cx, cy, r = info_image(im)
    scaling = image_size/(2*r)
    rotation = 0
    if augmentation:
        scaling *= 1 + 0.3 * (np.random.rand()-0.5)
        rotation = 360 * np.random.rand()
    M = cv2.getRotationMatrix2D((cx,cy), rotation, scaling)
    M[0,2] -= cx - image_size/2
    M[1,2] -= cy - image_size/2
    return cv2.warpAffine(im,M,(image_size, image_size))

class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)
        lab_planes = cv2.split(lab)
        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(16, 16))
        lab_planes[0] = clahe.apply(lab_planes[0])
        lab = cv2.merge(lab_planes)
        clahed = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)
        image = resize_image(clahed, cfg.img_size)
        # print(tf.test.is_gpu_available())
        # vessel_model = VesselNet('./vessels/')
        # process(lab, vessel_model=vessel_model)
        # print(np.shape(image))
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-180, 180)),
        transforms.RandomVerticalFlip(),
        transforms.ColorJitter(brightness=0.2, contrast=0.5, saturation=0.5, hue=0.1),
        transforms.RandomResizedCrop(cfg.img_size_crop),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
        ])

transforms_valid = transforms.Compose([
        transforms.RandomResizedCrop(cfg.img_size_crop),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","
import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedKFold

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'           #### NEED TO CHANGED IT!!!!!!!!!!!!!!!!!!!
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_images/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))
    print(""Training path: {}"".format(train_path))
    # Data Processing


    ## SHUFFLE DATA
    skf = None
    # if cfg.valid_type == 'holdout':
    test_805 = pd.read_csv('./input/test_805.csv')
    test_805 = pd.concat([test_805, test_805])
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True,
                                                                   random_state=cfg.seed, stratify=train_csv['diagnosis'])
    # train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid) # need to change It!!!!!
    # elif cfg.valid_type == 'cv':
    #     skf = StratifiedKFold(n_splits=cfg.n_folds, shuffle=True, random_state=cfg.seed)



    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model
    cfg.model.load_state_dict(
        torch.load('./Model_weights_finetuning/finetune16_like14_but_01_01_01_01_colorjitter.pth')['model'])

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()

    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                pass
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                pass
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [1e-3]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b5']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",20825.01
2019.09.02 07:26:01,new_old,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        self.model = None
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'ResNet152':
            model = models.resnet152(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet152.pth""))
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            # model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))
            # model._conv_stem = nn.Conv2d(4, 48, kernel_size=3, stride=2, bias=False)
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            # #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b6':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b6-c76e70fd.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b7':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b7-dcc49843.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            #model.cuda()
            self.model = model


",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 1e-05
)",<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb95ffce48>,10.0,42502209.0,200.0,16.0,0.976137,1.937571,0.0111,,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'data_imgsize_256'
        self.img_size = 224
        # self.img_size_crop = 256

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        # self.num_classes = 5
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 10
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP

        self.num_workers = 16
        self.n_epochs = 200
        self.batch_size = batch_size
        self.valid_type = 'holdout' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50","# from retinal_process import *
# from utils_img import *

from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config
import math

# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()
# import tensorflow as tf

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p



def subtract_median_bg_image(im):
    k = np.max(im.shape)//20*2+1
    bg = cv2.medianBlur(im, k)
    return cv2.addWeighted (im, 4, bg, -4, 128)

PARAM = 96
def Radius_Reduction(img,PARAM):
    h,w,c=img.shape
    Frame=np.zeros((h,w,c),dtype=np.uint8)
    cv2.circle(Frame,(int(math.floor(w/2)),int(math.floor(h/2))),int(math.floor((h*PARAM)/float(2*100))), (255,255,255), -1)
    Frame1=cv2.cvtColor(Frame, cv2.COLOR_BGR2GRAY)
    img1 =cv2.bitwise_and(img,img,mask=Frame1)
    return img1

def info_image(im):
    # Compute the center (cx, cy) and radius of the eye
    cy = im.shape[0]//2
    midline = im[cy,:]
    midline = np.where(midline>midline.mean()/3)[0]
    if len(midline)>im.shape[1]//2:
        x_start, x_end = np.min(midline), np.max(midline)
    else: # This actually rarely happens p~1/10000
        x_start, x_end = im.shape[1]//10, 9*im.shape[1]//10
    cx = (x_start + x_end)/2
    r = (x_end - x_start)/2
    return cx, cy, r

def resize_image(im, image_size, augmentation=False):
    # Crops, resizes and potentially augments the image to IMAGE_SIZE
    cx, cy, r = info_image(im)
    scaling = image_size/(2*r)
    rotation = 0
    if augmentation:
        scaling *= 1 + 0.3 * (np.random.rand()-0.5)
        rotation = 360 * np.random.rand()
    M = cv2.getRotationMatrix2D((cx,cy), rotation, scaling)
    M[0,2] -= cx - image_size/2
    M[1,2] -= cy - image_size/2
    return cv2.warpAffine(im,M,(image_size, image_size))

class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)
        lab_planes = cv2.split(lab)
        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(16, 16))
        lab_planes[0] = clahe.apply(lab_planes[0])
        lab = cv2.merge(lab_planes)
        clahed = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)
        image = resize_image(clahed, cfg.img_size)
        # print(tf.test.is_gpu_available())
        # vessel_model = VesselNet('./vessels/')
        # process(lab, vessel_model=vessel_model)
        # print(np.shape(image))
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-180, 180)),
        transforms.RandomVerticalFlip(),
        transforms.ColorJitter(brightness=0.2, contrast=0.5, saturation=0.5, hue=0.1),
        # transforms.RandomResizedCrop(cfg.img_size_crop),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
        ])

transforms_valid = transforms.Compose([
        # transforms.RandomResizedCrop(cfg.img_size_crop),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","
import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedKFold

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'           #### NEED TO CHANGED IT!!!!!!!!!!!!!!!!!!!
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_images/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))
    print(""Training path: {}"".format(train_path))
    # Data Processing


    ## SHUFFLE DATA
    skf = None
    # if cfg.valid_type == 'holdout':
    test_805 = pd.read_csv('./input/test_805.csv')
    test_805 = pd.concat([test_805, test_805])
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True,
                                                                   random_state=cfg.seed, stratify=train_csv['diagnosis'])
    # train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid) # need to change It!!!!!
    # elif cfg.valid_type == 'cv':
    #     skf = StratifiedKFold(n_splits=cfg.n_folds, shuffle=True, random_state=cfg.seed)



    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model
    cfg.model.load_state_dict(
        torch.load('./Model_weights_finetuning/finetune16_like14_but_01_01_01_01_colorjitter.pth')['model'])

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()

    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                pass
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                pass
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [1e-3]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b5']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",
2019.09.02 07:50:38,new_old,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        self.model = None
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                # nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'ResNet152':
            model = models.resnet152(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet152.pth""))
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            # model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))
            # model._conv_stem = nn.Conv2d(4, 48, kernel_size=3, stride=2, bias=False)
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            # #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b6':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b6-c76e70fd.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b7':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b7-dcc49843.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            #model.cuda()
            self.model = model


",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 1e-05
)",<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb95ff9b38>,10.0,42502209.0,200.0,16.0,0.967711,1.931307,-0.0001,,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'data_imgsize_256'
        self.img_size = 224
        # self.img_size_crop = 256

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        # self.num_classes = 5
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 10
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP

        self.num_workers = 16
        self.n_epochs = 200
        self.batch_size = batch_size
        self.valid_type = 'holdout' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50","# from retinal_process import *
# from utils_img import *

from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config
import math

# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()
# import tensorflow as tf

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p



def subtract_median_bg_image(im):
    k = np.max(im.shape)//20*2+1
    bg = cv2.medianBlur(im, k)
    return cv2.addWeighted (im, 4, bg, -4, 128)

PARAM = 96
def Radius_Reduction(img,PARAM):
    h,w,c=img.shape
    Frame=np.zeros((h,w,c),dtype=np.uint8)
    cv2.circle(Frame,(int(math.floor(w/2)),int(math.floor(h/2))),int(math.floor((h*PARAM)/float(2*100))), (255,255,255), -1)
    Frame1=cv2.cvtColor(Frame, cv2.COLOR_BGR2GRAY)
    img1 =cv2.bitwise_and(img,img,mask=Frame1)
    return img1

def info_image(im):
    # Compute the center (cx, cy) and radius of the eye
    cy = im.shape[0]//2
    midline = im[cy,:]
    midline = np.where(midline>midline.mean()/3)[0]
    if len(midline)>im.shape[1]//2:
        x_start, x_end = np.min(midline), np.max(midline)
    else: # This actually rarely happens p~1/10000
        x_start, x_end = im.shape[1]//10, 9*im.shape[1]//10
    cx = (x_start + x_end)/2
    r = (x_end - x_start)/2
    return cx, cy, r

def resize_image(im, image_size, augmentation=False):
    # Crops, resizes and potentially augments the image to IMAGE_SIZE
    cx, cy, r = info_image(im)
    scaling = image_size/(2*r)
    rotation = 0
    if augmentation:
        scaling *= 1 + 0.3 * (np.random.rand()-0.5)
        rotation = 360 * np.random.rand()
    M = cv2.getRotationMatrix2D((cx,cy), rotation, scaling)
    M[0,2] -= cx - image_size/2
    M[1,2] -= cy - image_size/2
    return cv2.warpAffine(im,M,(image_size, image_size))

class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)
        lab_planes = cv2.split(lab)
        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(16, 16))
        lab_planes[0] = clahe.apply(lab_planes[0])
        lab = cv2.merge(lab_planes)
        clahed = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)
        image = resize_image(clahed, cfg.img_size)
        # print(tf.test.is_gpu_available())
        # vessel_model = VesselNet('./vessels/')
        # process(lab, vessel_model=vessel_model)
        # print(np.shape(image))
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-180, 180)),
        transforms.RandomVerticalFlip(),
        transforms.ColorJitter(brightness=0.2, contrast=0.5, saturation=0.5, hue=0.1),
        # transforms.RandomResizedCrop(cfg.img_size_crop),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
        ])

transforms_valid = transforms.Compose([
        # transforms.RandomResizedCrop(cfg.img_size_crop),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","
import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedKFold

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'           #### NEED TO CHANGED IT!!!!!!!!!!!!!!!!!!!
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_images/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))
    print(""Training path: {}"".format(train_path))
    # Data Processing


    ## SHUFFLE DATA
    skf = None
    # if cfg.valid_type == 'holdout':
    test_805 = pd.read_csv('./input/test_805.csv')
    test_805 = pd.concat([test_805, test_805])
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True,
                                                                   random_state=cfg.seed, stratify=train_csv['diagnosis'])
    # train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid) # need to change It!!!!!
    # elif cfg.valid_type == 'cv':
    #     skf = StratifiedKFold(n_splits=cfg.n_folds, shuffle=True, random_state=cfg.seed)



    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model
    cfg.model.load_state_dict(
        torch.load('./Model_weights_finetuning/finetune16_like14_but_01_01_01_01_colorjitter.pth')['model'])

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()

    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                pass
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                pass
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [1e-3]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b5']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",
2019.09.02 07:57:36,new_old,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        self.model = None
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            # model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            #     param.requires_grad = False
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                # nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'ResNet152':
            model = models.resnet152(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet152.pth""))
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            # model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))
            # model._conv_stem = nn.Conv2d(4, 48, kernel_size=3, stride=2, bias=False)
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            # #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b6':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b6-c76e70fd.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b7':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b7-dcc49843.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            #model.cuda()
            self.model = model


",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 1e-05
)",<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb9602b828>,10.0,42502209.0,18.0,16.0,0.939368,1.9351639999999999,-0.0004,,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'data_imgsize_256'
        self.img_size = 224
        # self.img_size_crop = 256

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        # self.num_classes = 5
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 10
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP

        self.num_workers = 16
        self.n_epochs = 200
        self.batch_size = batch_size
        self.valid_type = 'holdout' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50","# from retinal_process import *
# from utils_img import *

from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config
import math

# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()
# import tensorflow as tf

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p



def subtract_median_bg_image(im):
    k = np.max(im.shape)//20*2+1
    bg = cv2.medianBlur(im, k)
    return cv2.addWeighted (im, 4, bg, -4, 128)

PARAM = 96
def Radius_Reduction(img,PARAM):
    h,w,c=img.shape
    Frame=np.zeros((h,w,c),dtype=np.uint8)
    cv2.circle(Frame,(int(math.floor(w/2)),int(math.floor(h/2))),int(math.floor((h*PARAM)/float(2*100))), (255,255,255), -1)
    Frame1=cv2.cvtColor(Frame, cv2.COLOR_BGR2GRAY)
    img1 =cv2.bitwise_and(img,img,mask=Frame1)
    return img1

def info_image(im):
    # Compute the center (cx, cy) and radius of the eye
    cy = im.shape[0]//2
    midline = im[cy,:]
    midline = np.where(midline>midline.mean()/3)[0]
    if len(midline)>im.shape[1]//2:
        x_start, x_end = np.min(midline), np.max(midline)
    else: # This actually rarely happens p~1/10000
        x_start, x_end = im.shape[1]//10, 9*im.shape[1]//10
    cx = (x_start + x_end)/2
    r = (x_end - x_start)/2
    return cx, cy, r

def resize_image(im, image_size, augmentation=False):
    # Crops, resizes and potentially augments the image to IMAGE_SIZE
    cx, cy, r = info_image(im)
    scaling = image_size/(2*r)
    rotation = 0
    if augmentation:
        scaling *= 1 + 0.3 * (np.random.rand()-0.5)
        rotation = 360 * np.random.rand()
    M = cv2.getRotationMatrix2D((cx,cy), rotation, scaling)
    M[0,2] -= cx - image_size/2
    M[1,2] -= cy - image_size/2
    return cv2.warpAffine(im,M,(image_size, image_size))

class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)
        lab_planes = cv2.split(lab)
        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(16, 16))
        lab_planes[0] = clahe.apply(lab_planes[0])
        lab = cv2.merge(lab_planes)
        clahed = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)
        image = resize_image(clahed, cfg.img_size)
        # print(tf.test.is_gpu_available())
        # vessel_model = VesselNet('./vessels/')
        # process(lab, vessel_model=vessel_model)
        # print(np.shape(image))
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-180, 180)),
        transforms.RandomVerticalFlip(),
        transforms.ColorJitter(brightness=0.2, contrast=0.5, saturation=0.5, hue=0.1),
        # transforms.RandomResizedCrop(cfg.img_size_crop),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
        ])

transforms_valid = transforms.Compose([
        # transforms.RandomResizedCrop(cfg.img_size_crop),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","
import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedKFold

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'           #### NEED TO CHANGED IT!!!!!!!!!!!!!!!!!!!
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_images/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))
    print(""Training path: {}"".format(train_path))
    # Data Processing


    ## SHUFFLE DATA
    skf = None
    # if cfg.valid_type == 'holdout':
    test_805 = pd.read_csv('./input/test_805.csv')
    test_805 = pd.concat([test_805, test_805])
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True,
                                                                   random_state=cfg.seed, stratify=train_csv['diagnosis'])
    # train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid) # need to change It!!!!!
    # elif cfg.valid_type == 'cv':
    #     skf = StratifiedKFold(n_splits=cfg.n_folds, shuffle=True, random_state=cfg.seed)



    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model
    cfg.model.load_state_dict(
        torch.load('./Model_weights_finetuning/finetune16_like14_but_01_01_01_01_colorjitter.pth')['model'])

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()

    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                pass
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                pass
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [1e-3]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b5']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",6573.38
2019.09.02 21:46:59,new_old,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        self.model = None
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            # model.conv1 = nn.Conv2d(4, 64, (7, 7), (2, 2), (3, 3), bias=False)
            #     param.requires_grad = False
            print(model)
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                # nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'ResNet152':
            model = models.resnet152(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet152.pth""))
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            # model._conv_stem = nn.Conv2d(4, 32, kernel_size=3, stride=2, bias=False)
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))
            model._conv_stem = nn.Conv2d(4, 48, kernel_size=3, stride=2, bias=False)
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            # #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b6':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b6-c76e70fd.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b7':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b7-dcc49843.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            #model.cuda()
            self.model = model


",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 1e-05
)",<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb51706e10>,10.0,28343265.0,34.0,16.0,0.351347,0.372375,0.8149,,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'data_imgsize_256'
        self.img_size = 256
        # self.img_size_crop = 300

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        # self.num_classes = 5
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 10
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP

        self.num_workers = 16
        self.n_epochs = 200
        self.batch_size = batch_size
        self.valid_type = 'holdout' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50","from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config
import math

from utils_img import *
from retinal_process import *

# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()
# import tensorflow as tf

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p



def subtract_median_bg_image(im):
    k = np.max(im.shape)//20*2+1
    bg = cv2.medianBlur(im, k)
    return cv2.addWeighted (im, 4, bg, -4, 128)

PARAM = 96
def Radius_Reduction(img,PARAM):
    h,w,c=img.shape
    Frame=np.zeros((h,w,c),dtype=np.uint8)
    cv2.circle(Frame,(int(math.floor(w/2)),int(math.floor(h/2))),int(math.floor((h*PARAM)/float(2*100))), (255,255,255), -1)
    Frame1=cv2.cvtColor(Frame, cv2.COLOR_BGR2GRAY)
    img1 =cv2.bitwise_and(img,img,mask=Frame1)
    return img1

def info_image(im):
    # Compute the center (cx, cy) and radius of the eye
    cy = im.shape[0]//2
    midline = im[cy,:]
    midline = np.where(midline>midline.mean()/3)[0]
    if len(midline)>im.shape[1]//2:
        x_start, x_end = np.min(midline), np.max(midline)
    else: # This actually rarely happens p~1/10000
        x_start, x_end = im.shape[1]//10, 9*im.shape[1]//10
    cx = (x_start + x_end)/2
    r = (x_end - x_start)/2
    return cx, cy, r

def resize_image(im, image_size, augmentation=False):
    # Crops, resizes and potentially augments the image to IMAGE_SIZE
    cx, cy, r = info_image(im)
    scaling = image_size/(2*r)
    rotation = 0
    if augmentation:
        scaling *= 1 + 0.3 * (np.random.rand()-0.5)
        rotation = 360 * np.random.rand()
    M = cv2.getRotationMatrix2D((cx,cy), rotation, scaling)
    M[0,2] -= cx - image_size/2
    M[1,2] -= cy - image_size/2
    return cv2.warpAffine(im,M,(image_size, image_size))

class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir
        self.number = 0

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)
        # lab_planes = cv2.split(lab)
        # clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(16, 16))
        # lab_planes[0] = clahe.apply(lab_planes[0])
        # lab = cv2.merge(lab_planes)
        # clahed = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)
        # image = resize_image(clahed, cfg.img_size)
        # print(tf.test.is_gpu_available())
        # vessel_model = VesselNet('./vessels/')
        image = process(lab, opencv_veins=True)

        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        # transforms.RandomRotation((-120, 120)),
        transforms.RandomVerticalFlip(),
        transforms.ColorJitter(brightness=0.1, contrast=0.5, saturation=0.1, hue=0.1),
        # transforms.RandomResizedCrop(cfg.img_size_crop),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406, 0.5], [0.229, 0.224, 0.225, 0.5]),
        ])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406, 0.5], [0.229, 0.224, 0.225, 0.5])
    ])","
import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedKFold

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'           #### NEED TO CHANGED IT!!!!!!!!!!!!!!!!!!!
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_images/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))
    print(""Training path: {}"".format(train_path))
    # Data Processing


    ## SHUFFLE DATA
    skf = None
    # if cfg.valid_type == 'holdout':
    test_805 = pd.read_csv('./input/test_805.csv')
    test_805 = pd.concat([test_805, test_805])
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True,
                                                                   random_state=cfg.seed, stratify=train_csv['diagnosis'])
    # train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid) # need to change It!!!!!
    # elif cfg.valid_type == 'cv':
    #     skf = StratifiedKFold(n_splits=cfg.n_folds, shuffle=True, random_state=cfg.seed)



    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model
    # cfg.model.load_state_dict(
    #     torch.load('./Model_weights_finetuning/finetune18_like17_but_with_diffcoefs_colorjitter.pth')['model'])

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()

    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                pass
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                pass
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [1e-3]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b5']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",21078.8
2019.09.03 07:21:35,new_old,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        self.model = None
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            # model.conv1 = nn.Conv2d(4, 64, (7, 7), (2, 2), (3, 3), bias=False)
            #     param.requires_grad = False
            print(model)
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                # nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'ResNet152':
            model = models.resnet152(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet152.pth""))
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            # model._conv_stem = nn.Conv2d(4, 32, kernel_size=3, stride=2, bias=False)
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))
            model._conv_stem = nn.Conv2d(4, 48, kernel_size=3, stride=2, bias=False)
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            # #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b6':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b6-c76e70fd.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b7':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b7-dcc49843.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            #model.cuda()
            self.model = model


",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 1e-05
)",<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb51707e10>,10.0,28343265.0,18.0,16.0,0.471286,0.392532,0.8,,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'data_imgsize_256'
        self.img_size = 256
        # self.img_size_crop = 300

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        # self.num_classes = 5
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 10
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP

        self.num_workers = 16
        self.n_epochs = 200
        self.batch_size = batch_size
        self.valid_type = 'holdout' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50","from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config
import math

from utils_img import *
from retinal_process import *

# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()
# import tensorflow as tf

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p



def subtract_median_bg_image(im):
    k = np.max(im.shape)//20*2+1
    bg = cv2.medianBlur(im, k)
    return cv2.addWeighted (im, 4, bg, -4, 128)

PARAM = 96
def Radius_Reduction(img,PARAM):
    h,w,c=img.shape
    Frame=np.zeros((h,w,c),dtype=np.uint8)
    cv2.circle(Frame,(int(math.floor(w/2)),int(math.floor(h/2))),int(math.floor((h*PARAM)/float(2*100))), (255,255,255), -1)
    Frame1=cv2.cvtColor(Frame, cv2.COLOR_BGR2GRAY)
    img1 =cv2.bitwise_and(img,img,mask=Frame1)
    return img1

def info_image(im):
    # Compute the center (cx, cy) and radius of the eye
    cy = im.shape[0]//2
    midline = im[cy,:]
    midline = np.where(midline>midline.mean()/3)[0]
    if len(midline)>im.shape[1]//2:
        x_start, x_end = np.min(midline), np.max(midline)
    else: # This actually rarely happens p~1/10000
        x_start, x_end = im.shape[1]//10, 9*im.shape[1]//10
    cx = (x_start + x_end)/2
    r = (x_end - x_start)/2
    return cx, cy, r

def resize_image(im, image_size, augmentation=False):
    # Crops, resizes and potentially augments the image to IMAGE_SIZE
    cx, cy, r = info_image(im)
    scaling = image_size/(2*r)
    rotation = 0
    if augmentation:
        scaling *= 1 + 0.3 * (np.random.rand()-0.5)
        rotation = 360 * np.random.rand()
    M = cv2.getRotationMatrix2D((cx,cy), rotation, scaling)
    M[0,2] -= cx - image_size/2
    M[1,2] -= cy - image_size/2
    return cv2.warpAffine(im,M,(image_size, image_size))

class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir
        self.number = 0

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)
        # lab_planes = cv2.split(lab)
        # clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(16, 16))
        # lab_planes[0] = clahe.apply(lab_planes[0])
        # lab = cv2.merge(lab_planes)
        # clahed = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)
        # image = resize_image(clahed, cfg.img_size)
        # print(tf.test.is_gpu_available())
        # vessel_model = VesselNet('./vessels/')
        image = process(lab, crop='circle', opencv_veins=True)

        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        # transforms.RandomRotation((-120, 120)),
        transforms.RandomVerticalFlip(),
        transforms.ColorJitter(brightness=0.1, contrast=0.5, saturation=0.1, hue=0.1),
        # transforms.RandomResizedCrop(cfg.img_size_crop),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406, 0.5], [0.229, 0.224, 0.225, 0.5]),
        ])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406, 0.5], [0.229, 0.224, 0.225, 0.5])
    ])","
import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedKFold

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'           #### NEED TO CHANGED IT!!!!!!!!!!!!!!!!!!!
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_images/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))
    print(""Training path: {}"".format(train_path))
    # Data Processing


    ## SHUFFLE DATA
    skf = None
    # if cfg.valid_type == 'holdout':
    test_805 = pd.read_csv('./input/test_805.csv')
    test_805 = pd.concat([test_805, test_805])
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True,
                                                                   random_state=cfg.seed, stratify=train_csv['diagnosis'])
    # train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid) # need to change It!!!!!
    # elif cfg.valid_type == 'cv':
    #     skf = StratifiedKFold(n_splits=cfg.n_folds, shuffle=True, random_state=cfg.seed)



    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model
    cfg.model.load_state_dict(
        torch.load('./Model_weights/exp176_end_epoch26.pth')['model'])

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()

    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                pass
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                pass
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [1e-3]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b5']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",12618.84
2019.09.03 11:26:24,new_old,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        self.model = None
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            # model.conv1 = nn.Conv2d(4, 64, (7, 7), (2, 2), (3, 3), bias=False)
            #     param.requires_grad = False
            print(model)
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                # nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'ResNet152':
            model = models.resnet152(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet152.pth""))
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            # model._conv_stem = nn.Conv2d(4, 32, kernel_size=3, stride=2, bias=False)
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))
            model._conv_stem = nn.Conv2d(4, 48, kernel_size=3, stride=2, bias=False)
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            # #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b6':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b6-c76e70fd.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b7':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b7-dcc49843.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            #model.cuda()
            self.model = model


",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 1e-05
)",<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb51703eb8>,10.0,28343265.0,200.0,16.0,0.62837,0.503087,0.7696,,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'data_imgsize_256'
        self.img_size = 256
        # self.img_size_crop = 300

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        # self.num_classes = 5
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 10
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP

        self.num_workers = 16
        self.n_epochs = 200
        self.batch_size = batch_size
        self.valid_type = 'holdout' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50","from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config
import math

from utils_img import *
from retinal_process import *

# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()
# import tensorflow as tf

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p



def subtract_median_bg_image(im):
    k = np.max(im.shape)//20*2+1
    bg = cv2.medianBlur(im, k)
    return cv2.addWeighted (im, 4, bg, -4, 128)

PARAM = 96
def Radius_Reduction(img,PARAM):
    h,w,c=img.shape
    Frame=np.zeros((h,w,c),dtype=np.uint8)
    cv2.circle(Frame,(int(math.floor(w/2)),int(math.floor(h/2))),int(math.floor((h*PARAM)/float(2*100))), (255,255,255), -1)
    Frame1=cv2.cvtColor(Frame, cv2.COLOR_BGR2GRAY)
    img1 =cv2.bitwise_and(img,img,mask=Frame1)
    return img1

def info_image(im):
    # Compute the center (cx, cy) and radius of the eye
    cy = im.shape[0]//2
    midline = im[cy,:]
    midline = np.where(midline>midline.mean()/3)[0]
    if len(midline)>im.shape[1]//2:
        x_start, x_end = np.min(midline), np.max(midline)
    else: # This actually rarely happens p~1/10000
        x_start, x_end = im.shape[1]//10, 9*im.shape[1]//10
    cx = (x_start + x_end)/2
    r = (x_end - x_start)/2
    return cx, cy, r

def resize_image(im, image_size, augmentation=False):
    # Crops, resizes and potentially augments the image to IMAGE_SIZE
    cx, cy, r = info_image(im)
    scaling = image_size/(2*r)
    rotation = 0
    if augmentation:
        scaling *= 1 + 0.3 * (np.random.rand()-0.5)
        rotation = 360 * np.random.rand()
    M = cv2.getRotationMatrix2D((cx,cy), rotation, scaling)
    M[0,2] -= cx - image_size/2
    M[1,2] -= cy - image_size/2
    return cv2.warpAffine(im,M,(image_size, image_size))

class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir
        self.number = 0

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)
        # lab_planes = cv2.split(lab)
        # clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(16, 16))
        # lab_planes[0] = clahe.apply(lab_planes[0])
        # lab = cv2.merge(lab_planes)
        # clahed = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)
        # image = resize_image(clahed, cfg.img_size)
        # print(tf.test.is_gpu_available())
        # vessel_model = VesselNet('./vessels/')
        image = process(lab, crop='circle', opencv_veins=True)

        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        # transforms.RandomRotation((-120, 120)),
        transforms.RandomVerticalFlip(),
        transforms.ColorJitter(brightness=0.1, contrast=0.5, saturation=0.1, hue=0.1),
        # transforms.RandomResizedCrop(cfg.img_size_crop),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406, 0.5], [0.229, 0.224, 0.225, 0.5]),
        ])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406, 0.5], [0.229, 0.224, 0.225, 0.5])
    ])","
import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedKFold

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'           #### NEED TO CHANGED IT!!!!!!!!!!!!!!!!!!!
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_images/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))
    print(""Training path: {}"".format(train_path))
    # Data Processing


    ## SHUFFLE DATA
    skf = None
    # if cfg.valid_type == 'holdout':
    test_805 = pd.read_csv('./input/test_805.csv')
    test_805 = pd.concat([test_805, test_805])
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True,
                                                                   random_state=cfg.seed, stratify=train_csv['diagnosis'])
    # train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid) # need to change It!!!!!
    # elif cfg.valid_type == 'cv':
    #     skf = StratifiedKFold(n_splits=cfg.n_folds, shuffle=True, random_state=cfg.seed)



    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model
    cfg.model.load_state_dict(
        torch.load('./Model_weights_finetuning/finetune21_like_20_with_cycliccrop.pth')['model'])

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()

    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                pass
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                pass
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [1e-3]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b5']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",
2019.09.03 12:52:17,new_old,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models


class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        self.model = None
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            # model.conv1 = nn.Conv2d(4, 64, (7, 7), (2, 2), (3, 3), bias=False)
            #     param.requires_grad = False
            print(model)
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                # nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'ResNet152':
            model = models.resnet152(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet152.pth""))
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            # model._conv_stem = nn.Conv2d(4, 32, kernel_size=3, stride=2, bias=False)
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))
            model._conv_stem = nn.Conv2d(4, 48, kernel_size=3, stride=2, bias=False)
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            # #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b6':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b6-c76e70fd.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b7':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b7-dcc49843.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            #model.cuda()
            self.model = model


",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 1e-05
)",<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb51707e10>,10.0,28343265.0,29.0,16.0,0.331631,0.413575,0.7916,,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'data_imgsize_256'
        self.img_size = 256
        # self.img_size_crop = 300

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()
        # self.num_classes = 5
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 10
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP

        self.num_workers = 16
        self.n_epochs = 200
        self.batch_size = batch_size
        self.valid_type = 'holdout' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50","from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config
import math

from utils_img import *
from retinal_process import *

# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()
# import tensorflow as tf

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p



def subtract_median_bg_image(im):
    k = np.max(im.shape)//20*2+1
    bg = cv2.medianBlur(im, k)
    return cv2.addWeighted (im, 4, bg, -4, 128)

PARAM = 96
def Radius_Reduction(img,PARAM):
    h,w,c=img.shape
    Frame=np.zeros((h,w,c),dtype=np.uint8)
    cv2.circle(Frame,(int(math.floor(w/2)),int(math.floor(h/2))),int(math.floor((h*PARAM)/float(2*100))), (255,255,255), -1)
    Frame1=cv2.cvtColor(Frame, cv2.COLOR_BGR2GRAY)
    img1 =cv2.bitwise_and(img,img,mask=Frame1)
    return img1

def info_image(im):
    # Compute the center (cx, cy) and radius of the eye
    cy = im.shape[0]//2
    midline = im[cy,:]
    midline = np.where(midline>midline.mean()/3)[0]
    if len(midline)>im.shape[1]//2:
        x_start, x_end = np.min(midline), np.max(midline)
    else: # This actually rarely happens p~1/10000
        x_start, x_end = im.shape[1]//10, 9*im.shape[1]//10
    cx = (x_start + x_end)/2
    r = (x_end - x_start)/2
    return cx, cy, r

def resize_image(im, image_size, augmentation=False):
    # Crops, resizes and potentially augments the image to IMAGE_SIZE
    cx, cy, r = info_image(im)
    scaling = image_size/(2*r)
    rotation = 0
    if augmentation:
        scaling *= 1 + 0.3 * (np.random.rand()-0.5)
        rotation = 360 * np.random.rand()
    M = cv2.getRotationMatrix2D((cx,cy), rotation, scaling)
    M[0,2] -= cx - image_size/2
    M[1,2] -= cy - image_size/2
    return cv2.warpAffine(im,M,(image_size, image_size))

class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir
        self.number = 0

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)
        # lab_planes = cv2.split(lab)
        # clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(16, 16))
        # lab_planes[0] = clahe.apply(lab_planes[0])
        # lab = cv2.merge(lab_planes)
        # clahed = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)
        # image = resize_image(clahed, cfg.img_size)
        # print(tf.test.is_gpu_available())
        # vessel_model = VesselNet('./vessels/')
        image = process(lab, crop='normal', preprocessing='ben',opencv_veins=True)

        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        # transforms.RandomRotation((-120, 120)),
        transforms.RandomVerticalFlip(),
        transforms.ColorJitter(brightness=0.1, contrast=0.5, saturation=0.1, hue=0.1),
        # transforms.RandomResizedCrop(cfg.img_size_crop),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406, 0.5], [0.229, 0.224, 0.225, 0.5]),
        ])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406, 0.5], [0.229, 0.224, 0.225, 0.5])
    ])","
import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedKFold

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'           #### NEED TO CHANGED IT!!!!!!!!!!!!!!!!!!!
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_images/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))
    print(""Training path: {}"".format(train_path))
    # Data Processing


    ## SHUFFLE DATA
    skf = None
    # if cfg.valid_type == 'holdout':
    test_805 = pd.read_csv('./input/test_805.csv')
    test_805 = pd.concat([test_805, test_805])
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True,
                                                                   random_state=cfg.seed, stratify=train_csv['diagnosis'])
    # train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid) # need to change It!!!!!
    # elif cfg.valid_type == 'cv':
    #     skf = StratifiedKFold(n_splits=cfg.n_folds, shuffle=True, random_state=cfg.seed)



    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model
    cfg.model.load_state_dict(
        torch.load('./Model_weights_finetuning/finetune21_like_20_with_cycliccrop.pth')['model'])

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()

    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                pass
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                pass
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [1e-3]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b5']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",19304.7
2019.09.04 00:54:55,new_old,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models
from torchvision.models import resnext101_32x8d as resnext

class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        self.model = None
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            # model.conv1 = nn.Conv2d(4, 64, (7, 7), (2, 2), (3, 3), bias=False)
            #     param.requires_grad = False
            print(model)
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                # nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'ResNet152':
            model = models.resnet152(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet152.pth""))
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model
        elif model_type == 'ResNext101':
            model = resnext(pretrained=True)
            model.fc = nn.Linear(in_features=2048, out_features=5)
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            # model._conv_stem = nn.Conv2d(4, 32, kernel_size=3, stride=2, bias=False)
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))
            # model._conv_stem = nn.Conv2d(4, 48, kernel_size=3, stride=2, bias=False)
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            # #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b6':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b6-c76e70fd.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b7':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b7-dcc49843.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            #model.cuda()
            self.model = model


",AdaptiveLossFunction(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 1e-05
)",<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb50694cf8>,10.0,28342833.0,22.0,16.0,1.387932,1.353946,0.6447,,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np
from robust_loss_pytorch import AdaptiveLossFunction

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'data_imgsize_256'
        self.img_size = 256
        # self.img_size_crop = 300

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = AdaptiveLossFunction(num_dims=1, float_dtype=np.float32, device='cuda:0')
        # self.num_classes = 5
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 10
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP

        self.num_workers = 16
        self.n_epochs = 200
        self.batch_size = batch_size
        self.valid_type = 'holdout' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50","from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config
import math

from utils_img import *
from retinal_process import *

# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()
# import tensorflow as tf

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p



def subtract_median_bg_image(im):
    k = np.max(im.shape)//20*2+1
    bg = cv2.medianBlur(im, k)
    return cv2.addWeighted (im, 4, bg, -4, 128)

PARAM = 96
def Radius_Reduction(img,PARAM):
    h,w,c=img.shape
    Frame=np.zeros((h,w,c),dtype=np.uint8)
    cv2.circle(Frame,(int(math.floor(w/2)),int(math.floor(h/2))),int(math.floor((h*PARAM)/float(2*100))), (255,255,255), -1)
    Frame1=cv2.cvtColor(Frame, cv2.COLOR_BGR2GRAY)
    img1 =cv2.bitwise_and(img,img,mask=Frame1)
    return img1

def info_image(im):
    # Compute the center (cx, cy) and radius of the eye
    cy = im.shape[0]//2
    midline = im[cy,:]
    midline = np.where(midline>midline.mean()/3)[0]
    if len(midline)>im.shape[1]//2:
        x_start, x_end = np.min(midline), np.max(midline)
    else: # This actually rarely happens p~1/10000
        x_start, x_end = im.shape[1]//10, 9*im.shape[1]//10
    cx = (x_start + x_end)/2
    r = (x_end - x_start)/2
    return cx, cy, r

def resize_image(im, image_size, augmentation=False):
    # Crops, resizes and potentially augments the image to IMAGE_SIZE
    cx, cy, r = info_image(im)
    scaling = image_size/(2*r)
    rotation = 0
    if augmentation:
        scaling *= 1 + 0.3 * (np.random.rand()-0.5)
        rotation = 360 * np.random.rand()
    M = cv2.getRotationMatrix2D((cx,cy), rotation, scaling)
    M[0,2] -= cx - image_size/2
    M[1,2] -= cy - image_size/2
    return cv2.warpAffine(im,M,(image_size, image_size))

class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir
        self.number = 0

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)
        # lab_planes = cv2.split(lab)
        # clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(16, 16))
        # lab_planes[0] = clahe.apply(lab_planes[0])
        # lab = cv2.merge(lab_planes)
        # clahed = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)
        # image = resize_image(clahed, cfg.img_size)
        # print(tf.test.is_gpu_available())
        # vessel_model = VesselNet('./vessels/')
        image = process(lab, crop='normal', preprocessing='clahe',opencv_veins=False)

        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-150, 150)),
        transforms.RandomVerticalFlip(),
        transforms.ColorJitter(brightness=0.1, contrast=0.5, saturation=0.1, hue=0.1),
        # transforms.RandomResizedCrop(cfg.img_size_crop),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
        ])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ])","
import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedKFold

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'           #### NEED TO CHANGED IT!!!!!!!!!!!!!!!!!!!
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_full_images/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))
    print(""Training path: {}"".format(train_path))
    # Data Processing


    ## SHUFFLE DATA
    skf = None
    # if cfg.valid_type == 'holdout':
    test_805 = pd.read_csv('./input/test_805.csv')
    test_805 = pd.concat([test_805, test_805])
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True,
                                                                   random_state=cfg.seed, stratify=train_csv['diagnosis'])
    # train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid) # need to change It!!!!!
    # elif cfg.valid_type == 'cv':
    #     skf = StratifiedKFold(n_splits=cfg.n_folds, shuffle=True, random_state=cfg.seed)



    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model
    # cfg.model.load_state_dict(
    #     torch.load('./Model_weights_finetuning/finetune22_like21_ben_normalcrop.pth')['model'])

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()

    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                pass
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                # loss = cfg.criterion(output, target)
                loss = torch.mean(cfg.criterion.lossfun((output - target)))
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                pass
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            # target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                # loss = cfg.criterion(output, target)
                loss = torch.mean(cfg.criterion.lossfun((output - target)))
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [1e-3]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b5']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",13552.0
2019.09.04 16:25:52,new_old,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models
from torchvision.models import resnext101_32x8d as resnext

class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        self.model = None
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            # model.conv1 = nn.Conv2d(4, 64, (7, 7), (2, 2), (3, 3), bias=False)
            #     param.requires_grad = False
            print(model)
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                # nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'ResNet152':
            model = models.resnet152(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet152.pth""))
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model
        elif model_type == 'ResNext101':
            model = resnext(pretrained=True)
            model.fc = nn.Linear(in_features=2048, out_features=5)
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            # model._conv_stem = nn.Conv2d(4, 32, kernel_size=3, stride=2, bias=False)
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))
            # model._conv_stem = nn.Conv2d(4, 48, kernel_size=3, stride=2, bias=False)
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            # #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b6':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b6-c76e70fd.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b7':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b7-dcc49843.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            #model.cuda()
            self.model = model


",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 1e-05
)",<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb9c4f4ef0>,10.0,7702403.0,16.0,16.0,0.541734,0.397818,0.8203,,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np
from robust_loss_pytorch import AdaptiveLossFunction

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = True
    # torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'data_imgsize_256'
        self.img_size = 256
        # self.img_size_crop = 300

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()#AdaptiveLossFunction(num_dims=1, float_dtype=np.float32, device='cuda:0')
        # self.num_classes = 5
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 10
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP

        self.num_workers = 16
        self.n_epochs = 200
        self.batch_size = batch_size
        self.valid_type = 'holdout' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50","from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config
import math

from utils_img import *
from retinal_process import *

# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()
# import tensorflow as tf

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p



def subtract_median_bg_image(im):
    k = np.max(im.shape)//20*2+1
    bg = cv2.medianBlur(im, k)
    return cv2.addWeighted (im, 4, bg, -4, 128)

PARAM = 96
def Radius_Reduction(img,PARAM):
    h,w,c=img.shape
    Frame=np.zeros((h,w,c),dtype=np.uint8)
    cv2.circle(Frame,(int(math.floor(w/2)),int(math.floor(h/2))),int(math.floor((h*PARAM)/float(2*100))), (255,255,255), -1)
    Frame1=cv2.cvtColor(Frame, cv2.COLOR_BGR2GRAY)
    img1 =cv2.bitwise_and(img,img,mask=Frame1)
    return img1

def info_image(im):
    # Compute the center (cx, cy) and radius of the eye
    cy = im.shape[0]//2
    midline = im[cy,:]
    midline = np.where(midline>midline.mean()/3)[0]
    if len(midline)>im.shape[1]//2:
        x_start, x_end = np.min(midline), np.max(midline)
    else: # This actually rarely happens p~1/10000
        x_start, x_end = im.shape[1]//10, 9*im.shape[1]//10
    cx = (x_start + x_end)/2
    r = (x_end - x_start)/2
    return cx, cy, r

def resize_image(im, image_size, augmentation=False):
    # Crops, resizes and potentially augments the image to IMAGE_SIZE
    cx, cy, r = info_image(im)
    scaling = image_size/(2*r)
    rotation = 0
    if augmentation:
        scaling *= 1 + 0.3 * (np.random.rand()-0.5)
        rotation = 360 * np.random.rand()
    M = cv2.getRotationMatrix2D((cx,cy), rotation, scaling)
    M[0,2] -= cx - image_size/2
    M[1,2] -= cy - image_size/2
    return cv2.warpAffine(im,M,(image_size, image_size))

class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir
        self.number = 0

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)
        # lab_planes = cv2.split(lab)
        # clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(16, 16))
        # lab_planes[0] = clahe.apply(lab_planes[0])
        # lab = cv2.merge(lab_planes)
        # clahed = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)
        # image = resize_image(clahed, cfg.img_size)
        # print(tf.test.is_gpu_available())
        # vessel_model = VesselNet('./vessels/')
        image = process(lab, crop='normal', preprocessing='clahe',opencv_veins=False)

        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-150, 150)),
        transforms.RandomVerticalFlip(),
        transforms.ColorJitter(brightness=0.1, contrast=0.5, saturation=0.1, hue=0.1),
        # transforms.RandomResizedCrop(cfg.img_size_crop),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
        ])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ])","
import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedKFold

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'           #### NEED TO CHANGED IT!!!!!!!!!!!!!!!!!!!
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_full_images/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))
    print(""Training path: {}"".format(train_path))
    # Data Processing


    ## SHUFFLE DATA
    skf = None
    # if cfg.valid_type == 'holdout':
    test_805 = pd.read_csv('./input/test_805.csv')
    test_805 = pd.concat([test_805, test_805])
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True,
                                                                   random_state=cfg.seed, stratify=train_csv['diagnosis'])
    # train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid) # need to change It!!!!!
    # elif cfg.valid_type == 'cv':
    #     skf = StratifiedKFold(n_splits=cfg.n_folds, shuffle=True, random_state=cfg.seed)



    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model
    # cfg.model.load_state_dict(
    #     torch.load('./Model_weights_finetuning/finetune22_like21_ben_normalcrop.pth')['model'])

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()

    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                pass
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # loss = torch.mean(cfg.criterion.lossfun((output - target)))
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                pass
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            # target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # loss = torch.mean(cfg.criterion.lossfun((output - target)))
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [1e-3]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b3']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",12077.54
2019.09.04 23:33:22,new_old,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models
from torchvision.models import resnext101_32x8d as resnext

class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        self.model = None
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            # model.conv1 = nn.Conv2d(4, 64, (7, 7), (2, 2), (3, 3), bias=False)
            #     param.requires_grad = False
            print(model)
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                # nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'ResNet152':
            model = models.resnet152(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet152.pth""))
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model
        elif model_type == 'ResNext101':
            model = resnext(pretrained=True)
            # model.conv1 = nn.Conv2d(4, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
            model.fc = nn.Linear(in_features=2048, out_features=1)
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))


            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            # model._conv_stem = nn.Conv2d(4, 32, kernel_size=3, stride=2, bias=False)
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))
            # model._conv_stem = nn.Conv2d(4, 48, kernel_size=3, stride=2, bias=False)
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            # #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b6':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b6-c76e70fd.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b7':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b7-dcc49843.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            #model.cuda()
            self.model = model


",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 1e-05
)",<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb50b09898>,10.0,28342833.0,24.0,16.0,0.37251,0.297144,0.8351,,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np
from robust_loss_pytorch import AdaptiveLossFunction

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = True
    # torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'data_imgsize_256'
        self.img_size = 256
        # self.img_size_crop = 300

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()#AdaptiveLossFunction(num_dims=1, float_dtype=np.float32, device='cuda:0')
        # self.num_classes = 5
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 10
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP

        self.num_workers = 16
        self.n_epochs = 200
        self.batch_size = batch_size
        self.valid_type = 'holdout' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50","from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config
import math

from utils_img import *
from retinal_process import *

# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()
# import tensorflow as tf

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p



def subtract_median_bg_image(im):
    k = np.max(im.shape)//20*2+1
    bg = cv2.medianBlur(im, k)
    return cv2.addWeighted (im, 4, bg, -4, 128)

PARAM = 96
def Radius_Reduction(img,PARAM):
    h,w,c=img.shape
    Frame=np.zeros((h,w,c),dtype=np.uint8)
    cv2.circle(Frame,(int(math.floor(w/2)),int(math.floor(h/2))),int(math.floor((h*PARAM)/float(2*100))), (255,255,255), -1)
    Frame1=cv2.cvtColor(Frame, cv2.COLOR_BGR2GRAY)
    img1 =cv2.bitwise_and(img,img,mask=Frame1)
    return img1

def info_image(im):
    # Compute the center (cx, cy) and radius of the eye
    cy = im.shape[0]//2
    midline = im[cy,:]
    midline = np.where(midline>midline.mean()/3)[0]
    if len(midline)>im.shape[1]//2:
        x_start, x_end = np.min(midline), np.max(midline)
    else: # This actually rarely happens p~1/10000
        x_start, x_end = im.shape[1]//10, 9*im.shape[1]//10
    cx = (x_start + x_end)/2
    r = (x_end - x_start)/2
    return cx, cy, r

def resize_image(im, image_size, augmentation=False):
    # Crops, resizes and potentially augments the image to IMAGE_SIZE
    cx, cy, r = info_image(im)
    scaling = image_size/(2*r)
    rotation = 0
    if augmentation:
        scaling *= 1 + 0.3 * (np.random.rand()-0.5)
        rotation = 360 * np.random.rand()
    M = cv2.getRotationMatrix2D((cx,cy), rotation, scaling)
    M[0,2] -= cx - image_size/2
    M[1,2] -= cy - image_size/2
    return cv2.warpAffine(im,M,(image_size, image_size))

class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir
        self.number = 0

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        # lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)
        # lab_planes = cv2.split(lab)
        # clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(16, 16))
        # lab_planes[0] = clahe.apply(lab_planes[0])
        # lab = cv2.merge(lab_planes)
        # clahed = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)
        # image = resize_image(clahed, cfg.img_size)
        # print(tf.test.is_gpu_available())
        # vessel_model = VesselNet('./vessels/')
        image = process(image, crop='normal', preprocessing='clahe', fourth=None)

        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-150, 150)),
        transforms.RandomVerticalFlip(),
        transforms.ColorJitter(brightness=0.1, contrast=0.5, saturation=0.1, hue=0.1),
        # transforms.RandomResizedCrop(cfg.img_size_crop),
        transforms.ToTensor(),
        transforms.Normalize([0.406, 0.456, 0.485], [0.225, 0.224, 0.229]),
        ])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.406, 0.456, 0.485], [0.225, 0.224, 0.229])
    ])","
import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedKFold

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'           #### NEED TO CHANGED IT!!!!!!!!!!!!!!!!!!!
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_full_images/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))
    print(""Training path: {}"".format(train_path))
    # Data Processing


    ## SHUFFLE DATA
    skf = None
    # if cfg.valid_type == 'holdout':
    test_805 = pd.read_csv('./input/test_805.csv')
    test_805 = pd.concat([test_805, test_805])
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True,
                                                                   random_state=cfg.seed, stratify=train_csv['diagnosis'])
    # train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid) # need to change It!!!!!
    # elif cfg.valid_type == 'cv':
    #     skf = StratifiedKFold(n_splits=cfg.n_folds, shuffle=True, random_state=cfg.seed)



    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model
    # cfg.model.load_state_dict(
    #     torch.load('./Model_weights_finetuning/finetune23_big_dataset_b2.pth')['model'])

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()

    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                pass
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # loss = torch.mean(cfg.criterion.lossfun((output - target)))
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                pass
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            # target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # loss = torch.mean(cfg.criterion.lossfun((output - target)))
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [1e-3]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b5']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",35003.28
2019.09.05 14:45:01,new_old,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models
from torchvision.models import resnext101_32x8d as resnext

class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        self.model = None
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            # model.conv1 = nn.Conv2d(4, 64, (7, 7), (2, 2), (3, 3), bias=False)
            #     param.requires_grad = False
            print(model)
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                # nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'ResNet152':
            model = models.resnet152(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet152.pth""))
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model
        elif model_type == 'ResNext101':
            model = resnext(pretrained=True)
            # model.conv1 = nn.Conv2d(4, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
            model.fc = nn.Linear(in_features=2048, out_features=1)
            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))


            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            # model._conv_stem = nn.Conv2d(4, 32, kernel_size=3, stride=2, bias=False)
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))
            # model._conv_stem = nn.Conv2d(4, 48, kernel_size=3, stride=2, bias=False)
            # for i, layer in enumerate(model.):
            #     if ""batch_normalization"" in layer.name:
            #         effnet.layers[i] = GroupNormalization(groups=2, axis=-1, epsilon=0.1)
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            # #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b6':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b6-c76e70fd.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b7':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b7-dcc49843.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            #model.cuda()
            self.model = model


",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 1e-05
)",<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb9d1516d8>,10.0,86744385.0,200.0,16.0,0.5964619999999999,0.680645,0.7709,,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np
from robust_loss_pytorch import AdaptiveLossFunction

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'data_imgsize_299'
        self.img_size = 299
        # self.img_size_crop = 300

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()#AdaptiveLossFunction(num_dims=1, float_dtype=np.float32, device='cuda:0')
        # self.num_classes = 5
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 10
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP

        self.num_workers = 16
        self.n_epochs = 200
        self.batch_size = batch_size
        self.valid_type = 'holdout' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50","from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config
import math

from utils_img import *
from retinal_process import *

# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()
# import tensorflow as tf

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p



def subtract_median_bg_image(im):
    k = np.max(im.shape)//20*2+1
    bg = cv2.medianBlur(im, k)
    return cv2.addWeighted (im, 4, bg, -4, 128)

PARAM = 96
def Radius_Reduction(img,PARAM):
    h,w,c=img.shape
    Frame=np.zeros((h,w,c),dtype=np.uint8)
    cv2.circle(Frame,(int(math.floor(w/2)),int(math.floor(h/2))),int(math.floor((h*PARAM)/float(2*100))), (255,255,255), -1)
    Frame1=cv2.cvtColor(Frame, cv2.COLOR_BGR2GRAY)
    img1 =cv2.bitwise_and(img,img,mask=Frame1)
    return img1

def info_image(im):
    # Compute the center (cx, cy) and radius of the eye
    cy = im.shape[0]//2
    midline = im[cy,:]
    midline = np.where(midline>midline.mean()/3)[0]
    if len(midline)>im.shape[1]//2:
        x_start, x_end = np.min(midline), np.max(midline)
    else: # This actually rarely happens p~1/10000
        x_start, x_end = im.shape[1]//10, 9*im.shape[1]//10
    cx = (x_start + x_end)/2
    r = (x_end - x_start)/2
    return cx, cy, r

def resize_image(im, image_size, augmentation=False):
    # Crops, resizes and potentially augments the image to IMAGE_SIZE
    cx, cy, r = info_image(im)
    scaling = image_size/(2*r)
    rotation = 0
    if augmentation:
        scaling *= 1 + 0.3 * (np.random.rand()-0.5)
        rotation = 360 * np.random.rand()
    M = cv2.getRotationMatrix2D((cx,cy), rotation, scaling)
    M[0,2] -= cx - image_size/2
    M[1,2] -= cy - image_size/2
    return cv2.warpAffine(im,M,(image_size, image_size))

class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir
        self.number = 0

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        # lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)
        # lab_planes = cv2.split(lab)
        # clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(16, 16))
        # lab_planes[0] = clahe.apply(lab_planes[0])
        # lab = cv2.merge(lab_planes)
        # clahed = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)
        # image = resize_image(clahed, cfg.img_size)
        # print(tf.test.is_gpu_available())
        # vessel_model = VesselNet('./vessels/')
        image = process(image, size = cfg.img_size, crop='normal', preprocessing='clahe', fourth=None)

        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-150, 150)),
        transforms.RandomVerticalFlip(),
        transforms.ColorJitter(brightness=0.1, contrast=0.5, saturation=0.1, hue=0.1),
        # transforms.RandomResizedCrop(cfg.img_size_crop),
        transforms.ToTensor(),
        transforms.Normalize([0.406, 0.456, 0.485], [0.225, 0.224, 0.229]),
        ])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.406, 0.456, 0.485], [0.225, 0.224, 0.229])
    ])","
import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedKFold

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'           #### NEED TO CHANGED IT!!!!!!!!!!!!!!!!!!!
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_full_images/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))
    print(""Training path: {}"".format(train_path))
    # Data Processing


    ## SHUFFLE DATA
    skf = None
    # if cfg.valid_type == 'holdout':
    test_805 = pd.read_csv('./input/test_805.csv')
    test_805 = pd.concat([test_805, test_805])
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True,
                                                                   random_state=cfg.seed, stratify=train_csv['diagnosis'])
    # train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid) # need to change It!!!!!
    # elif cfg.valid_type == 'cv':
    #     skf = StratifiedKFold(n_splits=cfg.n_folds, shuffle=True, random_state=cfg.seed)



    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model
    cfg.model.load_state_dict(
        torch.load('./Model_weights_finetuning/finetune24_bigdata_b5_clahe.pth')['model'])

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()

    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                pass
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # loss = torch.mean(cfg.criterion.lossfun((output - target)))
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                pass
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            # target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # loss = torch.mean(cfg.criterion.lossfun((output - target)))
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [1e-3]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b5']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",
2019.09.06 00:40:34,new_old,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models
from torchvision.models import resnext101_32x8d as resnext
from torchvision.models import DenseNet

class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        self.model = None
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            # model.conv1 = nn.Conv2d(4, 64, (7, 7), (2, 2), (3, 3), bias=False)
            #     param.requires_grad = False
            print(model)
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                # nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'ResNet152':
            model = models.resnet152(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet152.pth""))
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model
        elif model_type == 'ResNext101':
            model = resnext(pretrained=True)
            # model.conv1 = nn.Conv2d(4, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
            model.fc = nn.Linear(in_features=2048, out_features=1)
            self.model = model

        elif model_type == 'DenseNet':
            model = models.densenet201(pretrained=True)
            # model.conv1 = nn.Conv2d(4, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
            model.classifier = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=1920, out_features=1)
            )
            print(model)

            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))


            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            # model._conv_stem = nn.Conv2d(4, 32, kernel_size=3, stride=2, bias=False)
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))
            # model._conv_stem = nn.Conv2d(4, 48, kernel_size=3, stride=2, bias=False)
            # for i, layer in enumerate(model.):
            #     if ""batch_normalization"" in layer.name:
            #         effnet.layers[i] = GroupNormalization(groups=2, axis=-1, epsilon=0.1)
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            # #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b6':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b6-c76e70fd.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b7':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b7-dcc49843.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            #model.cuda()
            self.model = model


",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 1e-05
)",<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb50a98470>,10.0,18094849.0,200.0,16.0,0.710476,0.714962,0.2883,,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np
from robust_loss_pytorch import AdaptiveLossFunction

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'data_imgsize_224'
        self.img_size = 224
        # self.img_size_crop = 300

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()#AdaptiveLossFunction(num_dims=1, float_dtype=np.float32, device='cuda:0')
        # self.num_classes = 5
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 10
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP

        self.num_workers = 16
        self.n_epochs = 200
        self.batch_size = batch_size
        self.valid_type = 'holdout' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50","from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config
import math

from utils_img import *
from retinal_process import *

# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()
# import tensorflow as tf

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p



def subtract_median_bg_image(im):
    k = np.max(im.shape)//20*2+1
    bg = cv2.medianBlur(im, k)
    return cv2.addWeighted (im, 4, bg, -4, 128)

PARAM = 96
def Radius_Reduction(img,PARAM):
    h,w,c=img.shape
    Frame=np.zeros((h,w,c),dtype=np.uint8)
    cv2.circle(Frame,(int(math.floor(w/2)),int(math.floor(h/2))),int(math.floor((h*PARAM)/float(2*100))), (255,255,255), -1)
    Frame1=cv2.cvtColor(Frame, cv2.COLOR_BGR2GRAY)
    img1 =cv2.bitwise_and(img,img,mask=Frame1)
    return img1

def info_image(im):
    # Compute the center (cx, cy) and radius of the eye
    cy = im.shape[0]//2
    midline = im[cy,:]
    midline = np.where(midline>midline.mean()/3)[0]
    if len(midline)>im.shape[1]//2:
        x_start, x_end = np.min(midline), np.max(midline)
    else: # This actually rarely happens p~1/10000
        x_start, x_end = im.shape[1]//10, 9*im.shape[1]//10
    cx = (x_start + x_end)/2
    r = (x_end - x_start)/2
    return cx, cy, r

def resize_image(im, image_size, augmentation=False):
    # Crops, resizes and potentially augments the image to IMAGE_SIZE
    cx, cy, r = info_image(im)
    scaling = image_size/(2*r)
    rotation = 0
    if augmentation:
        scaling *= 1 + 0.3 * (np.random.rand()-0.5)
        rotation = 360 * np.random.rand()
    M = cv2.getRotationMatrix2D((cx,cy), rotation, scaling)
    M[0,2] -= cx - image_size/2
    M[1,2] -= cy - image_size/2
    return cv2.warpAffine(im,M,(image_size, image_size))

class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir
        self.number = 0

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        # lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)
        # lab_planes = cv2.split(lab)
        # clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(16, 16))
        # lab_planes[0] = clahe.apply(lab_planes[0])
        # lab = cv2.merge(lab_planes)
        # clahed = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)
        # image = resize_image(clahed, cfg.img_size)
        # print(tf.test.is_gpu_available())
        # vessel_model = VesselNet('./vessels/')
        image = process(image, size = cfg.img_size, crop='normal', preprocessing='clahe', fourth=None)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-150, 150)),
        transforms.RandomVerticalFlip(),
        transforms.ColorJitter(brightness=0.1, contrast=0.5, saturation=0.1, hue=0.1),
        # transforms.RandomResizedCrop(cfg.img_size_crop),
        transforms.ToTensor(),
        transforms.Normalize([0.406, 0.456, 0.485], [0.225, 0.224, 0.229]),
        ])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.406, 0.456, 0.485], [0.225, 0.224, 0.229])
    ])","
import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedKFold

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'           #### NEED TO CHANGED IT!!!!!!!!!!!!!!!!!!!
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_full_images/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))
    print(""Training path: {}"".format(train_path))
    # Data Processing


    ## SHUFFLE DATA
    skf = None
    # if cfg.valid_type == 'holdout':
    test_805 = pd.read_csv('./input/test_805.csv')
    test_805 = pd.concat([test_805, test_805])
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True,
                                                                   random_state=cfg.seed, stratify=train_csv['diagnosis'])
    # train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid) # need to change It!!!!!
    # elif cfg.valid_type == 'cv':
    #     skf = StratifiedKFold(n_splits=cfg.n_folds, shuffle=True, random_state=cfg.seed)



    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model
    cfg.model.load_state_dict(
        torch.load('./Model_weights_finetuning/finetune25_like24_but_resnext101_img_size_299.pth')['model'])

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()

    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                pass
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # loss = torch.mean(cfg.criterion.lossfun((output - target)))
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                pass
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            # target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # loss = torch.mean(cfg.criterion.lossfun((output - target)))
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [1e-3]
    p_horizontalflip_list = [0.4]
    model_type_list = ['ResNext101']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",
2019.09.06 07:13:29,new_old,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models
from torchvision.models import resnext101_32x8d as resnext
from torchvision.models import DenseNet

class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        self.model = None
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            # model.conv1 = nn.Conv2d(4, 64, (7, 7), (2, 2), (3, 3), bias=False)
            #     param.requires_grad = False
            print(model)
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                # nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'ResNet152':
            model = models.resnet152(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet152.pth""))
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model
        elif model_type == 'ResNext101':
            model = resnext(pretrained=True)
            # model.conv1 = nn.Conv2d(4, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
            model.fc = nn.Linear(in_features=2048, out_features=1)
            self.model = model

        elif model_type == 'DenseNet':
            model = models.densenet201(pretrained=True)
            # model.conv1 = nn.Conv2d(4, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
            model.classifier = nn.Linear(in_features=1920, out_features=5)
            # model.classifier = nn.Sequential(
            #     nn.Dropout(0.4),
            #     nn.Linear(in_features=1920, out_features=1)
            # )
            print(model)

            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))


            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            # model._conv_stem = nn.Conv2d(4, 32, kernel_size=3, stride=2, bias=False)
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))
            # model._conv_stem = nn.Conv2d(4, 48, kernel_size=3, stride=2, bias=False)
            # for i, layer in enumerate(model.):
            #     if ""batch_normalization"" in layer.name:
            #         effnet.layers[i] = GroupNormalization(groups=2, axis=-1, epsilon=0.1)
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            # #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b6':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b6-c76e70fd.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b7':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b7-dcc49843.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            #model.cuda()
            self.model = model


",CrossEntropyLoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 1e-05
)",<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb50a965c0>,10.0,18102533.0,200.0,16.0,0.882641,1.376008,0.0,,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np
from robust_loss_pytorch import AdaptiveLossFunction

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'data_imgsize_224'
        self.img_size = 224
        # self.img_size_crop = 300

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.CrossEntropyLoss()#AdaptiveLossFunction(num_dims=1, float_dtype=np.float32, device='cuda:0')
        # self.num_classes = 5
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 10
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP

        self.num_workers = 16
        self.n_epochs = 200
        self.batch_size = batch_size
        self.valid_type = 'holdout' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50","from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config
import math

from utils_img import *
from retinal_process import *

# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()
# import tensorflow as tf

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p



def subtract_median_bg_image(im):
    k = np.max(im.shape)//20*2+1
    bg = cv2.medianBlur(im, k)
    return cv2.addWeighted (im, 4, bg, -4, 128)

PARAM = 96
def Radius_Reduction(img,PARAM):
    h,w,c=img.shape
    Frame=np.zeros((h,w,c),dtype=np.uint8)
    cv2.circle(Frame,(int(math.floor(w/2)),int(math.floor(h/2))),int(math.floor((h*PARAM)/float(2*100))), (255,255,255), -1)
    Frame1=cv2.cvtColor(Frame, cv2.COLOR_BGR2GRAY)
    img1 =cv2.bitwise_and(img,img,mask=Frame1)
    return img1

def info_image(im):
    # Compute the center (cx, cy) and radius of the eye
    cy = im.shape[0]//2
    midline = im[cy,:]
    midline = np.where(midline>midline.mean()/3)[0]
    if len(midline)>im.shape[1]//2:
        x_start, x_end = np.min(midline), np.max(midline)
    else: # This actually rarely happens p~1/10000
        x_start, x_end = im.shape[1]//10, 9*im.shape[1]//10
    cx = (x_start + x_end)/2
    r = (x_end - x_start)/2
    return cx, cy, r

def resize_image(im, image_size, augmentation=False):
    # Crops, resizes and potentially augments the image to IMAGE_SIZE
    cx, cy, r = info_image(im)
    scaling = image_size/(2*r)
    rotation = 0
    if augmentation:
        scaling *= 1 + 0.3 * (np.random.rand()-0.5)
        rotation = 360 * np.random.rand()
    M = cv2.getRotationMatrix2D((cx,cy), rotation, scaling)
    M[0,2] -= cx - image_size/2
    M[1,2] -= cy - image_size/2
    return cv2.warpAffine(im,M,(image_size, image_size))

class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir
        self.number = 0

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        # lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)
        # lab_planes = cv2.split(lab)
        # clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(16, 16))
        # lab_planes[0] = clahe.apply(lab_planes[0])
        # lab = cv2.merge(lab_planes)
        # clahed = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)
        # image = resize_image(clahed, cfg.img_size)
        # print(tf.test.is_gpu_available())
        # vessel_model = VesselNet('./vessels/')
        image = process(image, size = cfg.img_size, crop='normal', preprocessing='clahe', fourth=None)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-150, 150)),
        transforms.RandomVerticalFlip(),
        transforms.ColorJitter(brightness=0.1, contrast=0.5, saturation=0.1, hue=0.1),
        # transforms.RandomResizedCrop(cfg.img_size_crop),
        transforms.ToTensor(),
        transforms.Normalize([0.406, 0.456, 0.485], [0.225, 0.224, 0.229]),
        ])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.406, 0.456, 0.485], [0.225, 0.224, 0.229])
    ])","
import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedKFold

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'           #### NEED TO CHANGED IT!!!!!!!!!!!!!!!!!!!
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_full_images/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))
    print(""Training path: {}"".format(train_path))
    # Data Processing


    ## SHUFFLE DATA
    skf = None
    # if cfg.valid_type == 'holdout':
    test_805 = pd.read_csv('./input/test_805.csv')
    test_805 = pd.concat([test_805, test_805])
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True,
                                                                   random_state=cfg.seed, stratify=train_csv['diagnosis'])
    # train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid) # need to change It!!!!!
    # elif cfg.valid_type == 'cv':
    #     skf = StratifiedKFold(n_splits=cfg.n_folds, shuffle=True, random_state=cfg.seed)



    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model
    cfg.model.load_state_dict(
        torch.load('./Model_weights_finetuning/finetune25_like24_but_resnext101_img_size_299.pth')['model'])

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()

    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                pass
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # loss = torch.mean(cfg.criterion.lossfun((output - target)))
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                pass
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            # target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # loss = torch.mean(cfg.criterion.lossfun((output - target)))
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [1e-3]
    p_horizontalflip_list = [0.4]
    model_type_list = ['ResNext101']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",
2019.09.06 07:35:47,new_old,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models
from torchvision.models import resnext101_32x8d as resnext
from torchvision.models import DenseNet

class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        self.model = None
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            # model.conv1 = nn.Conv2d(4, 64, (7, 7), (2, 2), (3, 3), bias=False)
            #     param.requires_grad = False
            print(model)
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                # nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'ResNet152':
            model = models.resnet152(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet152.pth""))
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model
        elif model_type == 'ResNext101':
            model = resnext(pretrained=True)
            # model.conv1 = nn.Conv2d(4, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
            model.fc = nn.Linear(in_features=2048, out_features=1)
            self.model = model

        elif model_type == 'DenseNet':
            model = models.densenet121(pretrained=True)
            # model.conv1 = nn.Conv2d(4, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
            model.classifier = nn.Linear(in_features=1024, out_features=1)
            # model.classifier = nn.Sequential(
            #     nn.Dropout(0.4),
            #     nn.Linear(in_features=1920, out_features=1)
            # )
            print(model)

            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))


            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            # model._conv_stem = nn.Conv2d(4, 32, kernel_size=3, stride=2, bias=False)
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))
            # model._conv_stem = nn.Conv2d(4, 48, kernel_size=3, stride=2, bias=False)
            # for i, layer in enumerate(model.):
            #     if ""batch_normalization"" in layer.name:
            #         effnet.layers[i] = GroupNormalization(groups=2, axis=-1, epsilon=0.1)
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            # #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b6':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b6-c76e70fd.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b7':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b7-dcc49843.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            #model.cuda()
            self.model = model


",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 1e-05
)",<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb50b347b8>,10.0,6954881.0,200.0,16.0,0.967815,1.885315,0.0,,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np
from robust_loss_pytorch import AdaptiveLossFunction

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'data_imgsize_224'
        self.img_size = 224
        # self.img_size_crop = 300

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()#AdaptiveLossFunction(num_dims=1, float_dtype=np.float32, device='cuda:0')
        # self.num_classes = 5
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 10
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP

        self.num_workers = 16
        self.n_epochs = 200
        self.batch_size = batch_size
        self.valid_type = 'holdout' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50","from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config
import math

from utils_img import *
from retinal_process import *

# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()
# import tensorflow as tf

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p



def subtract_median_bg_image(im):
    k = np.max(im.shape)//20*2+1
    bg = cv2.medianBlur(im, k)
    return cv2.addWeighted (im, 4, bg, -4, 128)

PARAM = 96
def Radius_Reduction(img,PARAM):
    h,w,c=img.shape
    Frame=np.zeros((h,w,c),dtype=np.uint8)
    cv2.circle(Frame,(int(math.floor(w/2)),int(math.floor(h/2))),int(math.floor((h*PARAM)/float(2*100))), (255,255,255), -1)
    Frame1=cv2.cvtColor(Frame, cv2.COLOR_BGR2GRAY)
    img1 =cv2.bitwise_and(img,img,mask=Frame1)
    return img1

def info_image(im):
    # Compute the center (cx, cy) and radius of the eye
    cy = im.shape[0]//2
    midline = im[cy,:]
    midline = np.where(midline>midline.mean()/3)[0]
    if len(midline)>im.shape[1]//2:
        x_start, x_end = np.min(midline), np.max(midline)
    else: # This actually rarely happens p~1/10000
        x_start, x_end = im.shape[1]//10, 9*im.shape[1]//10
    cx = (x_start + x_end)/2
    r = (x_end - x_start)/2
    return cx, cy, r

def resize_image(im, image_size, augmentation=False):
    # Crops, resizes and potentially augments the image to IMAGE_SIZE
    cx, cy, r = info_image(im)
    scaling = image_size/(2*r)
    rotation = 0
    if augmentation:
        scaling *= 1 + 0.3 * (np.random.rand()-0.5)
        rotation = 360 * np.random.rand()
    M = cv2.getRotationMatrix2D((cx,cy), rotation, scaling)
    M[0,2] -= cx - image_size/2
    M[1,2] -= cy - image_size/2
    return cv2.warpAffine(im,M,(image_size, image_size))

class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir
        self.number = 0

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        # lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)
        # lab_planes = cv2.split(lab)
        # clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(16, 16))
        # lab_planes[0] = clahe.apply(lab_planes[0])
        # lab = cv2.merge(lab_planes)
        # clahed = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)
        # image = resize_image(clahed, cfg.img_size)
        # print(tf.test.is_gpu_available())
        # vessel_model = VesselNet('./vessels/')
        image = process(image, size = cfg.img_size, crop='normal', preprocessing='clahe', fourth=None)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-150, 150)),
        transforms.RandomVerticalFlip(),
        transforms.ColorJitter(brightness=0.1, contrast=0.5, saturation=0.1, hue=0.1),
        # transforms.RandomResizedCrop(cfg.img_size_crop),
        transforms.ToTensor(),
        transforms.Normalize([0.406, 0.456, 0.485], [0.225, 0.224, 0.229]),
        ])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.406, 0.456, 0.485], [0.225, 0.224, 0.229])
    ])","
import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedKFold

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'           #### NEED TO CHANGED IT!!!!!!!!!!!!!!!!!!!
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_full_images/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))
    print(""Training path: {}"".format(train_path))
    # Data Processing


    ## SHUFFLE DATA
    skf = None
    # if cfg.valid_type == 'holdout':
    test_805 = pd.read_csv('./input/test_805.csv')
    test_805 = pd.concat([test_805, test_805])
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True,
                                                                   random_state=cfg.seed, stratify=train_csv['diagnosis'])
    # train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid) # need to change It!!!!!
    # elif cfg.valid_type == 'cv':
    #     skf = StratifiedKFold(n_splits=cfg.n_folds, shuffle=True, random_state=cfg.seed)



    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model
    cfg.model.load_state_dict(
        torch.load('./Model_weights_finetuning/finetune25_like24_but_resnext101_img_size_299.pth')['model'])

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()

    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                pass
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # loss = torch.mean(cfg.criterion.lossfun((output - target)))
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                pass
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            # target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # loss = torch.mean(cfg.criterion.lossfun((output - target)))
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [1e-3]
    p_horizontalflip_list = [0.4]
    model_type_list = ['ResNext101']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",
2019.09.06 07:53:02,new_old,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models
from torchvision.models import resnext101_32x8d as resnext
from torchvision.models import DenseNet

class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        self.model = None
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            # model.conv1 = nn.Conv2d(4, 64, (7, 7), (2, 2), (3, 3), bias=False)
            #     param.requires_grad = False
            print(model)
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                # nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'ResNet152':
            model = models.resnet152(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet152.pth""))
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model
        elif model_type == 'ResNext101':
            model = resnext(pretrained=True)
            # model.conv1 = nn.Conv2d(4, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
            model.fc = nn.Linear(in_features=2048, out_features=1)
            self.model = model

        elif model_type == 'DenseNet':
            model = models.densenet121(pretrained=True)
            # model.conv1 = nn.Conv2d(4, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
            model.classifier = nn.Linear(in_features=1024, out_features=1)
            # model.classifier = nn.Sequential(
            #     nn.Dropout(0.4),
            #     nn.Linear(in_features=1920, out_features=1)
            # )
            print(model)

            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))


            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            # model._conv_stem = nn.Conv2d(4, 32, kernel_size=3, stride=2, bias=False)
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))
            # model._conv_stem = nn.Conv2d(4, 48, kernel_size=3, stride=2, bias=False)
            # for i, layer in enumerate(model.):
            #     if ""batch_normalization"" in layer.name:
            #         effnet.layers[i] = GroupNormalization(groups=2, axis=-1, epsilon=0.1)
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            # #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b6':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b6-c76e70fd.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b7':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b7-dcc49843.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            #model.cuda()
            self.model = model


",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 1e-05
)",<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb50be1080>,10.0,86744385.0,200.0,16.0,0.557289,0.63092,0.777,,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np
from robust_loss_pytorch import AdaptiveLossFunction

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'data_imgsize_224'
        self.img_size = 224
        # self.img_size_crop = 300

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()#AdaptiveLossFunction(num_dims=1, float_dtype=np.float32, device='cuda:0')
        # self.num_classes = 5
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 10
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP

        self.num_workers = 16
        self.n_epochs = 200
        self.batch_size = batch_size
        self.valid_type = 'holdout' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50","from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config
import math

from utils_img import *
from retinal_process import *

# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()
# import tensorflow as tf

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p



def subtract_median_bg_image(im):
    k = np.max(im.shape)//20*2+1
    bg = cv2.medianBlur(im, k)
    return cv2.addWeighted (im, 4, bg, -4, 128)

PARAM = 96
def Radius_Reduction(img,PARAM):
    h,w,c=img.shape
    Frame=np.zeros((h,w,c),dtype=np.uint8)
    cv2.circle(Frame,(int(math.floor(w/2)),int(math.floor(h/2))),int(math.floor((h*PARAM)/float(2*100))), (255,255,255), -1)
    Frame1=cv2.cvtColor(Frame, cv2.COLOR_BGR2GRAY)
    img1 =cv2.bitwise_and(img,img,mask=Frame1)
    return img1

def info_image(im):
    # Compute the center (cx, cy) and radius of the eye
    cy = im.shape[0]//2
    midline = im[cy,:]
    midline = np.where(midline>midline.mean()/3)[0]
    if len(midline)>im.shape[1]//2:
        x_start, x_end = np.min(midline), np.max(midline)
    else: # This actually rarely happens p~1/10000
        x_start, x_end = im.shape[1]//10, 9*im.shape[1]//10
    cx = (x_start + x_end)/2
    r = (x_end - x_start)/2
    return cx, cy, r

def resize_image(im, image_size, augmentation=False):
    # Crops, resizes and potentially augments the image to IMAGE_SIZE
    cx, cy, r = info_image(im)
    scaling = image_size/(2*r)
    rotation = 0
    if augmentation:
        scaling *= 1 + 0.3 * (np.random.rand()-0.5)
        rotation = 360 * np.random.rand()
    M = cv2.getRotationMatrix2D((cx,cy), rotation, scaling)
    M[0,2] -= cx - image_size/2
    M[1,2] -= cy - image_size/2
    return cv2.warpAffine(im,M,(image_size, image_size))

class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir
        self.number = 0

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        # lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)
        # lab_planes = cv2.split(lab)
        # clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(16, 16))
        # lab_planes[0] = clahe.apply(lab_planes[0])
        # lab = cv2.merge(lab_planes)
        # clahed = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)
        # image = resize_image(clahed, cfg.img_size)
        # print(tf.test.is_gpu_available())
        # vessel_model = VesselNet('./vessels/')
        image = process(image, size = cfg.img_size, crop='normal', preprocessing='clahe', fourth=None)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-150, 150)),
        transforms.RandomVerticalFlip(),
        transforms.ColorJitter(brightness=0.1, contrast=0.5, saturation=0.1, hue=0.1),
        # transforms.RandomResizedCrop(cfg.img_size_crop),
        transforms.ToTensor(),
        transforms.Normalize([0.406, 0.456, 0.485], [0.225, 0.224, 0.229]),
        ])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.406, 0.456, 0.485], [0.225, 0.224, 0.229])
    ])","
import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedKFold

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'           #### NEED TO CHANGED IT!!!!!!!!!!!!!!!!!!!
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_full_images/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))
    print(""Training path: {}"".format(train_path))
    # Data Processing


    ## SHUFFLE DATA
    skf = None
    # if cfg.valid_type == 'holdout':
    test_805 = pd.read_csv('./input/test_805.csv')
    test_805 = pd.concat([test_805, test_805])
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True,
                                                                   random_state=cfg.seed, stratify=train_csv['diagnosis'])
    # train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid) # need to change It!!!!!
    # elif cfg.valid_type == 'cv':
    #     skf = StratifiedKFold(n_splits=cfg.n_folds, shuffle=True, random_state=cfg.seed)



    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model
    cfg.model.load_state_dict(
        torch.load('./Model_weights_finetuning/finetune25_like24_but_resnext101_img_size_299.pth')['model'])

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()

    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                pass
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # loss = torch.mean(cfg.criterion.lossfun((output - target)))
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                pass
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            # target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # loss = torch.mean(cfg.criterion.lossfun((output - target)))
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [1e-3]
    p_horizontalflip_list = [0.4]
    model_type_list = ['ResNext101']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",
2019.09.06 13:01:01,new_old,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models
from torchvision.models import resnext101_32x8d as resnext
from torchvision.models import DenseNet

class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        self.model = None
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            # model.conv1 = nn.Conv2d(4, 64, (7, 7), (2, 2), (3, 3), bias=False)
            #     param.requires_grad = False
            print(model)
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                # nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'ResNet152':
            model = models.resnet152(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet152.pth""))
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model
        elif model_type == 'ResNext101':
            model = resnext(pretrained=True)
            # model.conv1 = nn.Conv2d(4, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
            model.fc = nn.Linear(in_features=2048, out_features=1)
            self.model = model

        elif model_type == 'DenseNet':
            model = models.densenet121(pretrained=True)
            # model.conv1 = nn.Conv2d(4, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
            model.classifier = nn.Linear(in_features=1024, out_features=1)
            # model.classifier = nn.Sequential(
            #     nn.Dropout(0.4),
            #     nn.Linear(in_features=1920, out_features=1)
            # )
            print(model)

            self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))


            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            # model._conv_stem = nn.Conv2d(4, 32, kernel_size=3, stride=2, bias=False)
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))
            # model._conv_stem = nn.Conv2d(4, 48, kernel_size=3, stride=2, bias=False)
            # for i, layer in enumerate(model.):
            #     if ""batch_normalization"" in layer.name:
            #         effnet.layers[i] = GroupNormalization(groups=2, axis=-1, epsilon=0.1)
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            # #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b6':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b6-c76e70fd.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b7':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b7-dcc49843.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            #model.cuda()
            self.model = model


",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 1e-05
)",<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb50abccc0>,10.0,40738009.0,12.0,16.0,0.528369,0.408796,0.8186,,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np
from robust_loss_pytorch import AdaptiveLossFunction

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'data_imgsize_224'
        self.img_size = 224
        # self.img_size_crop = 300

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()#AdaptiveLossFunction(num_dims=1, float_dtype=np.float32, device='cuda:0')
        # self.num_classes = 5
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 10
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP

        self.num_workers = 16
        self.n_epochs = 200
        self.batch_size = batch_size
        self.valid_type = 'holdout' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50","from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config
import math

from utils_img import *
from retinal_process import *

# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()
# import tensorflow as tf

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p



def subtract_median_bg_image(im):
    k = np.max(im.shape)//20*2+1
    bg = cv2.medianBlur(im, k)
    return cv2.addWeighted (im, 4, bg, -4, 128)

PARAM = 96
def Radius_Reduction(img,PARAM):
    h,w,c=img.shape
    Frame=np.zeros((h,w,c),dtype=np.uint8)
    cv2.circle(Frame,(int(math.floor(w/2)),int(math.floor(h/2))),int(math.floor((h*PARAM)/float(2*100))), (255,255,255), -1)
    Frame1=cv2.cvtColor(Frame, cv2.COLOR_BGR2GRAY)
    img1 =cv2.bitwise_and(img,img,mask=Frame1)
    return img1

def info_image(im):
    # Compute the center (cx, cy) and radius of the eye
    cy = im.shape[0]//2
    midline = im[cy,:]
    midline = np.where(midline>midline.mean()/3)[0]
    if len(midline)>im.shape[1]//2:
        x_start, x_end = np.min(midline), np.max(midline)
    else: # This actually rarely happens p~1/10000
        x_start, x_end = im.shape[1]//10, 9*im.shape[1]//10
    cx = (x_start + x_end)/2
    r = (x_end - x_start)/2
    return cx, cy, r

def resize_image(im, image_size, augmentation=False):
    # Crops, resizes and potentially augments the image to IMAGE_SIZE
    cx, cy, r = info_image(im)
    scaling = image_size/(2*r)
    rotation = 0
    if augmentation:
        scaling *= 1 + 0.3 * (np.random.rand()-0.5)
        rotation = 360 * np.random.rand()
    M = cv2.getRotationMatrix2D((cx,cy), rotation, scaling)
    M[0,2] -= cx - image_size/2
    M[1,2] -= cy - image_size/2
    return cv2.warpAffine(im,M,(image_size, image_size))

class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir
        self.number = 0

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        # lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)
        # lab_planes = cv2.split(lab)
        # clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(16, 16))
        # lab_planes[0] = clahe.apply(lab_planes[0])
        # lab = cv2.merge(lab_planes)
        # clahed = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)
        # image = resize_image(clahed, cfg.img_size)
        # print(tf.test.is_gpu_available())
        # vessel_model = VesselNet('./vessels/')
        image = process(image, size = cfg.img_size, crop='normal', preprocessing='clahe', fourth=None)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-150, 150)),
        transforms.RandomVerticalFlip(),
        transforms.ColorJitter(brightness=0.1, contrast=0.5, saturation=0.1, hue=0.1),
        # transforms.RandomResizedCrop(cfg.img_size_crop),
        transforms.ToTensor(),
        transforms.Normalize([0.406, 0.456, 0.485], [0.225, 0.224, 0.229]),
        ])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.406, 0.456, 0.485], [0.225, 0.224, 0.229])
    ])","
import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedKFold

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'           #### NEED TO CHANGED IT!!!!!!!!!!!!!!!!!!!
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_full_images/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))
    print(""Training path: {}"".format(train_path))
    # Data Processing


    ## SHUFFLE DATA
    skf = None
    # if cfg.valid_type == 'holdout':
    test_805 = pd.read_csv('./input/test_805.csv')
    test_805 = pd.concat([test_805, test_805])
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True,
                                                                   random_state=cfg.seed, stratify=train_csv['diagnosis'])
    # train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid) # need to change It!!!!!
    # elif cfg.valid_type == 'cv':
    #     skf = StratifiedKFold(n_splits=cfg.n_folds, shuffle=True, random_state=cfg.seed)



    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model
    cfg.model.load_state_dict(
        torch.load('./Model_weights_finetuning/finetune25_like24_but_resnext101_img_size_299.pth')['model'])

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()

    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                pass
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # loss = torch.mean(cfg.criterion.lossfun((output - target)))
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                pass
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            # target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # loss = torch.mean(cfg.criterion.lossfun((output - target)))
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [1e-3]
    p_horizontalflip_list = [0.4]
    model_type_list = ['ResNext101']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",18649.89
2019.09.06 18:46:07,new_old,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models
from torchvision.models import resnext101_32x8d as resnext
from torchvision.models import DenseNet

class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        self.model = None
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            # model.conv1 = nn.Conv2d(4, 64, (7, 7), (2, 2), (3, 3), bias=False)
            #     param.requires_grad = False
            print(model)
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                # nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'ResNet152':
            model = models.resnet152(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet152.pth""))
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model
        elif model_type == 'ResNext101':
            model = resnext(pretrained=True)
            # model.conv1 = nn.Conv2d(4, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
            model.fc = nn.Linear(in_features=2048, out_features=1)
            self.model = model

        elif model_type == 'DenseNet':
            # model = models.densenet121(pretrained=True)
            # model.conv1 = nn.Conv2d(4, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
            # model.classifier = nn.Linear(in_features=1024, out_features=1)
            # model.classifier = nn.Sequential(
            #     nn.Dropout(0.4),
            #     nn.Linear(in_features=1920, out_features=1)
            # )
            # print(model)
            pass
            # self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))


            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            # model._conv_stem = nn.Conv2d(4, 32, kernel_size=3, stride=2, bias=False)
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))
            # model._conv_stem = nn.Conv2d(4, 48, kernel_size=3, stride=2, bias=False)
            # for i, layer in enumerate(model.):
            #     if ""batch_normalization"" in layer.name:
            #         effnet.layers[i] = GroupNormalization(groups=2, axis=-1, epsilon=0.1)
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            # #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b6':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b6-c76e70fd.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b7':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b7-dcc49843.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            #model.cuda()
            self.model = model


",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 1e-05
)",<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb50aa4cf8>,10.0,63789521.0,200.0,12.0,0.515767,0.41289899999999996,0.8062,,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np
from robust_loss_pytorch import AdaptiveLossFunction

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'data_imgsize_224'
        self.img_size = 200
        # self.img_size_crop = 300

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()#AdaptiveLossFunction(num_dims=1, float_dtype=np.float32, device='cuda:0')
        # self.num_classes = 5
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 10
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP

        self.num_workers = 16
        self.n_epochs = 200
        self.batch_size = batch_size
        self.valid_type = 'holdout' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50","from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config
import math

from utils_img import *
from retinal_process import *

# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()
# import tensorflow as tf

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p



def subtract_median_bg_image(im):
    k = np.max(im.shape)//20*2+1
    bg = cv2.medianBlur(im, k)
    return cv2.addWeighted (im, 4, bg, -4, 128)

PARAM = 96
def Radius_Reduction(img,PARAM):
    h,w,c=img.shape
    Frame=np.zeros((h,w,c),dtype=np.uint8)
    cv2.circle(Frame,(int(math.floor(w/2)),int(math.floor(h/2))),int(math.floor((h*PARAM)/float(2*100))), (255,255,255), -1)
    Frame1=cv2.cvtColor(Frame, cv2.COLOR_BGR2GRAY)
    img1 =cv2.bitwise_and(img,img,mask=Frame1)
    return img1

def info_image(im):
    # Compute the center (cx, cy) and radius of the eye
    cy = im.shape[0]//2
    midline = im[cy,:]
    midline = np.where(midline>midline.mean()/3)[0]
    if len(midline)>im.shape[1]//2:
        x_start, x_end = np.min(midline), np.max(midline)
    else: # This actually rarely happens p~1/10000
        x_start, x_end = im.shape[1]//10, 9*im.shape[1]//10
    cx = (x_start + x_end)/2
    r = (x_end - x_start)/2
    return cx, cy, r

def resize_image(im, image_size, augmentation=False):
    # Crops, resizes and potentially augments the image to IMAGE_SIZE
    cx, cy, r = info_image(im)
    scaling = image_size/(2*r)
    rotation = 0
    if augmentation:
        scaling *= 1 + 0.3 * (np.random.rand()-0.5)
        rotation = 360 * np.random.rand()
    M = cv2.getRotationMatrix2D((cx,cy), rotation, scaling)
    M[0,2] -= cx - image_size/2
    M[1,2] -= cy - image_size/2
    return cv2.warpAffine(im,M,(image_size, image_size))

class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir
        self.number = 0

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        # lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)
        # lab_planes = cv2.split(lab)
        # clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(16, 16))
        # lab_planes[0] = clahe.apply(lab_planes[0])
        # lab = cv2.merge(lab_planes)
        # clahed = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)
        # image = resize_image(clahed, cfg.img_size)
        # print(tf.test.is_gpu_available())
        # vessel_model = VesselNet('./vessels/')
        image = process(image, size = cfg.img_size, crop='normal', preprocessing='clahe', fourth=None)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-150, 150)),
        transforms.RandomVerticalFlip(),
        transforms.ColorJitter(brightness=0.1, contrast=0.5, saturation=0.1, hue=0.1),
        # transforms.RandomResizedCrop(cfg.img_size_crop),
        transforms.ToTensor(),
        transforms.Normalize([0.406, 0.456, 0.485], [0.225, 0.224, 0.229]),
        ])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.406, 0.456, 0.485], [0.225, 0.224, 0.229])
    ])","
import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedKFold

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'           #### NEED TO CHANGED IT!!!!!!!!!!!!!!!!!!!
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_full_images/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))
    print(""Training path: {}"".format(train_path))
    # Data Processing


    ## SHUFFLE DATA
    skf = None
    # if cfg.valid_type == 'holdout':
    test_805 = pd.read_csv('./input/test_805.csv')
    test_805 = pd.concat([test_805, test_805])
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True,
                                                                   random_state=cfg.seed, stratify=train_csv['diagnosis'])
    # train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid) # need to change It!!!!!
    # elif cfg.valid_type == 'cv':
    #     skf = StratifiedKFold(n_splits=cfg.n_folds, shuffle=True, random_state=cfg.seed)



    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model
    cfg.model.load_state_dict(
        torch.load('./Model_weights_finetuning/finetune26_like25_b6_image_size224.pth')['model'])

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()

    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                pass
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # loss = torch.mean(cfg.criterion.lossfun((output - target)))
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                pass
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            # target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # loss = torch.mean(cfg.criterion.lossfun((output - target)))
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [5e-4]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b6']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",
2019.09.06 23:58:30,new_old,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models
from torchvision.models import resnext101_32x8d as resnext
from torchvision.models import DenseNet

class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        self.model = None
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            # model.conv1 = nn.Conv2d(4, 64, (7, 7), (2, 2), (3, 3), bias=False)
            #     param.requires_grad = False
            print(model)
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                # nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'ResNet152':
            model = models.resnet152(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet152.pth""))
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model
        elif model_type == 'ResNext101':
            model = resnext(pretrained=True)
            # model.conv1 = nn.Conv2d(4, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
            model.fc = nn.Linear(in_features=2048, out_features=1)
            self.model = model

        elif model_type == 'DenseNet':
            # model = models.densenet121(pretrained=True)
            # model.conv1 = nn.Conv2d(4, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
            # model.classifier = nn.Linear(in_features=1024, out_features=1)
            # model.classifier = nn.Sequential(
            #     nn.Dropout(0.4),
            #     nn.Linear(in_features=1920, out_features=1)
            # )
            # print(model)
            pass
            # self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))


            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            # model._conv_stem = nn.Conv2d(4, 32, kernel_size=3, stride=2, bias=False)
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))
            # model._conv_stem = nn.Conv2d(4, 48, kernel_size=3, stride=2, bias=False)
            # for i, layer in enumerate(model.):
            #     if ""batch_normalization"" in layer.name:
            #         effnet.layers[i] = GroupNormalization(groups=2, axis=-1, epsilon=0.1)
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            # #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b6':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b6-c76e70fd.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b7':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b7-dcc49843.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            #model.cuda()
            self.model = model


",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 1e-05
)",<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb50aa7cf8>,10.0,63789521.0,200.0,12.0,0.488886,0.345494,0.8574,,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np
from robust_loss_pytorch import AdaptiveLossFunction

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'data_imgsize_224'
        self.img_size = 224
        # self.img_size_crop = 300

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()#AdaptiveLossFunction(num_dims=1, float_dtype=np.float32, device='cuda:0')
        # self.num_classes = 5
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 10
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP

        self.num_workers = 16
        self.n_epochs = 200
        self.batch_size = batch_size
        self.valid_type = 'holdout' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50","from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config
import math

from utils_img import *
from retinal_process import *

# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()
# import tensorflow as tf

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p



def subtract_median_bg_image(im):
    k = np.max(im.shape)//20*2+1
    bg = cv2.medianBlur(im, k)
    return cv2.addWeighted (im, 4, bg, -4, 128)

PARAM = 96
def Radius_Reduction(img,PARAM):
    h,w,c=img.shape
    Frame=np.zeros((h,w,c),dtype=np.uint8)
    cv2.circle(Frame,(int(math.floor(w/2)),int(math.floor(h/2))),int(math.floor((h*PARAM)/float(2*100))), (255,255,255), -1)
    Frame1=cv2.cvtColor(Frame, cv2.COLOR_BGR2GRAY)
    img1 =cv2.bitwise_and(img,img,mask=Frame1)
    return img1

def info_image(im):
    # Compute the center (cx, cy) and radius of the eye
    cy = im.shape[0]//2
    midline = im[cy,:]
    midline = np.where(midline>midline.mean()/3)[0]
    if len(midline)>im.shape[1]//2:
        x_start, x_end = np.min(midline), np.max(midline)
    else: # This actually rarely happens p~1/10000
        x_start, x_end = im.shape[1]//10, 9*im.shape[1]//10
    cx = (x_start + x_end)/2
    r = (x_end - x_start)/2
    return cx, cy, r

def resize_image(im, image_size, augmentation=False):
    # Crops, resizes and potentially augments the image to IMAGE_SIZE
    cx, cy, r = info_image(im)
    scaling = image_size/(2*r)
    rotation = 0
    if augmentation:
        scaling *= 1 + 0.3 * (np.random.rand()-0.5)
        rotation = 360 * np.random.rand()
    M = cv2.getRotationMatrix2D((cx,cy), rotation, scaling)
    M[0,2] -= cx - image_size/2
    M[1,2] -= cy - image_size/2
    return cv2.warpAffine(im,M,(image_size, image_size))

class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir
        self.number = 0

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        # lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)
        # lab_planes = cv2.split(lab)
        # clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(16, 16))
        # lab_planes[0] = clahe.apply(lab_planes[0])
        # lab = cv2.merge(lab_planes)
        # clahed = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)
        # image = resize_image(clahed, cfg.img_size)
        # print(tf.test.is_gpu_available())
        # vessel_model = VesselNet('./vessels/')
        image = process(image, size = cfg.img_size, crop='normal', preprocessing='clahe', fourth=None)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-150, 150)),
        transforms.RandomVerticalFlip(),
        transforms.ColorJitter(brightness=0.1, contrast=0.5, saturation=0.1, hue=0.1),
        # transforms.RandomResizedCrop(cfg.img_size_crop),
        transforms.ToTensor(),
        transforms.Normalize([0.406, 0.456, 0.485], [0.225, 0.224, 0.229]),
        ])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.406, 0.456, 0.485], [0.225, 0.224, 0.229])
    ])","
import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedKFold

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'           #### NEED TO CHANGED IT!!!!!!!!!!!!!!!!!!!
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_full_images/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))
    print(""Training path: {}"".format(train_path))
    # Data Processing


    ## SHUFFLE DATA
    skf = None
    # if cfg.valid_type == 'holdout':
    test_805 = pd.read_csv('./input/test_805.csv')
    test_805 = pd.concat([test_805, test_805])
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True,
                                                                   random_state=cfg.seed, stratify=train_csv['diagnosis'])
    # train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid) # need to change It!!!!!
    # elif cfg.valid_type == 'cv':
    #     skf = StratifiedKFold(n_splits=cfg.n_folds, shuffle=True, random_state=cfg.seed)



    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model
    cfg.model.load_state_dict(
        torch.load('./Model_weights_finetuning/finetune26_like25_b6_image_size224.pth')['model'])

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()

    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                pass
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # loss = torch.mean(cfg.criterion.lossfun((output - target)))
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                pass
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            # target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # loss = torch.mean(cfg.criterion.lossfun((output - target)))
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [5e-4]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b6']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",
2019.09.07 07:34:38,new_old,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models
from torchvision.models import resnext101_32x8d as resnext
from torchvision.models import DenseNet

class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        self.model = None
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            # model.conv1 = nn.Conv2d(4, 64, (7, 7), (2, 2), (3, 3), bias=False)
            #     param.requires_grad = False
            print(model)
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                # nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'ResNet152':
            model = models.resnet152(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet152.pth""))
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model
        elif model_type == 'ResNext101':
            model = resnext(pretrained=True)
            # model.conv1 = nn.Conv2d(4, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
            model.fc = nn.Linear(in_features=2048, out_features=1)
            self.model = model

        elif model_type == 'DenseNet':
            # model = models.densenet121(pretrained=True)
            # model.conv1 = nn.Conv2d(4, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
            # model.classifier = nn.Linear(in_features=1024, out_features=1)
            # model.classifier = nn.Sequential(
            #     nn.Dropout(0.4),
            #     nn.Linear(in_features=1920, out_features=1)
            # )
            # print(model)
            pass
            # self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))


            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            # model._conv_stem = nn.Conv2d(4, 32, kernel_size=3, stride=2, bias=False)
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))
            # model._conv_stem = nn.Conv2d(4, 48, kernel_size=3, stride=2, bias=False)
            # for i, layer in enumerate(model.):
            #     if ""batch_normalization"" in layer.name:
            #         effnet.layers[i] = GroupNormalization(groups=2, axis=-1, epsilon=0.1)
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            # #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b6':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b6-c76e70fd.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b7':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b7-dcc49843.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            #model.cuda()
            self.model = model


",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 1e-05
)",<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb9c503710>,10.0,7702403.0,200.0,16.0,0.38240799999999997,0.347421,0.8311,,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np
from robust_loss_pytorch import AdaptiveLossFunction

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'data_imgsize_224'
        self.img_size = 260
        # self.img_size_crop = 300

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()#AdaptiveLossFunction(num_dims=1, float_dtype=np.float32, device='cuda:0')
        # self.num_classes = 5
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 10
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP

        self.num_workers = 16
        self.n_epochs = 200
        self.batch_size = batch_size
        self.valid_type = 'holdout' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50","from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config
import math

from utils_img import *
from retinal_process import *

# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()
# import tensorflow as tf

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p



def subtract_median_bg_image(im):
    k = np.max(im.shape)//20*2+1
    bg = cv2.medianBlur(im, k)
    return cv2.addWeighted (im, 4, bg, -4, 128)

PARAM = 96
def Radius_Reduction(img,PARAM):
    h,w,c=img.shape
    Frame=np.zeros((h,w,c),dtype=np.uint8)
    cv2.circle(Frame,(int(math.floor(w/2)),int(math.floor(h/2))),int(math.floor((h*PARAM)/float(2*100))), (255,255,255), -1)
    Frame1=cv2.cvtColor(Frame, cv2.COLOR_BGR2GRAY)
    img1 =cv2.bitwise_and(img,img,mask=Frame1)
    return img1

def info_image(im):
    # Compute the center (cx, cy) and radius of the eye
    cy = im.shape[0]//2
    midline = im[cy,:]
    midline = np.where(midline>midline.mean()/3)[0]
    if len(midline)>im.shape[1]//2:
        x_start, x_end = np.min(midline), np.max(midline)
    else: # This actually rarely happens p~1/10000
        x_start, x_end = im.shape[1]//10, 9*im.shape[1]//10
    cx = (x_start + x_end)/2
    r = (x_end - x_start)/2
    return cx, cy, r

def resize_image(im, image_size, augmentation=False):
    # Crops, resizes and potentially augments the image to IMAGE_SIZE
    cx, cy, r = info_image(im)
    scaling = image_size/(2*r)
    rotation = 0
    if augmentation:
        scaling *= 1 + 0.3 * (np.random.rand()-0.5)
        rotation = 360 * np.random.rand()
    M = cv2.getRotationMatrix2D((cx,cy), rotation, scaling)
    M[0,2] -= cx - image_size/2
    M[1,2] -= cy - image_size/2
    return cv2.warpAffine(im,M,(image_size, image_size))

class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir
        self.number = 0

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        # lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)
        # lab_planes = cv2.split(lab)
        # clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(16, 16))
        # lab_planes[0] = clahe.apply(lab_planes[0])
        # lab = cv2.merge(lab_planes)
        # clahed = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)
        # image = resize_image(clahed, cfg.img_size)
        # print(tf.test.is_gpu_available())
        # vessel_model = VesselNet('./vessels/')
        image = process(image, size = cfg.img_size, crop='normal', preprocessing='clahe', fourth=None)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-150, 150)),
        transforms.RandomVerticalFlip(),
        transforms.ColorJitter(brightness=0.1, contrast=0.5, saturation=0.1, hue=0.1),
        # transforms.RandomResizedCrop(cfg.img_size_crop),
        transforms.ToTensor(),
        transforms.Normalize([0.406, 0.456, 0.485], [0.225, 0.224, 0.229]),
        ])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.406, 0.456, 0.485], [0.225, 0.224, 0.229])
    ])","
import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedKFold

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'           #### NEED TO CHANGED IT!!!!!!!!!!!!!!!!!!!
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_full_images/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))
    print(""Training path: {}"".format(train_path))
    # Data Processing


    ## SHUFFLE DATA
    skf = None
    # if cfg.valid_type == 'holdout':
    test_805 = pd.read_csv('./input/test_805.csv')
    test_805 = pd.concat([test_805, test_805])
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True,
                                                                   random_state=cfg.seed, stratify=train_csv['diagnosis'])
    # train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid) # need to change It!!!!!
    # elif cfg.valid_type == 'cv':
    #     skf = StratifiedKFold(n_splits=cfg.n_folds, shuffle=True, random_state=cfg.seed)



    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model
    cfg.model.load_state_dict(
        torch.load('./Model_weights_finetuning/exp213_end_epoch6.pth')['model'])

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()

    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                pass
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # loss = torch.mean(cfg.criterion.lossfun((output - target)))
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                pass
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            # target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # loss = torch.mean(cfg.criterion.lossfun((output - target)))
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [12]
    lr_list = [5e-4]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b7']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",
2019.09.07 11:13:31,new_old,"import torch
import torch.nn as nn
import torchvision.models as models # Pre-Trained models
from input.EfficientNet_PyTorch.efficientnet_pytorch import EfficientNet
import timm     # Another Pre-trained models
from torchvision.models import resnext101_32x8d as resnext
from torchvision.models import DenseNet

class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.conv = nn.Sequential(
            ConvBlock(in_channels=3, out_channels=64),
            ConvBlock(in_channels=64, out_channels=128),
            ConvBlock(in_channels=128, out_channels=256),
            ConvBlock(in_channels=256, out_channels=512)
        )

        self.fc = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.conv(x)
        x = torch.mean(x, dim=3)
        x, _ = torch.max(x, dim=2)
        x = self.fc(x)
        return x

## --------------_##
##  RESNEXT MODEL ##
## -------------- ##
from torchvision.models.resnet import ResNet, Bottleneck

# def _resnext(path, block, layers, pretrained, progress, **kwargs):
#     model = ResNet(block, layers, **kwargs)
#     model.load_state_dict(torch.load(path))
#     return model
#
# def resnext101_32x16d_wsl(path, progress=True, **kwargs):
#     """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
#     and finetuned on ImageNet from Figure 5 in
#     `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_
#     Args:
#         progress (bool): If True, displays a progress bar of the download to stderr.
#     """"""
#     kwargs['groups'] = 32
#     kwargs['width_per_group'] = 16
#     return _resnext(path, Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)


class MainModel:
    def __init__(self, model_type, num_classes=1):
        self.model = None
        if model_type == 'Simple':
            self.model = SimpleModel(num_classes)
        elif model_type == 'ResNet101':
            model = models.resnet101(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet101-5d3b4d8f.pth""))
            # for param in model.parameters():
            # model.conv1 = nn.Conv2d(4, 64, (7, 7), (2, 2), (3, 3), bias=False)
            #     param.requires_grad = False
            print(model)
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                # nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model

        elif model_type == 'ResNet152':
            model = models.resnet152(pretrained=False)
            model.load_state_dict(torch.load(""./input/pretrained-models/resnet152.pth""))
            # model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
            model.fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=2048, out_features=1),
            )
            self.model = model
        elif model_type == 'ResNext101':
            model = resnext(pretrained=True)
            # model.conv1 = nn.Conv2d(4, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
            model.fc = nn.Linear(in_features=2048, out_features=1)
            self.model = model

        elif model_type == 'DenseNet':
            # model = models.densenet121(pretrained=True)
            # model.conv1 = nn.Conv2d(4, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
            # model.classifier = nn.Linear(in_features=1024, out_features=1)
            # model.classifier = nn.Sequential(
            #     nn.Dropout(0.4),
            #     nn.Linear(in_features=1920, out_features=1)
            # )
            # print(model)
            pass
            # self.model = model

        elif model_type == 'efficientnet-b0':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b0-08094119.pth'))


            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Linear(in_features=1024, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp51_end_epoch30.pth')['model'])
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b1':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b1-dbc7070a.pth'))
            # model._conv_stem = nn.Conv2d(4, 32, kernel_size=3, stride=2, bias=False)
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b2':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b2-27687264.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b3':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b3-c8376fa2.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b4':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b4-e116e8b3.pth'))
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1),
                # nn.Dropout(0.4),
                # nn.Linear(in_features=1024, out_features=1, bias=True)
            )
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b5':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b5-586e6cc6.pth'))
            # model._conv_stem = nn.Conv2d(4, 48, kernel_size=3, stride=2, bias=False)
            # for i, layer in enumerate(model.):
            #     if ""batch_normalization"" in layer.name:
            #         effnet.layers[i] = GroupNormalization(groups=2, axis=-1, epsilon=0.1)
            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            # #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b6':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b6-c76e70fd.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            #model.cuda()
            self.model = model
        elif model_type == 'efficientnet-b7':
            model = EfficientNet.from_name(model_type)
            model.load_state_dict(torch.load('./input/pretrained-models/efficientnet-b7-dcc49843.pth'))

            in_features = model._fc.in_features
            model._fc = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(in_features=in_features, out_features=1)
            )
            ###################### DELETE!!!!!!!!!!!!!!!!!!!!!!!!
            # model.load_state_dict(torch.load('./Model_weights/exp43_end_epoch.pt9')['model'])
            #########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            #model.cuda()
            self.model = model


",MSELoss(),"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 1e-05
)",<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb50b6bda0>,10.0,10697769.0,20.0,16.0,0.353563,0.361130,0.8299,,"from model import MainModel
from torch import optim
import torch.nn as nn
import os
from logger import Logger
from libs.earlystopping import EarlyStopping

import torch, random
import numpy as np
from robust_loss_pytorch import AdaptiveLossFunction

def seed_torch(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class Config:
    def __init__(self, batch_size=32, lr=0.00015, p_horizontalflip=0.4, model_type='ResNet101', training_mode='only_new'):
        ## INFO ABOUT EXPERIMENT
        self.logsFileName = 'LOGS'
        self.logsFileName_finetuning = 'LOGS_finetuning'
        self.seed = 13

        seed_torch(self.seed)

        if os.path.exists('./Logs/' + self.logsFileName + '.csv'):
            if training_mode == 'only_new':
                self.df_logger = Logger(self.logsFileName + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
            elif training_mode == 'finetuning':
                self.df_logger = Logger(self.logsFileName_finetuning + '.csv', 'df')
                self.experiment_name = 'exp{}'.format(len(self.df_logger.logsFile)) + '_end_epoch'
                self.df_logger.save()
        else:
            self.experiment_name = 'exp{}'.format(0) + '_end_epoch'
        self.exper_type = 'data_imgsize_300'
        self.img_size = 300
        # self.img_size_crop = 300

        ## MODEL PARAMETERS
        self.weights_dir = './Model_weights/'
        self.weights_dir_finetuning = './Model_weights_finetuning/'
        self.model_type = model_type

        self.model = MainModel(model_type=self.model_type).model

        self.pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.lr = lr
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=2, verbose=True)
        self.criterion = nn.MSELoss()#AdaptiveLossFunction(num_dims=1, float_dtype=np.float32, device='cuda:0')
        # self.num_classes = 5
        self.model_param_list = [self.model, self.optimizer, self.scheduler]

        ## EARLY STOPPING
        self.early_stopping_patience = 10
        self.early_stopping = EarlyStopping(self.early_stopping_patience)
        self.early_stopping_loss = 'pytorch' #kappa

        ## TRAINING & VALIDATION SETUP

        self.num_workers = 16
        self.n_epochs = 200
        self.batch_size = batch_size
        self.valid_type = 'holdout' #CV
        self.valid_size = 0.2
        self.n_folds = 5 ## for CV!



        ## TRANSFORMER AND DATASET
        self.p_horizontalflip = p_horizontalflip
        self.data_type = 'new'

        ## PRINT FREQUENCY
        self.print_frequency = 50","from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
import os
import cv2
from os.path import isfile
import numpy as np
from config import Config
import math

from utils_img import *
from retinal_process import *

# The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam
cfg = Config()
# import tensorflow as tf

def crop_image1(img, tol=7):
    # img is image data
    # tol  is tolerance

    mask = img > tol
    return img[np.ix_(mask.any(1), mask.any(0))]


def crop_image_from_gray(img, tol=7):
    if img.ndim == 2:
        mask = img > tol
        return img[np.ix_(mask.any(1), mask.any(0))]
    elif img.ndim == 3:
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mask = gray_img > tol

        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]
        if (check_shape == 0):  # image is too dark so that we crop out everything,
            return img  # return original image
        else:
            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]
            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]
            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]
            #         print(img1.shape,img2.shape,img3.shape)
            img = np.stack([img1, img2, img3], axis=-1)
        #         print(img.shape)
        return img

def expand_path(p, train_path):
    p = str(p)
    if isfile(train_path + p + "".png""):
        return train_path + (p + "".png"")
    # if isfile(train_old_path + p + '.png'):
    #     return train_old_path + (p + "".png"")
    # if isfile(test + p + "".png""):
    #     return test + (p + "".png"")
    return p



def subtract_median_bg_image(im):
    k = np.max(im.shape)//20*2+1
    bg = cv2.medianBlur(im, k)
    return cv2.addWeighted (im, 4, bg, -4, 128)

PARAM = 96
def Radius_Reduction(img,PARAM):
    h,w,c=img.shape
    Frame=np.zeros((h,w,c),dtype=np.uint8)
    cv2.circle(Frame,(int(math.floor(w/2)),int(math.floor(h/2))),int(math.floor((h*PARAM)/float(2*100))), (255,255,255), -1)
    Frame1=cv2.cvtColor(Frame, cv2.COLOR_BGR2GRAY)
    img1 =cv2.bitwise_and(img,img,mask=Frame1)
    return img1

def info_image(im):
    # Compute the center (cx, cy) and radius of the eye
    cy = im.shape[0]//2
    midline = im[cy,:]
    midline = np.where(midline>midline.mean()/3)[0]
    if len(midline)>im.shape[1]//2:
        x_start, x_end = np.min(midline), np.max(midline)
    else: # This actually rarely happens p~1/10000
        x_start, x_end = im.shape[1]//10, 9*im.shape[1]//10
    cx = (x_start + x_end)/2
    r = (x_end - x_start)/2
    return cx, cy, r

def resize_image(im, image_size, augmentation=False):
    # Crops, resizes and potentially augments the image to IMAGE_SIZE
    cx, cy, r = info_image(im)
    scaling = image_size/(2*r)
    rotation = 0
    if augmentation:
        scaling *= 1 + 0.3 * (np.random.rand()-0.5)
        rotation = 360 * np.random.rand()
    M = cv2.getRotationMatrix2D((cx,cy), rotation, scaling)
    M[0,2] -= cx - image_size/2
    M[1,2] -= cy - image_size/2
    return cv2.warpAffine(im,M,(image_size, image_size))

class CreateDataset(Dataset):

    def __init__(self, df_data, data_dir, transform=None):
        self.df = df_data
        self.transform = transform
        self.train_path = data_dir
        self.number = 0

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        label = self.df.diagnosis.values[idx]
        label = np.expand_dims(label, -1)

        p = self.df.id_code.values[idx]
        p_path = expand_path(p, self.train_path)
        image = cv2.imread(p_path)
        # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        # lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)
        # lab_planes = cv2.split(lab)
        # clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(16, 16))
        # lab_planes[0] = clahe.apply(lab_planes[0])
        # lab = cv2.merge(lab_planes)
        # clahed = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)
        # image = resize_image(clahed, cfg.img_size)
        # print(tf.test.is_gpu_available())
        # vessel_model = VesselNet('./vessels/')
        image = process(image, size = cfg.img_size, crop='normal', preprocessing='clahe', fourth=None)
        image = transforms.ToPILImage()(image)

        if self.transform:
            image = self.transform(image)

        return image, label

transforms_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation((-150, 150)),
        transforms.RandomVerticalFlip(),
        transforms.ColorJitter(brightness=0.1, contrast=0.5, saturation=0.1, hue=0.1),
        # transforms.RandomResizedCrop(cfg.img_size_crop),
        transforms.ToTensor(),
        transforms.Normalize([0.406, 0.456, 0.485], [0.225, 0.224, 0.229]),
        ])

transforms_valid = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.406, 0.456, 0.485], [0.225, 0.224, 0.229])
    ])","
import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Plotting

# Import Image Libraries - Pillow and OpenCV
from PIL import Image
import cv2

# Import PyTorch and useful fuctions
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision
import torch.optim as optim
import torchvision.models as models # Pre-Trained models


# Import useful sklearn functions
import sklearn
from sklearn.metrics import cohen_kappa_score, accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedKFold

import time
from datetime import datetime
from tqdm import tqdm_notebook

import os
import random

# User-defined modules
from train_dataset import transforms_train, transforms_valid, CreateDataset
from config import Config

from logger import Logger

# Open source libs


## GLOBAL CONSTANTS:



def add_data_to_loggers(loggers_list, column_name, data):
    loggers_list[0].add_data(column_name, data)
    loggers_list[1].add_data(column_name, data)

# FOR DETERMINISTIC RESLTS
from config import seed_torch

def __init_fn(worker_id):
    np.random.seed(13 + worker_id)

def main(batch_size, lr, p_horizontalflip, model_type, info):
    ## CONFIG!
    cfg = Config(batch_size=batch_size, lr=lr, p_horizontalflip=p_horizontalflip, model_type=model_type)

    ## REPRODUCIBILITY
    seed_torch(cfg.seed)


    print(os.listdir(""./input""))
    base_dir = ""./input""

    # Loading Data + EDA

    train_new_csv = pd.read_csv('./input/train_new.csv')
    train_old_csv = pd.read_csv('./input/train_old.csv')
    train_csv = None
    train_path = None
    if cfg.data_type == 'new_old_mixed':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_images/'
    elif cfg.data_type == 'new':
        train_csv = train_new_csv
        train_path = './input/train_new_images/'           #### NEED TO CHANGED IT!!!!!!!!!!!!!!!!!!!
    elif cfg.data_type == 'old':
        train_csv = train_old_csv
        train_path = './input/train_old_images/'
    elif cfg.data_type == 'new_old_mixed_ben_preprocessing':
        train_csv = pd.concat([train_new_csv, train_old_csv], axis=0)
        train_path = './input/train_mixed_BEN_preprocessing/'
    elif cfg.data_type == 'new_old_balanced':
        train_csv = pd.read_csv('./input/train_balanced.csv')
        train_path = './input/train_mixed_full_images/'
    test_csv = pd.read_csv('./input/test.csv')
    print('Train Size = {}'.format(len(train_csv)))
    print('Public Test Size = {}'.format(len(test_csv)))

    counts = train_csv['diagnosis'].value_counts()
    class_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']
    for i, x in enumerate(class_list):
        counts[x] = counts.pop(i)
        print(""{:^12} - class examples: {:^6}"".format(x, counts[x]))
    print(""Training path: {}"".format(train_path))
    # Data Processing


    ## SHUFFLE DATA
    skf = None
    # if cfg.valid_type == 'holdout':
    test_805 = pd.read_csv('./input/test_805.csv')
    test_805 = pd.concat([test_805, test_805])
    train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True,
                                                                   random_state=cfg.seed, stratify=train_csv['diagnosis'])
    # train_csv, valid_csv = train_test_split(train_csv, test_size=cfg.valid_size,  shuffle=True, random_state=cfg.seed, stratify=train_csv['diagnosis'])
    train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=transforms_train)
    valid_data = CreateDataset(df_data=valid_csv, data_dir=train_path, transform=transforms_valid) # need to change It!!!!!
    # elif cfg.valid_type == 'cv':
    #     skf = StratifiedKFold(n_splits=cfg.n_folds, shuffle=True, random_state=cfg.seed)



    # obtain training indices that will be used for validation


    # Create Samplers


    # prepare data loaders (combine dataset and sampler)
    train_loader = DataLoader(train_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size,
                              pin_memory=True,
                              num_workers=cfg.num_workers,
                              shuffle=True,
                              worker_init_fn=__init_fn)

    # Model
    cfg.model.load_state_dict(
        torch.load('./Model_weights_finetuning/finetune27_b2_imgsize260_not_full_trained.pth')['model'])

    # check if CUDA is available
    train_on_gpu = torch.cuda.is_available()

    if not train_on_gpu:
        print('CUDA is not available.  Training on CPU ...')
    else:
        print('CUDA is available!  Training on GPU ...')
        cfg.model = cfg.model.cuda()

    # Trainable Parameters
    print(""Number of trainable parameters: \n{}"".format(cfg.pytorch_total_params))

    #Training(Fine-Tuning) and Validation
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # keeping track of losses as it happen
    train_losses = []
    valid_losses = []
    val_kappa = []
    test_accuracies = []
    valid_accuracies = []
    kappa_epoch = []

    # Loggers

    logger_df = Logger(logsFileName=cfg.logsFileName + '.csv', mode = 'df')
    logger_txt = Logger(logsFileName=cfg.logsFileName + '.txt', mode = 'txt')
    loggers_list = [logger_df, logger_txt]

    loggers_list[0].add_empty_row()
    loggers_list[1].add_empty_row()

    loggers_list[1].add_data('Experiment N: {}'.format(len(loggers_list[0].logsFile)-1), '')
    loggers_list[1].add_data(info, '')
    add_data_to_loggers(loggers_list, 'date', datetime.strftime(datetime.now(), ""%Y.%m.%d %H:%M:%S""))

    add_data_to_loggers(loggers_list, 'data-type', cfg.data_type)
    loggers_list[0].add_data('net-architecture', open('model.py', 'r+').read())
    add_data_to_loggers(loggers_list, 'loss-func', str(cfg.criterion))
    add_data_to_loggers(loggers_list, 'optim', str(cfg.optimizer))

    if cfg.scheduler is not None:
        add_data_to_loggers(loggers_list, 'scheduler', str(cfg.scheduler))

    if cfg.early_stopping is not None:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping_patience)
    else:
        add_data_to_loggers(loggers_list, 'early-stopping-patience', cfg.early_stopping)

    add_data_to_loggers(loggers_list, 'parameters-amount', cfg.pytorch_total_params)
    add_data_to_loggers(loggers_list, 'n-epochs', cfg.n_epochs)
    add_data_to_loggers(loggers_list, 'batch-size', cfg.batch_size)

    train_loss_best = np.inf
    valid_loss_best = np.inf
    kappa_best = 0

    add_data_to_loggers(loggers_list, 'best-train-loss', train_loss_best)
    add_data_to_loggers(loggers_list, 'best-valid-loss', valid_loss_best)
    add_data_to_loggers(loggers_list, 'best-kappa', kappa_best)
    add_data_to_loggers(loggers_list, 'lb-kappa-score', np.nan)

    loggers_list[0].add_data('cfg', open('config.py', 'r+').read())
    loggers_list[0].add_data('dataset', open('train_dataset.py', 'r+').read())
    loggers_list[0].add_data('trainloop', open('training.py', 'r+').read())



    ## PRINT OUTPUT FREQUENCY
    print_frequency = cfg.print_frequency
    start_full_time = time.time()

    for epoch in range(1, cfg.n_epochs + 1):
        # For timing
        # loggers_list[0].open()
        loggers_list[1].open()
        start_epoch_time = time.time()

        # keep track of training and validation loss
        train_loss_batch = []
        train_loss_epoch = []
        valid_loss_epoch = []
        ###################
        # train the cfg.model #
        ###################
        cfg.model.train()
        batch_n = 0
        for data, target in train_loader:
            batch_n += 1
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                pass
                data, target = data.cuda(), target.cuda().float()
            target = target.view(-1, 1)
            # clear the gradients of all optimized variables
            cfg.optimizer.zero_grad()
            with torch.set_grad_enabled(True):
                # forward pass: compute predicted outputs by passing inputs to the cfg.model
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # loss = torch.mean(cfg.criterion.lossfun((output - target)))
                # backward pass: compute gradient of the loss with respect to cfg.model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                cfg.optimizer.step()
                # data = data.cpu()
                train_loss_batch.append(loss.item())
                if batch_n % print_frequency == (print_frequency-1):
                    print('Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    loggers_list[1].add_data(None, 'Train loss on {} batch: {:.6f}'.format(batch_n+1, np.mean(train_loss_batch)))
                    train_loss_epoch.append(np.mean(train_loss_batch))
                    train_loss_batch = []
        train_loss_epoch.append(np.mean(train_loss_batch))
        torch.cuda.empty_cache()
        ######################
        # validate the cfg.model #
        ######################
        cfg.model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            if train_on_gpu:
                pass
                data, target = data.cuda(), target.cuda().float()
            # forward pass: compute predicted outputs by passing inputs to the cfg.model
            # target = target.view(-1, 1)
            with torch.no_grad():
                output = cfg.model(data)
                # calculate the batch loss
                loss = cfg.criterion(output, target)
                # loss = torch.mean(cfg.criterion.lossfun((output - target)))
            # loss = loss.cpu()
            # update average validation loss
            valid_loss_epoch.append(loss.item())
            y_actual = target.data.cpu().numpy()
            y_pred = output[:, -1].detach().cpu().numpy()
            val_kappa.append(cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic'))

            # calculate average losses
        train_loss_epoch = np.mean(train_loss_epoch)
        valid_loss_epoch = np.mean(valid_loss_epoch)
        valid_kappa = np.nanmean(val_kappa)
        kappa_epoch.append(valid_kappa)
        train_losses.append(train_loss_epoch)
        valid_losses.append(valid_loss_epoch)

        ## SCHEDULER STEP
        if cfg.scheduler is not None:
            if cfg.early_stopping_loss == 'pytorch':
                cfg.scheduler.step(valid_loss_epoch)
            elif cfg.early_stopping_loss == 'kappa':
                cfg.scheduler.step(1-valid_kappa)

        ## LOGGINS LOSSES
        if cfg.early_stopping_loss == 'pytorch':
            if valid_loss_best > valid_loss_epoch:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))
        elif cfg.early_stopping_loss == 'kappa':
            if kappa_best < valid_kappa:
                valid_loss_best  = valid_loss_epoch
                train_loss_best = train_loss_epoch
                kappa_best = valid_kappa
                add_data_to_loggers(loggers_list, 'best-train-loss', '{:.6f}'.format(train_loss_best))
                add_data_to_loggers(loggers_list, 'best-valid-loss', '{:.6f}'.format(valid_loss_best))
                add_data_to_loggers(loggers_list, 'best-kappa', '{:.4f}'.format(kappa_best))

        # print training/validation statistics
        print('Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))
        loggers_list[1].add_data('', 'Epoch: {} | Training Loss: {:.6f} | Val. Loss: {:.6f} | Val. Kappa Score: {:.4f} | LR: {:.6f} | Estimated time: {:.2f}'.format(
            epoch, train_loss_epoch, valid_loss_epoch, valid_kappa, cfg.optimizer.param_groups[0]['lr'], time.time() - start_epoch_time))

        ##################
        # Early Stopping #
        ##################
        if cfg.early_stopping_loss == 'pytorch':
            cfg.early_stopping(valid_loss_epoch, model_params_list=cfg.model_param_list, experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        elif cfg.early_stopping_loss == 'kappa':
            cfg.early_stopping(1 - valid_kappa, model_params_list=cfg.model_param_list,
                               experiment_name=cfg.weights_dir + cfg.experiment_name, epoch=epoch)
        if cfg.early_stopping.early_stop:
            add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
            add_data_to_loggers(loggers_list, 'n-epochs', epoch)
            loggers_list[0].save()
            loggers_list[1].save()
            break
        loggers_list[0].save()
        loggers_list[1].save()

    loggers_list[1].open()
    add_data_to_loggers(loggers_list, 'time_estimated', '{:.2f}'.format(time.time() - start_full_time))
    loggers_list[0].save()
    loggers_list[1].save()
    del cfg.model
    torch.cuda.empty_cache()

if __name__ == '__main__':
    batch_size_list = [16]
    lr_list = [5e-4]
    p_horizontalflip_list = [0.4]
    model_type_list = ['efficientnet-b2']
    for batch_size in batch_size_list:
        for lr in lr_list:
            for p_horizontalflip in p_horizontalflip_list:
                for model_type in model_type_list:
                    info = ""\n\n\nEXPERIMENT WITH BATCH_SIZE: {}, LR: {}, p_horizontalflip: {}, model_type: {}\n\n\n"".format(batch_size, lr, p_horizontalflip, model_type)
                    print(info)
                    main(batch_size, lr, p_horizontalflip, model_type, info)
",21122.09
